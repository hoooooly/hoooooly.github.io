

<!DOCTYPE html>
<html lang="Zh" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Holy Chan">
  <meta name="keywords" content="">
  
  <title>request、pyquest和pymongodb案例实战 - Holy的个人站点</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"e0b23de316904d07fb7110bd99d69a80","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"5nl8DQfwVmV1EVq02TMrVpLl-gzGzoHsz","app_key":"RbsmRlFYbVuACVohRTd4N6Cr","server_url":"https://5nl8dqfw.lc-cn-n1-shared.com"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Holy</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                主页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于我
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="request、pyquest和pymongodb案例实战">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-05-23 21:12" pubdate>
        May 23, 2021 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      5.1k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      63
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">request、pyquest和pymongodb案例实战</h1>
            
            <div class="markdown-body">
              <h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>在本节课开始之前，我们需要做好如下的准备工作：</p>
<ul>
<li>安装好<code>Python3</code>（最低为 3.6 版本），并能成功运行<code>Python3</code>程序。</li>
<li>了解<code>Python</code>多进程的基本原理。</li>
<li>了解<code>PythonHTTP</code>请求库<code>requests</code>的基本用法。</li>
<li>了解正则表达式的用法和<code>Python</code>中正则表达式库<code>re</code>的基本用法。</li>
<li>了解<code>PythonHTML</code>解析库<code>pyquery</code>的基本用法。</li>
<li>了解<code>MongoDB</code>并安装和启动<code>MongoDB</code>服务。</li>
<li>了解<code>Python</code>的<code>MongoDB</code>操作库<code>PyMongo</code>的基本用法。</li>
</ul>
<h2 id="爬虫目标"><a href="#爬虫目标" class="headerlink" title="爬虫目标"></a>爬虫目标</h2><p>一个基本的静态网站作为案例进行爬取，需要爬取的链接为：<a target="_blank" rel="noopener" href="https://static1.scrape.cuiqingcai.com/">https://static1.scrape.cuiqingcai.com/</a>，这个网站里面包含了一些电影信息。</p>
<p>要完成的目标是：</p>
<ul>
<li>用<code>requests</code>爬取这个站点每一页的电影列表，顺着列表再爬取每个电影的详情页。</li>
<li>用<code>pyquery</code>和正则表达式提取每部电影的名称、封面、类别、上映时间、评分、剧情简介等内容。</li>
<li>把以上爬取的内容存入<code>MongoDB</code>数据库。</li>
<li>使用多进程实现爬取的加速。</li>
</ul>
<h3 id="爬取列表页"><a href="#爬取列表页" class="headerlink" title="爬取列表页"></a>爬取列表页</h3><p>爬取的第一步肯定要从列表页入手，首先观察一下列表页的结构和翻页规则。在浏览器中访问<a target="_blank" rel="noopener" href="https://static1.scrape.cuiqingcai.com/"></a>，然后打开浏览器开发者工具，观察每一个电影信息区块对应的<code>HTML</code>，以及进入到详情页的<code>URL</code>是怎样的，如图所示：</p>
<p><img src="Screenshot_1.webp" srcset="/img/loading.gif" lazyload></p>
<p>每部电影对应的区块都是一个<code>div</code>节点，它的<code>class</code>属性都有<code>el-card</code>这个值。每个列表页有10个这样的<code>div</code>节点，也就对应着10部电影的信息。</p>
<p>再分析下从列表页是怎么进入到详情页的，选中电影的名称，看下结果：</p>
<p><img src="Screenshot_2.webp" srcset="/img/loading.gif" lazyload></p>
<p>这个名称实际上是一个<code>h2</code>节点，其内部的文字就是电影的标题。<code>h2</code>节点的外面包含了一个<code>a</code>节点，这个<code>a</code>节点带有<code>href</code>属性，这就是一个超链接，其中<code>href</code>的值为<code>/detail/1</code>，这是一个相对网站的根<code>URL</code><a target="_blank" rel="noopener" href="https://static1.scrape.cuiqingcai.com/">https://static1.scrape.cuiqingcai.com/</a>路径，加上网站的根<code>URL</code>就构成了<a target="_blank" rel="noopener" href="https://static1.scrape.cuiqingcai.com/detail/1">https://static1.scrape.cuiqingcai.com/detail/1</a>，也就是这部电影详情页的<code>URL</code>。这样只需要提取这个<code>href</code>属性就能构造出详情页的<code>URL</code>并接着爬取了。</p>
<p>接下来分析下翻页的逻辑，拉到页面的最下方，可以看到分页页码，如图所示：</p>
<p><img src="Screenshot_3.webp" srcset="/img/loading.gif" lazyload></p>
<p>页面显示一共有<code>100</code>条数据，10页的内容，因此页码最多是10。接着我们点击第2页，如图所示：</p>
<p><img src="Screenshot_4.webp" srcset="/img/loading.gif" lazyload></p>
<p>可以看到网页的<code>URL</code>变成了<code>https://static1.scrape.cuiqingcai.com/page/2</code>，相比根<code>URL</code>多了<code>/page/2</code>这部分内容。网页的结构还是和原来一模一样，所以我们可以和第1页一样处理。</p>
<p>接着查看第3页、第4页等内容，可以发现有这么一个规律，每一页的<code>URL</code>最后分别变成了<code>/page/3</code>、<code>/page/4</code>。所以，<code>/page</code>后面跟的就是列表页的页码，当然第1页也是一样，在根<code>URL</code>后面加上<code>/page/1</code>也是能访问的，只不过网站做了一下处理，默认的页码是1，所以显示第1页的内容。</p>
<p>分析到这里，逻辑基本就清晰了。</p>
<p>如果要完成列表页的爬取，可以这么实现：</p>
<ul>
<li>遍历页码构造<code>10</code>页的索引页<code>URL</code>。</li>
<li>从每个索引页分析提取出每个电影的详情页<code>URL</code>。</li>
</ul>
<p>先定义一些基础的变量，并引入一些必要的库，写法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> pymongo<br><span class="hljs-keyword">from</span> pyquery <span class="hljs-keyword">import</span> PyQuery <span class="hljs-keyword">as</span> pq<br><span class="hljs-keyword">from</span> urllib.parse <span class="hljs-keyword">import</span> urljoin<br><br>logging.basicConfig(level=logging.INFO, <span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;%(asctime)s - %(levelname)s: %(message)s&#x27;</span>)<br><br>BASE_URL = <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com&#x27;</span><br>TOTAL_PAGE = <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure>

<p>引入<code>requests</code>用来爬取页面，<code>logging</code>用来输出信息，<code>re</code>用来实现正则表达式解析，<code>pyquery</code>用来直接解析网页，<code>pymongo</code>用来实现<code>MongoDB</code>存储，<code>urljoin</code>用来做<code>URL</code>的拼接。</p>
<p>接着定义日志输出级别和输出格式，完成之后再定义<code>BASE_URL</code>为当前站点的根<code>URL</code>，<code>TOTAL_PAGE</code>为需要爬取的总页码数量。</p>
<p>定义好了之后，来实现一个页面爬取的方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">scrape_page</span>(<span class="hljs-params">url</span>):</span><br>    logging.info(<span class="hljs-string">&#x27;scraping %s...&#x27;</span>, url)<br>    <span class="hljs-keyword">try</span>:<br>        response = requests.get(url, verify=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">if</span> response.status_code == <span class="hljs-number">200</span>:<br>            <span class="hljs-keyword">return</span> response.text<br>        logging.error(<span class="hljs-string">&#x27;get invalid status code %s while scraping %s&#x27;</span>, response.status_code, url)<br>    <span class="hljs-keyword">except</span> requests.RequestException:<br>        logging.error(<span class="hljs-string">&#x27;error occurred while scraping %s&#x27;</span>, url, exc_info=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<p>考虑到不仅要爬取列表页，还要爬取详情页，在这里定义一个较通用的爬取页面的方法，叫作<code>scrape_page</code>，它接收一个<code>url</code>参数，返回页面的<code>html</code>代码。</p>
<p>首先判断状态码是不是<code>200</code>，如果是，则直接返回页面的<code>HTML</code>代码，如果不是，则会输出错误日志信息。另外，这里实现了<code>requests</code>的异常处理，如果出现了爬取异常，则会输出对应的错误日志信息。这时将<code>logging</code>的<code>error</code>方法的<code>exc_info</code>参数设置为<code>True</code>则可以打印出<code>Traceback</code>错误堆栈信息。 </p>
<p>有了<code>scrape_page</code>方法之后，给这个方法传入一个<code>url</code>，正常情况下它就可以返回页面的<code>HTML</code>代码。</p>
<p>在这个基础上，来定义列表页的爬取方法吧，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">scrape_index</span>(<span class="hljs-params">page</span>):</span><br>    index_url = <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;BASE_URL&#125;</span>/page/<span class="hljs-subst">&#123;page&#125;</span>&#x27;</span><br>    <span class="hljs-keyword">return</span> scrape_page(index_url)<br></code></pre></td></tr></table></figure>

<p>方法名称叫作<code>scrape_index</code>，这个方法会接收一个<code>page</code>参数，即列表页的页码，在方法里面实现列表页的<code>URL</code>拼接，然后调用<code>scrape_page</code>方法爬取即可得到列表页的<code>HTML</code>代码了。</p>
<p>获取了<code>HTML</code>代码后，下一步就是解析列表页，并得到每部电影的详情页的<code>URL</code>了，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_index</span>(<span class="hljs-params">html</span>):</span><br>    doc = pq(html)<br>    links = doc(<span class="hljs-string">&#x27;.el-card .name&#x27;</span>)<br>    <span class="hljs-keyword">for</span> link <span class="hljs-keyword">in</span> links.items():<br>        href = link.attr(<span class="hljs-string">&#x27;href&#x27;</span>)<br>        detail_url = urljoin(BASE_URL, href)<br>        logging.info(<span class="hljs-string">&#x27;get detail info %s&#x27;</span>, detail_url)<br>        <span class="hljs-keyword">yield</span> detail_url<br></code></pre></td></tr></table></figure>

<p>这里我们定义了<code>parse_index</code>方法，它接收一个<code>html</code>参数，即列表页的<code>HTML</code>代码。接着用<code>pyquery</code>新建一个<code>PyQuery</code>对象，完成之后再用<code>.el-card .name</code>选择器选出来每个电影名称对应的超链接节点。遍历这些节点，通过调用<code>attr</code>方法并传入<code>href</code>获得详情页的<code>URL</code>路径，得到的<code>href</code>就是上文所说的类似<code>/detail/1</code>这样的结果。这并不是一个完整的<code>URL</code>，所以需要借助<code>urljoin</code>方法把<code>BASE_URL</code>和<code>href</code>拼接起来，获得详情页的完整<code>URL</code>，得到的结果就是类似<code>https://static1.scrape.cuiqingcai.com/detail/1</code>这样完整的<code>URL</code>了，最后<code>yield</code>返回即可。</p>
<p>通过调用<code>parse_index</code>方法传入列表页的<code>HTML</code>代码就可以获得该列表页所有电影的详情页<code>URL</code>了，接下来把上面的方法串联调用一下，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, TOTAL_PAGE+<span class="hljs-number">1</span>):<br>        index_html = scrape_index(page)<br>        detail_urls = parse_index((index_html))<br>        logging.info(<span class="hljs-string">&#x27;detail urls %s&#x27;</span>, <span class="hljs-built_in">list</span>(detail_urls))<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></td></tr></table></figure>

<p>定义了<code>main</code>方法来完成上面所有方法的调用，首先使用<code>range</code>方法遍历一下页码，得到的<code>page</code>是<code>1~10</code>，接着把<code>page</code>变量传给<code>scrape_index</code>方法，得到列表页的<code>HTML</code>，赋值为<code>index_html</code>变量。接下来再将<code>index_html</code>变量传给<code>parse_index</code>方法，得到列表页所有电影的详情页<code>URL</code>，赋值为<code>detail_urls</code>，结果是一个生成器，调用<code>list</code>方法就可以将其输出出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,059 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">1</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,060 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">2</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,061 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">3</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,062 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">4</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,063 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">5</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,064 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">6</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,065 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">7</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,067 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">8</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,070 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">9</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,072 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="hljs-number">10</span><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,074 - INFO: detail urls [<span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/1&#x27;</span>, <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/2&#x27;</span>, <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/3&#x27;</span>, <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/4&#x27;</span>, <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/5&#x27;</span>, <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/6&#x27;</span>, <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/7&#x27;</span>, <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/8&#x27;</span>, <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/9&#x27;</span>, <span class="hljs-string">&#x27;https://static1.scrape.cuiqingcai.com/detail/10&#x27;</span>]<br><span class="hljs-number">2021</span>-05-<span class="hljs-number">23</span> <span class="hljs-number">23</span>:<span class="hljs-number">23</span>:04,076 - INFO: scraping https://static1.scrape.cuiqingcai.com/page/<span class="hljs-number">2.</span>..<br>....<br></code></pre></td></tr></table></figure>

<p>由于输出内容比较多，这里只贴了一部分。可以看到，在这个过程中程序首先爬取了第<code>1</code>页列表页，然后得到了对应详情页的每个<code>URL</code>，接着再接着爬第2页、第3页，一直到第10页，依次输出了每一页的详情页<code>URL</code>。这样，就成功获取到所有电影详情页<code>URL</code>。</p>
<h3 id="爬取详情页"><a href="#爬取详情页" class="headerlink" title="爬取详情页"></a>爬取详情页</h3><p>首先观察一下详情页的<code>HTML</code>代码，如图所示：</p>
<p><img src="Screenshot_5.webp" srcset="/img/loading.gif" lazyload></p>
<p>经过分析，要提取的内容和对应的节点信息如下：</p>
<ul>
<li>封面：是一个<code>img</code>节点，其<code>class</code>属性为<code>cover</code>。</li>
<li>名称：是一个<code>h2</code>节点，其内容便是名称。</li>
<li>类别：是<code>span</code>节点，其内容便是类别</li>
<li>内容，其外侧是<code>button</code>节点，再外侧则是<code>class</code>为<code>categories</code>的<code>div</code>节点。</li>
<li>上映时间：是<code>span</code>节点，其内容包含了上映时间，其外侧是包含了<code>class</code>为<code>info</code>的<code>div</code>节点。但注意这个<code>div</code>前面还有一个<code>class</code>为<code>info</code>的<code>div</code>节点，可以使用其内容来区分，也可以使用<code>nth-child</code>或<code>nth- of-type</code>这样的选择器来区分。另外提取结果中还多了「上映」二字，可以用正则表达式把日期提取出来。</li>
<li>评分：是一个<code>p</code>节点，其内容便是评分，<code>p</code>节点的<code>class</code>属性为<code>score</code>。</li>
<li>剧情简介：是一个<code>p</code>节点，其内容便是</li>
<li>剧情简介，其外侧是<code>class</code>为<code>drama</code>的<code>div</code>节点。</li>
</ul>
<p>刚才已经成功获取了详情页的<code>URL</code>，接下来要定义一个详情页的爬取方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">scrape_detail</span>(<span class="hljs-params">url</span>):</span> <br>    <span class="hljs-keyword">return</span> scrape_page(url)<br></code></pre></td></tr></table></figure>

<p>定义了一个<code>scrape_detail</code>方法，它接收一个<code>url</code>参数，并通过调用<code>scrape_page</code>方法获得网页源代码。由于刚才已经实现了<code>scrape_page</code>方法，所以在这里不用再写一遍页面爬取的逻辑，直接调用即可，这就做到了代码复用。</p>
<p>单独定义一个<code>scrape_detail</code>方法在逻辑上会显得更清晰，而且以后如果想要对<code>scrape_detail</code>方法进行改动，比如添加日志输出或是增加预处理，都可以在 <code>scrape_detail</code>里面实现，而不用改动<code>scrape_page</code>方法，灵活性会更好。</p>
<p>详情页的爬取方法已经实现了，接着就是详情页的解析了，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_detail</span>(<span class="hljs-params">html</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;详情页的解析&quot;&quot;&quot;</span><br>    doc = pq(html)<br>    cover = doc(<span class="hljs-string">&#x27;img.cover&#x27;</span>).attr(<span class="hljs-string">&#x27;src&#x27;</span>)<br>    name = doc(<span class="hljs-string">&#x27;a &gt; h2&#x27;</span>).text()<br>    categories = [item.text() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> doc(<span class="hljs-string">&#x27;.categories button span&#x27;</span>).items()]<br>    published_at = doc(<span class="hljs-string">&#x27;.info:contains(上映)&#x27;</span>).text()<br>    published_at = re.search(<span class="hljs-string">&#x27;(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)&#x27;</span>, published_at).group(<span class="hljs-number">1</span>) \<br>        <span class="hljs-keyword">if</span> published_at <span class="hljs-keyword">and</span> re.search(<span class="hljs-string">&#x27;\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;&#x27;</span>, published_at) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>    drama = doc(<span class="hljs-string">&#x27;.drama p&#x27;</span>).text()<br>    score = doc(<span class="hljs-string">&#x27;p.score&#x27;</span>).text()<br>    score = <span class="hljs-built_in">float</span>(score) <span class="hljs-keyword">if</span> score <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">return</span>&#123;<br>        <span class="hljs-string">&#x27;cover&#x27;</span>: cover,<br>        <span class="hljs-string">&#x27;name&#x27;</span>: name,<br>        <span class="hljs-string">&#x27;categories&#x27;</span>: categories,<br>        <span class="hljs-string">&#x27;published_at&#x27;</span>: published_at,<br>        <span class="hljs-string">&#x27;drama&#x27;</span>: drama,<br>        <span class="hljs-string">&#x27;score&#x27;</span>: score<br>    &#125;<br></code></pre></td></tr></table></figure>

<p>定义了<code>parse_detail</code>方法用于解析详情页，它接收一个<code>html</code>参数，解析其中的内容，并以字典的形式返回结果。每个字段的解析情况如下所述：</p>
<ul>
<li><code>cove</code>r：封面，直接选取<code>class</code>为<code>cover</code>的<code>img</code>节点，并调用<code>attr</code>方法获取<code>src</code>属性的内容即可。</li>
<li><code>name</code>：名称，直接选取<code>a</code>节点的直接子节点<code>h2</code>节点，并调用<code>text</code>方法提取其文本内容即可得到名称。</li>
<li><code>categories</code>：类别，由于类别是多个，所以这里首先用<code>.categories button span</code>选取了<code>class</code>为<code>categories</code>的节点内部的<code>span</code>节点，其结果是多个，所以这里进行了遍历，取出了每个<code>span</code>节点的文本内容，得到的便是列表形式的类别。</li>
<li><code>published_at</code>：上映时间，由于<code>pyquery</code>支持使用<code>:contains</code>直接指定包含的文本内容并进行提取，且每个上映时间信息都包含了「上映」二字，所以这里就直接使用<code>:contains(上映)</code>提取了<code>class</code>为<code>info</code>的<code>div</code>节点。提取之后，得到的结果类似「1993-07-26 上映」这样，并不想要「上映」这两个字，所以又调用了正则表达式把日期单独提取出来了。当然这里也可以直接使用<code>strip</code>或<code>replace</code>方法把多余的文字去掉。</li>
<li><code>drama</code>：直接提取<code>class</code>为<code>drama</code>的节点内部的<code>p</code>节点的文本即可。</li>
<li><code>score</code>：直接提取<code>class</code>为<code>score</code>的<code>p</code>节点的文本即可，由于提取结果是字符串，所以我们需要把它转成浮点数，即``float`类型。</li>
</ul>
<p>上述字段提取完毕之后，构造一个字典返回。这样，成功完成了详情页的提取和分析了。</p>
<p>将main方法稍微改写一下，增加这两个方法的调用，改写如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, TOTAL_PAGE+<span class="hljs-number">1</span>):<br>        index_html = scrape_index(page)<br>        detail_urls = parse_index((index_html))<br>        <span class="hljs-keyword">for</span> detail_url <span class="hljs-keyword">in</span> detail_urls:<br>            detail_html = scrape_detail(detail_url)<br>            data = parse_detail(detail_html)<br>            logging.info(<span class="hljs-string">&#x27;get detail data %s&#x27;</span>, data)<br></code></pre></td></tr></table></figure>

<p>首先遍历了<code>detail_urls</code>，获取了每个详情页的<code>URL</code>，然后依次调用了<code>scrape_detail</code>和<code>parse_detail</code>方法，最后得到了每个详情页的提取结果，赋值为<code>data</code>并输出。</p>
<p>运行结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">2021</span>-05-<span class="hljs-number">24</span> <span class="hljs-number">12</span>:<span class="hljs-number">46</span>:<span class="hljs-number">31</span>,<span class="hljs-number">432</span> - INFO: get detail data &#123;<span class="hljs-string">&#x27;cover&#x27;</span>: <span class="hljs-string">&#x27;https://p0.meituan.net/movie/b0d986a8bf89278afbb19f6abaef70f31206570.jpg@464w_644h_1e_1c&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&quot;辛德勒的名单 - Schindler&#x27;s List&quot;</span>, <span class="hljs-string">&#x27;categories&#x27;</span>: [<span class="hljs-string">&#x27;剧情&#x27;</span>, <span class="hljs-string">&#x27;历史&#x27;</span>, <span class="hljs-string">&#x27;战争&#x27;</span>], <span class="hljs-string">&#x27;published_at&#x27;</span>: <span class="hljs-string">&#x27;1993-11-30&#x27;</span>, <span class="hljs-string">&#x27;drama&#x27;</span>: <span class="hljs-string">&#x27;1939年，波兰在纳粹德国的统治下，党卫军对犹太人进行了隔离统治。德国商人奥斯卡·辛德勒（连姆·尼森 饰）来到德军统治下的克拉科夫，开设了一间搪瓷厂，生产军需用品。凭着出众的社交能力和大量的金钱，辛德勒和德军建立了良好 的关系，他的工厂雇用犹太人工作，大发战争财。1943年，克拉科夫的犹太人遭到了惨绝人寰的大屠杀，辛德勒目睹这一切，受到了极大的震撼，他贿赂军官，让自己的工厂成为集中营的附属劳役营，在那些疯狂屠杀的日子里，他的工厂也成为了犹太人的避难所。1944年，德国战败前夕，屠杀犹太人的行动越发疯狂，辛德勒向德军军官开出了1200人的名单，倾家荡产买下了这些犹太人的生命。在那些暗无天日的岁月里，拯救一个人，就是拯救全世界。&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">9.5</span>&#125;<br></code></pre></td></tr></table></figure>

<p>可以看到，已经成功提取出每部电影的基本信息，包括封面、名称、类别，等等。</p>
<h2 id="保存到MongoDB"><a href="#保存到MongoDB" class="headerlink" title="保存到MongoDB"></a>保存到MongoDB</h2><p>请确保现在有一个可以正常连接和使用的<code>MongoDB</code>数据库。 将数据导入<code>MongoDB</code>需要用到<code>PyMongo</code>这个库，这个在最开始已经引入过了。那么接下来我们定义一下 <code>MongoDB</code>的连接配置，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">MONGO_CONNECTION_STRING = <span class="hljs-string">&#x27;mongodb://localhost:27017&#x27;</span><br>MONGO_DB_NAME = <span class="hljs-string">&#x27;movies&#x27;</span><br>MONGO_COLLECTIONS_NAME = <span class="hljs-string">&#x27;movies&#x27;</span><br><br>client = pymongo.MongoClient(MONGO_CONNECTION_STRING)<br>db = client[<span class="hljs-string">&#x27;movies&#x27;</span>]<br>collection = db[<span class="hljs-string">&#x27;movies&#x27;</span>]<br></code></pre></td></tr></table></figure>

<p>在这里声明了几个变量，介绍如下：</p>
<ul>
<li><code>MONGO_CONNECTION_STRING</code>：<code>MongoDB</code>的连接字符串，里面定义了<code>MongoDB</code>的基本连接信息，如<code>host</code>、<code>port</code>，还可以定义用户名密码等内容。</li>
<li><code>MONGO_DB_NAME</code>：<code>MongoDB</code>数据库的名称。</li>
<li><code>MONGO_COLLECTION_NAME</code>：<code>MongoDB</code>的集合名称。</li>
</ul>
<p>这里用<code>MongoClient</code>声明了一个连接对象，然后依次声明了存储的数据库和集合。接下来，再实现一个将数据保存到<code>MongoDB</code>的方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save_data</span>(<span class="hljs-params">data</span>):</span><br>    collection.update_one(&#123;<br>        <span class="hljs-string">&#x27;name&#x27;</span>: data.get(<span class="hljs-string">&#x27;name&#x27;</span>)<br>    &#125;,&#123;<br>        <span class="hljs-string">&#x27;$set&#x27;</span>: data<br>    &#125;, upsert=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>

<p>声明了一个<code>save_data</code>方法，它接收一个<code>data</code>参数，也就是我们刚才提取的电影详情信息。在方法里面，调用了<code>update_one</code>方法，第1个参数是<strong>查询条件</strong>，即根据<code>name</code>进行查询；第2个参数是<code>data</code>对象本身，也就是所有的数据，这里用<code>$set</code>操作符表示更新操作；第3个参数很关键，这里实际上是<code>upsert</code>参数，如果把这个设置为 <code>True</code>，则可以做到存在即更新，不存在即插入的功能，更新会根据第一个参数设置的<code>name</code>字段，所以这样可以防止数据库中出现同名的电影数据。</p>
<blockquote>
<p>注：实际上电影可能有同名，但该场景下的爬取数据没有同名情况，当然这里更重要的是实现<code>MongoDB</code>的去重操作。</p>
</blockquote>
<p>接下来将<code>main</code>方法稍微改写一下就好了，改写如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, TOTAL_PAGE+<span class="hljs-number">1</span>):<br>        index_html = scrape_index(page)<br>        detail_urls = parse_index((index_html))<br>        <span class="hljs-keyword">for</span> detail_url <span class="hljs-keyword">in</span> detail_urls:<br>            detail_html = scrape_detail(detail_url)<br>            data = parse_detail(detail_html)<br>            logging.info(<span class="hljs-string">&#x27;get detail data %s\n&#x27;</span>, data)<br>            logging.info(<span class="hljs-string">&#x27;saving data to mongodb&#x27;</span>)<br>            save_data(data)<br>            logging.info(<span class="hljs-string">&#x27;data saved successfully\n&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>重新运行，输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">......<br><span class="hljs-number">2021</span>-05-<span class="hljs-number">24</span> <span class="hljs-number">13</span>:<span class="hljs-number">16</span>:<span class="hljs-number">46</span>,<span class="hljs-number">882</span> - INFO: get detail data &#123;<span class="hljs-string">&#x27;cover&#x27;</span>: <span class="hljs-string">&#x27;https://p0.meituan.net/movie/58782fa5439c25d764713f711ebecd1e201941.jpg@464w_644h_1e_1c&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;魂断蓝桥 - Waterloo Bridge&#x27;</span>, <span class="hljs-string">&#x27;categories&#x27;</span>: [<span class="hljs-string">&#x27;剧情&#x27;</span>, <span class="hljs-string">&#x27;爱情&#x27;</span>, <span class="hljs-string">&#x27;战争&#x27;</span>], <span class="hljs-string">&#x27;published_at&#x27;</span>: <span class="hljs-string">&#x27;1940-05-17&#x27;</span>, <span class="hljs-string">&#x27;drama&#x27;</span>: <span class="hljs-string">&#x27;第一次世界大战期间，回国度假的陆军中 </span><br><span class="hljs-string">尉罗伊（罗伯特·泰勒 饰）在滑铁卢桥上邂逅了舞蹈演员玛拉（费雯·丽 饰），两人彼此倾心，爱情迅速升温。就在两人决定结婚之时，罗伊应招回营地，两人被迫分离。由</span><br><span class="hljs-string">于错过剧团演出，玛拉被开除，只能和好友相依为命。不久玛拉得知罗伊阵亡的消息，几欲崩溃，备受打击。失去爱情的玛拉感到一切都失去了意义，为了生存，她和好友不</span><br><span class="hljs-string">得不沦为妓女。然而命运弄人，就在此时玛拉竟然再次遇到了罗伊。虽然为罗伊的生还兴奋不已，玛拉却因自己的失身陷入痛苦之中。感到一切难以挽回的玛拉潸然离开，独</span><br><span class="hljs-string">自来到两人最初相遇的地点——滑铁卢桥上…&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">9.5</span>&#125;<br><br><span class="hljs-number">2021</span>-05-<span class="hljs-number">24</span> <span class="hljs-number">13</span>:<span class="hljs-number">16</span>:<span class="hljs-number">46</span>,<span class="hljs-number">885</span> - INFO: saving data to mongodb<br><span class="hljs-number">2021</span>-05-<span class="hljs-number">24</span> <span class="hljs-number">13</span>:<span class="hljs-number">16</span>:<span class="hljs-number">46</span>,<span class="hljs-number">889</span> - INFO: data saved successfully<br></code></pre></td></tr></table></figure>

<p>运行完毕之后我们可以使用<code>MongoDB</code>客户端工具可视化查看已经爬取到的数据，结果如下：</p>
<p><img src="Screenshot_6.webp" srcset="/img/loading.gif" lazyload></p>
<h2 id="多进程加速"><a href="#多进程加速" class="headerlink" title="多进程加速"></a>多进程加速</h2><p>由于整个的爬取是单进程的，而且只能逐条爬取，速度稍微有点慢，有没有方法来对整个爬取过程进行加速呢？</p>
<p>在前面学习了多进程的基本原理和使用方法，下面就来实践一下多进程的爬取。</p>
<p>由于一共有<code>10</code>页详情页，并且这<code>10</code>页内容是互不干扰的，所以可以一页开一个进程来爬取。由于这<code>10</code>个列表页页码正好可以提前构造成一个列表，所以可以选用多进程里面的进程池<code>Pool</code>来实现这个过程。</p>
<p>这里需要改写下<code>main</code>方法的调用，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>(<span class="hljs-params">page</span>):</span><br>    <span class="hljs-keyword">for</span> page <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, TOTAL_PAGE+<span class="hljs-number">1</span>):<br>        index_html = scrape_index(page)<br>        detail_urls = parse_index((index_html))<br>        <span class="hljs-keyword">for</span> detail_url <span class="hljs-keyword">in</span> detail_urls:<br>            detail_html = scrape_detail(detail_url)<br>            data = parse_detail(detail_html)<br>            logging.info(<span class="hljs-string">&#x27;get detail data %s\n&#x27;</span>, data)<br>            logging.info(<span class="hljs-string">&#x27;saving data to mongodb&#x27;</span>)<br>            save_data(data)<br>            logging.info(<span class="hljs-string">&#x27;data saved successfully\n&#x27;</span>)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    pool = multiprocessing.Pool()<br>    pages = <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, TOTAL_PAGE + <span class="hljs-number">1</span>)<br>    pool.<span class="hljs-built_in">map</span>(main, pages)<br>    pool.close()<br>    pool.join()<br></code></pre></td></tr></table></figure>

<p>这里首先给<code>main</code>方法添加一个参数<code>page</code>，用以表示列表页的页码。接着声明了一个进程池，并声明<code>pages</code>为所有需要遍历的页码，即<code>1~10</code>。最后调用<code>map</code>方法，第1个参数就是需要被调用的方法，第2个参数就是<code>pages</code>，即需要遍历的页码。</p>
<p>这样<code>pages</code>就会被依次遍历。把<code>1~10</code>这10个页码分别传递给<code>main</code>方法，并把每次的调用变成一个进程，加入到进程池中执行，进程池会根据当前运行环境来决定运行多少进程。</p>
<p>运行输出结果和之前类似，但是可以明显看到加了多进程执行之后，爬取速度快了非常多。可以清空一下之前的<code>MongoDB</code>数据，可以发现数据依然可以被正常保存到<code>MongoDB</code>数据库中。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/request/">request</a>
                    
                      <a class="hover-with-bg" href="/tags/pyquest/">pyquest</a>
                    
                      <a class="hover-with-bg" href="/tags/pymongodb/">pymongodb</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/05/24/Ajax%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E8%A7%A3%E6%9E%90/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Ajax的原理和解析</span>
                        <span class="visible-mobile">Vorheriger</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/05/23/MongoDB%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/">
                        <span class="hidden-mobile">MongoDB数据库的使用</span>
                        <span class="visible-mobile">Nächster</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Suchen</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">Stichwort</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


  
  <!-- 备案信息 -->
  <div class="beian">
    <span>
      <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
        鄂ICP备2021008617号-1
      </a>
    </span>
    
  </div>


  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>




  <script defer src="/js/leancloud.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>












  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?e0b23de316904d07fb7110bd99d69a80";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
