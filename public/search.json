[{"title":"Django入门-视图函数","date":"2021-06-11T05:45:32.000Z","url":"/2021/06/11/Django%E5%85%A5%E9%97%A8-%E8%A7%86%E5%9B%BE%E5%87%BD%E6%95%B0/","tags":[["django","/tags/django/"]],"categories":[["Django","/categories/Django/"]],"content":"第一个视图函数第一个视图函数目前我们已经有一个视图函数叫 home ，这个视图在我们的应用程序主页上显示为&quot;Hello， World!&quot; 。 boards/views.py 首先要做的是导入Board模型并列出所有的板块。 boards/views.py 访问如下： 当然，真正的项目里面不会这样去渲染HTML，渲染的部分会交给Django模板引擎来实现。 模板引擎在manage.py所在的目录创建一个名为templates的新文件夹： 在templates文件夹中，创建一个名为home.html的HTML文件。 templates/home.html 上面的例子中，混入了原始的HTML和一些特殊的标签&#123;% for ... in ... %&#125;和&#123;&#123;variable&#125;&#125;。它们是Django模板语法的一部分。上面的列子展示了如何使用for遍历列表对象。&#123;&#123;board.name&#125;&#125;会在HTMl模板中会被渲染成板块的名称，最后生成动态HTML文档。 在我们可以使用这个HTML页面之前，我们必须告诉Django在哪里可以找到我们应用程序的模板。 打开myproject目录下面的settings.py文件，搜索 TEMPLATES 变量，并设置 DIRS 的值为 os.path.join（BASE_DIR，&#39;templates&#39;）∶ 本质上，刚添加的这一行所做的事情就是找到项目的完整路径并在后面附加&quot;templates&quot;。 我们可以使用Python shell进行调试∶ 打开python shell进行调试。 可以看到，它只是指向了我们在前面步骤中创建的templates文件夹。 现在我们可以更新视图： boards/views.py 重新运行，访问。 可以修改HTML文件，使其更加美观。 templates/home.html 不用重启，刷新页面： table { margin: auto; font-size: 80%; } "},{"title":"抓取网易云音乐热评","date":"2021-06-10T13:00:14.000Z","url":"/2021/06/10/%E6%8A%93%E5%8F%96%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E7%83%AD%E8%AF%84/","tags":[["python","/tags/python/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"查看页面源代码，发现源代码中并不含有热评。通过工具抓包查看请求，数据是通过后端数据库请求过来的。 查看标头可以发现，这条获取热评的请求传递的参数是加密的。 首先，我们要找到未加密的参数。 通过查看请求程序，可以看到当前网页调用了那些JS脚本执行过程，从下往上调用。 查看最上面的JS脚本，这是产生请求的最后一个脚本，看看这个脚本里面有什么。 点开后，咋一看没法理解，这是经过压缩后的JS脚本，没关系，点击左下方的&#123;&#125;优质打印，就是格式化该脚本。 通过查看JS脚本，发现send函数，设置断点。 重新刷新页面，可以看到所有的变量，找到request中的url为/weapi/cdns?csrf_token=，不是我们想要的地址，继续刷新， 直到出现，这就是我们想要的url。 从上图中可以看到，params参数是经过加密的，接着往上分析。 查看这里，d7e参数，查看这里的params依旧是加密的。接着往回找，是谁调用的b7g.bsO0x函数。 这里可以看到params参数依旧是加密的。 找到这里就可以看到params参数没有加密，说明加密过程就在 t7m.be7X函数中完成。 然后，把参数按照网易的规则进行加密处理，得到两个参数params和encSecKry。 最后，请求到网易，拿到评论信息。 table { margin: auto; font-size: 80%; } "},{"title":"Django入门-模型设计","date":"2021-06-09T13:29:50.000Z","url":"/2021/06/09/Django%E5%85%A5%E9%97%A8-%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1/","tags":[["python","/tags/python/"],["django","/tags/django/"]],"categories":[["Django","/categories/Django/"]],"content":"模型这些模型基本上代表了应用程序的数据库设计。在本节中要做的是创建 Django 所表示的类，这些类就是在上一节中建模的类∶ Board，Topic和 Post。User 模型被命名为内置应用叫 auth，它以命名空间 django.contrib.auth 的形式出现在 INSTALLED_APPS配置中。 我们要做的工作都在 boards/models.py 文件中。以下是我们在Django应用程序中如何表示类图的代码∶ 所有模型都是django.db.models.Model类的子类。每个类将被转换为数据库表。每个字段由 django.db.models.Field子类（内置在Django core）的实例表示，它们并将被转换为数据库的列。 字段 CharField，DateTimeField等等，都是 django.db.models.Field 的子类，包含在Django的核心里面-随时可以使用。 在这里，我们仅使用 CharField``，TextField ，DateTimeField，和ForeignKey字段来定义我们的模型。不过在Django提供了更广泛的选择来代表不同类型的数据，例如 IntegerField，BooleanField， DecimalField和其它一些字段。 我们会在需要的时候提及它们。 有些字段需要参数，例如CharField。我们应该始终设定一个max_length 。这些信息将用于创建数据库列。Django需要知道数据库列需要多大。该 max_length 参数也将被Django Forms API用来验证用户输入。 在 Board 模型定义中，更具体地说，在 name 字段中，我们设置了参数 unique=True ，顾名思义，它将强制数据库级别字段的唯一性。 在 Post 模型中， created_at 字段有一个可选参数， auto_now_add 设置为 True 。这将告诉Django创建 Post 对象时为当前日期和时间。 模型之间的关系使用 Foreignkey 字段。它将在模型之间创建一个连接，并在数据库级别创建适当的关系 （注∶ 外键关联）。该 Foreignkey 字段需要一个位置参数 related_name ，用于引用它关联的模型。（注∶ 例如 created_by 是外键字段，关联的User模型，表明这个帖子是谁创建的， related name=posts 表示在 User 那边可以使用 user.posts 来查看这个用户创建了哪些帖子） 例如，在 Topic 模型中， board 字段是 Board 模型的 ForeignKey 。它告诉Django，一个 Topic 实例只涉及一个Board实例。 related_name 参数将用于创建反向关系， Board 实例通过属性 topics 访问属于这个版块下的 Topic 列表。 Django自动创建这种反向关系， related_name 是可选项。但是，如果我们不为它设置一个名称，Django会自动生成它∶（class_name）_set 。例如，在 Board 模型中，所有 Topic 列表将用 topic_set 属性表示。而这里我们将其重新命名为了 topics ，以使其感觉更自然。 在 Post 模型中，该 updated_by 字段设置 related_name=&#39;+&#39;。这指示 Django我们不需要这种反向关系，所以它会被忽略（注∶也就是说我们不需要关系用户修改过哪些帖子）。 下面您可以看到类图和Django模型的源代码之间的比较，绿线表示我们如何处理反向关系。 迁移模型下一步是告诉Django创建数据库，以便我们可以开始使用它。打开终端，激活虚拟环境，转到 manage.py文件所在的文件夹，然后运行以下命令∶ 你会看到输出的内容是∶ 此时， Django 在 boards/migrations 目录创建了一个名为 0001_initial.py 的文件。它代表了应用程序模型的当前状态。在下一步，Django将使用该文件创建表和列。 迁移文件将被翻译成SQL语句。如果您熟悉SQL，则可以运行以下命令来检验将是要被数据库执行的SQL指令 如果你不熟悉SQL，也不要担心。所有的工作都将使用Django ORM来完成，它是一个与数据库进行通信的抽象层。下一步是将我们生成的迁移文件应用到数据库∶ 输出结果如下： 好了数据库就迁移成功了。 试验 Models API使用Python进行开发的一个重要优点是交互式shell。我一直在使用它。这是一种快速尝试和试验API的方法。 可以使用manage.py工具加载我们的项目来启动 Python shell ∶ 这与直接输入 python 指令来调用交互式控制台是非常相似的，除此之外，项目将被添加到 sys.path 并加载Django。这意味着我们可以在项目中导入我们的模型和其他资源并使用它。 让我们从导入Board类开始∶ 要创建新的 board 对象，我们可以执行以下操作∶ 为了将这个对象保存在数据库中，我们必须调用save方法∶ save 方法用于创建和更新对象。这里Django创建了一个新对象，因为这时 Board 实例没有id。第一次保存后，Django会自动设置ID∶ 您可以将其余的字段当做Python属性访问∶ 要更新一个值，我们可以这样做∶ 每个Django模型都带有一个特殊的属性；我们称之为模型管理器（Model Manager）。你可以通过属性 objects 来访问这个管理器，它主要用于数据库操作。例如，我们可以使用它来直接创建一个新的Board对象∶ 所以，现在我们有两个版块了。我们可以使用 objects 列出数据库中所有现有的版块 结果是一个QuerySet。稍后我们会进一步了解。基本上，它是从数据库中查询的对象列表。我们看到有两个对象，但显示的名称是 Board object。这是因为我们尚未实现 Board 的 __str___ 方法。 _str__ 方法是对象的字符串表示形式。我们可以使用版块的名称来表示它。 首先，退出交互式控制台∶ 现在编辑boards app中的 models.py 文件: 再进入交互台，查询： 这样是不是好多了呢。 我们可以将这个QuerySet看作一个列表。假设我们想遍历它并打印每个版块的描述∶ 同样，我们可以使用模型的 管理器（Manager）来查询数据库并返回单个对象。为此，我们要使用 get 方法∶ 但我们必须小心这种操作。如果我们试图查找一个不存在的对象，例如，查找id=3的版块，它会引发一个异常∶ get 方法的参数可以是模型的任何字段，但最好使用可唯一标识对象的字段来查询。否则，查询可能会返回多个对象，这也会导致异常。 请注意，查询区分大小写，小写”django”不匹配∶ table { margin: auto; font-size: 80%; } "},{"title":"Django入门-系统设计","date":"2021-06-09T01:56:24.000Z","url":"/2021/06/09/Django%E5%85%A5%E9%97%A8-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","tags":[["django","/tags/django/"]],"categories":[["Django","/categories/Django/"]],"content":"论坛项目在进入模型，视图等其它有趣的部分之前，先让我们花点时间，简要地讨论我们将要开发的这个项目。如果你已经有了Web 开发的经验并且觉得它太繁琐了，那么你可以浏览一下图片以了解我们将要构建的内容，然后直接跳转到本教程的模型部分。 用列图我们的项目是一个论坛系统，整个项目的构思是维护几个论坛版块（boards），每个版块像一个分类一样。在指定的版块里面，用户可以通过创建新主题（Topic）开始讨论，其他用户可以参与讨论回复。我们需要找到一种方法来区分普通用户和管理员用户，因为只有管理员可以创建版块。下图概述了主要的用例和每种类型的用户角色∶ 从用例图中，我们可以开始思考项目所需的实体类有哪些。这些实体就是我们要创建的模型，它与我们的Django应用程序处理的数据非常密切。 类图从用例图中，我们可以开始思考项目所需的实体类有哪些。这些实体就是我们要创建的模型，它与我们的Django应用程序处理的数据非常密切。 为了能够实现上面描述的用例，我们需要至少实现下面几个模型∶ Board， Topic，Post和User。 Board∶ 版块 Topic∶主题 Post∶帖子（译注∶ 其实就是主题的回复或评论） 类与类之间的实线告诉我们，在一个主题（Topic）中，我们需要有一个字段（译注∶其实就是通过外键来关联）来确定它属于哪个版块（Board）。同样，帖子（Post）也需要一个字段来表示它属于哪个主题，这样我们就可以列出在特定主题内创建的帖子。最后，我们需要一个字段来表示主题是谁发起的，帖子是谁发的。用户和版块之间也有联系，谁创建的版块。但是这些信息与应用程序无关。 现在我们的类图有基本的表现形式，我们还要考虑这些模型将承载哪些信息。这很容易让事情变得复杂，所以试着先把重要的内容列出来，这些内容是我们启动项目需要的信息。后面我们再使用 Django 的迁移（Migrations）功能来改进模型，您将在下一节中详细了解这些内容。但就目前而言，这是模型最基本的内容∶ 这个类图强调的是模型之间的关系，这些线条和箭头最终会在稍后转换为字段。 对于 Board 模型，我们将从两个字段开始∶ name 和 description。name 字段必须是唯一的，为了避免有重复的名称。 description 用于说明这个版块是做什么用的。 Topic 模型包括四个字段∶subject 表示主题内容，last update 用来定义话题的排序，starter 用来识别谁发起的话题，board 用于指定它属于哪个版块。 Post 模型有一个 message 字段，用于存储回复的内容，created at 在排序时候用 （最先发表的帖子排最前面），updated at 告诉用户是否更新了内容，同时，还需要有对应的 User 模型的引用，Post 由谁创建的和谁更新的。 最后是 User 模型。在类图中，我只提到了字段 username，password， email， is superuser 标志，因为这几乎是我们现在要使用的所有东西。 需要注意的是，我们不需要创建 User 模型，因为Django已经在contrib包中内置了User模型，我们将直接拿来用。 关于类图之间的对应关系（数字 1，0.* 等等），这里教你如何阅读∶ 一个 topic 必须与一个（1）Board（这意味着它不能为空）相关联，但是 Board 下面可能与许多个或者0个 topic 关联 （0.*）。这意味着 Board 下面可能没有主题。（译注∶ 一对多关系） 一个 Topic 至少有一个 Post（发起话题时，同时会发布一个帖子），并且它也可能有许多 Post（1.*）。一个Post必须与一个并且只有一个Topic（1）相关联。 一个 Topic必须有一个且只有一个User相关联，topic的发起者是（1）。而一个用户可能有很多或者没有 topic （0.*）。 Post 必须有一个并且只有一个与之关联的用户，用户可以有许多或没有 Post（0.*）。Post和User之间的第二个关联是直接关联（参见该行最后的箭头），就是 Post 可以被用户修改（updated_by），updated_by有可能是空（Post 没有被修改）。 画这个类图的另一种方法是强调字段而不是模型之间的关系∶ 上面的表示方式与前面的表示方式是对等的，不过这种方式更接近我们将要使用 Django Models API 设计的内容。在这种表示方式中，我们可以更清楚地看到，在 Post 模型中，关联了 Topic，created_by（创建者）和 updated by（更新者）字段。另一个值得注意的事情是，在 Topic 模型中，有一个名为 posts （） 的操作（一个类方法）。我们将通过反向关系来实现这一目标，Django 将自动在数据库中执行查询以返回特定主题的所有帖子列表。 table { margin: auto; font-size: 80%; } "},{"title":"spider的用法","date":"2021-06-07T16:49:45.000Z","url":"/2021/06/08/spider%E7%9A%84%E7%94%A8%E6%B3%95/","tags":[["scarpy","/tags/scarpy/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"Spider 的用法在 Scrapy 中，要抓取网站的链接配置、抓取逻辑、解析逻辑等其实都是在 Spider 中配置的。 Spider运行流程在实现 Scrapy 爬虫项目时，最核心的类便是 Spider 类了，它定义了如何爬取某个网站的流程和解析方式。简单来讲， Spider 要做的事就是如下两件： 定义爬取网站的动作； 分析爬取下来的网页。 对于 Spider 类来说，整个爬取循环如下所述。 以初始的 URL 初始化 Request ，并设置回调函数。 当 该 Request 成功请求并返回时，将生成 Response ，并作为参数传给该回调函数。 在回调函数内分析返回的网页内容。返回结果可以有两种形式，一种是解析到的有效结果返回字典或 Item 对象。下一步可经过处理后（或直接）保存，另一种是解析到的下一个（如下一页）链接，可以 利用此链接构造 Request 并设置新的回调函数，返回 Request 。 如果返回的是字典或 Item 对象，可通过 Feed Exports 等形式存入文件，如果设置了 Pipeline 的话，可以经由 Pipeline 处理（如过滤、修正等）并保存。 如果返回的是 Reqeust ，那么 Request 执行成功得到 Response 之后会再次传递给 Request 中定义的回调函数，可以再次使用选择器来分析新得到的网页内容，并根据分析的数据生成 Item 。 通过以上几步循环往复进行，便完成了站点的爬取。 Spider类分析Spider 继承自 scrapy.spiders.Spider ，这个类是最简单最基本的 Spider 类，每个其他的 Spider 必须继承自这个类，还有后面要说明的一些特殊 Spider 类也都是继承自它。 这个类里提供了 start_requests 方法的默认实现，读取并请求 start_urls 属性，并根据返回的结果调用 parse 方法解析结果。另外它还有一些基础属性，下面对其进行讲解。 name ：爬虫名称，是定义 Spider 名字的字符串。Spider 的名字定义了 Scrapy如何定位并初始化 Spider，所以其必须是唯一的。 不过我们可以生成多个相同的 Spider 实例，这没有任何限制。 name 是 Spider 最重要的属性，而且是必需的。如果该 Spider 爬取单个网站，一个常见的做法是以该网站的域名名称来命名 Spider。例如，如果 Spider 爬取 mywebsite.com，该 Spider 通常会被命名为 mywebsite。 allowed_domains：允许爬取的域名，是可选配置，不在此范围的链接不会被跟进爬取。 start_urls ：起始 URL 列表，当我们没有实现 start_requests 方法时，默认会从这个列表开始抓取。 custom_settings ：这是一个字典，是专属于本 Spider 的配置，此设置会覆盖项目全局的设置，而且此设置必须在初始化前被更新，所以它必须定义成类变量。 crawler ：此属性是由 from_crawler 方法设置的，代表的是本 Spider 类对应的 Crawler 对象， Crawler 对象中包含了很多项目组件，利用它我们可以获取项目的一些配置信息，如最常见的就是获取项目的设置信息，即 Settings 。 settings ：是一个 Settings 对象，利用它我们可以直接获取项目的全局设置变量。 除了一些基础属性， Spider 还有一些常用的方法，在此介绍如下。 start_requests ：此方法用于生成初始请求，它必须返回一个可迭代对象，此方法会默认使用 start_urls 里面的 URL 来构造 Request ，而且 Request 是 GET 请求方式。如果我们想在启动时以 POST 方式访问某个 站点，可以直接重写这个方法，发送 POST 请求时我们使用 FormRequest 即可。 parse ：当 Response 没有指定回调函数时，该方法会默认被调用，它负责处理 Response ，处理返回结果，并从中提取出想要的数据和下一步的请求，然后返回。该方法需要返回一个包含 Request 或 Item 的可迭代对象。 closed ：当 Spider 关闭时，该方法会被调用，在这里一般会定义释放资源的一些操作或其他收尾操作。 Selector 的用法Scrapy 还提供了自己的数据提取方法，即 Selector（选择器） 。 Selector 是基于 lxml 构建的，支持 XPath选择器 、 CSS选择器 ，以及 正则表达式 ，功能全面，解析速度和准确度非常高。 直接使用Selector 是一个可以独立使用的模块。我们可以直接利用 Selector 这个类来构建一个选择器对象，然后调用它的相关方法如 xpath、css 等来提取数据。 例如，针对一段 HTML代码，我们可以用如下方式构建 Selector 对象来提取数据： 这里我们没有在 Scrapy 框架中运行，而是把 Scrapy 中的 Selector 单独拿出来使用了，构建的时候传入 text 参数，就生成了一个 Selector 选择器对象，然后就可以像前面我们所用的 Scrapy 中的解析方式一样，调用 xpath、css 等方法来提取了。 在这里我们查找的是源代码中的 title 中的文本，在 XPath选择器 最后加 text 方法就可以实现文本的提取了。 Scrapy Shell由于 Selector 主要是与 Scrapy 结合使用，如 Scrapy 的回调函数中的参数 response 直接调用 xpath() 或者 css() 方法来提取数据，所以在这里我们借助 Scrapy Shell 来模拟 Scrapy 请求的过程，来讲解相关的提取方法。 我们用官方文档的一个样例页面来做演示：。 开启 Scrapy Shell，在命令行中输入如下命令： 这样我们就进入了 Scrapy Shell 模式。这个过程其实是 Scrapy 发起了一次请求，请求的 URL 就是刚才命令行下输入的 URL ，然后把一些可操作的变量传递给我们，如 request、response 等。 输出结果： 可以在命令行模式下输入命令调用对象的一些操作方法，回车之后实时显示结果。这与 Python 的命令行交互模式是类似的。 XPath选择器进入 Scrapy Shell 之后，我们将主要操作 response 变量来进行解析。因为我们解析的是 HTML 代码，Selector 将自动使用 HTML语法来分析。 response 有一个属性 selector，我们调用 response.selector 返回的内容就相当于用 response 的 text 构造了一个 Selector 对象。通过这个 Selector 对象我们可以调用解析方法如 xpath、css 等，通过向方法传入 XPath或 CSS 选择器参数就可以实现信息的提取。 我们用一个实例感受一下，如下所示： 打印结果的形式是 Selector 组成的列表，其实它是 SelectorList 类型， SelectorList 和 Selector 都可以继续调用 xpath 和 css 等方法来进一步提取数据。 在上面的例子中，我们提取了 a 节点。接下来，我们尝试继续调用 xpath 方法来提取 a 节点内包含的 img 节点，如下所示： 我们获得了 a 节点里面的所有 img节点，结果为 5。 选择器的最前方加 .（点），这代表提取元素内部的数据，如果没有加点，则代表从根节点开始提取。此处我们用了 ./img的提取方式，则代表从 a 节点里进行提取。如果此处我们用 //img，则还 是从 html节点里进行提取。 我们刚才使用了 response.selector.xpath方法对数据进行了提取。Scrapy提供了两个实用的快捷方法，response.xpath和 response.css，它们二者的功能完全等同于 response.selector.xpath和 response.selector.css。方便起 见，后面我们统一直接调用 response 的 xpath和 css 方法进行选择。 现在我们得到的是 SelectorList 类型的变量，该变量是由 Selector 对象组成的列表。我们可以用索引单独取出其中某个 Selector 元素，如下所示： 可以像操作列表一样操作这个 SelectorList 。但是现在获取的内容是 Selector 或者 SelectorList 类型，并不是真正的文本内容。那么具体的内容怎么提取呢？ 比如我们现在想提取出 a 节点元素，就可以利用 extract 方法，如下所示： 这里使用了 extract 方法，我们就可以把真实需要的内容获取下来。 还可以改写 XPath 表达式，来选取节点的内部文本和属性，如下所示： 只需要再加一层 /text() 就可以获取节点的内部文本，或者加一层 /@href就可以获取节点的 href属性。其中，@ 符号后面内容就是要获取的属性名称。 现在我们可以用一个规则把所有符合要求的节点都获取下来，返回的类型是列表类型。 但是这里有一个问题：如果符合要求的节点只有一个，那么返回的结果会是什么呢？我们再用一个实例来感受一下，如下所示： 用属性限制了匹配的范围，使 XPath只可以匹配到一个元素。然后用 extract 方法提取结果，其结果还是一个列表形式，其文本是列表的第一个元素。但很多情况下，我们其实想要的数据就是第一个元素内容，这里我们通过加一个索引来获取，如下所示： 但是，这个写法很明显是有风险的。一旦 XPath有问题，那么 extract 后的结果可能是一个空列表。如果我们再用索引来获取，那不就可能会导致数组越界吗？ 所以，另外一个方法可以专门提取单个元素，它叫作 extract_first。我们可以改写上面的例子如下所示： 直接利用 extract_first 方法将匹配的第一个结果提取出来，同时我们也不用担心数组越界的问题。也可以给extract_first()传递一个默认值，找不到的话会使用默认值。 CSS 选择器接下来，我们看看 CSS 选择器的用法。Scrapy的选择器同时还对接了 CSS 选择器，使用 response.css() 方法可以使用 CSS 选择器来选择对应的元素。 例如在上文我们选取了所有的 a 节点，那么 CSS 选择器同样可以做到，如下所示： 同样，调用 extract 方法就可以提取出节点，如下所示： 用法和 XPath选择是完全一样的。另外，我们也可以进行属性选择和嵌套选择，如下所示： 也可以使用 extract_first() 方法提取列表的第一个元素，如下所示： 接下来的两个用法不太一样。节点的内部文本和属性的获取是这样实现的，如下所示： 获取文本和属性需要用 ::text 和 ::attr() 的写法。而其他库如 BeautifulSoup 或 PyQuery 都有单独的方法。 CSS 选择器和 XPath选择器一样可以嵌套选择。我们可以先用 XPath选择器选中所有 a 节点，再利用 CSS 选择器选中 img节点，再用 XPath选择器获取属性。我们用一个实例来感受一下，如下所示： 可以随意使用 xpath 和 css 方法二者自由组合实现嵌套查询，二者是完全兼容的。 正则匹配Scrapy的选择器还支持正则匹配。比如，在示例的 a 节点中的文本类似于 Name:My image 1，现在我们只想把 Name:后面的内容提取出来，这时就可以借助 re 方法，实现如下： 给 re 方法传入一个正则表达式，其中 (.*) 就是要匹配的内容，输出的结果就是正则表达式匹配的分组，结果会依次输出。 如果同时存在两个分组，那么结果依然会被按序输出，如下所示： 类似 extract_first 方法，re_first 方法可以选取列表的第一个元素，用法如下： 不论正则匹配了几个分组，结果都会等于列表的第一个元素。 值得注意的是， response 对象不能直接调用 re 和 re_first 方法。如果想要对全文进行正则匹配，可以先调用 xpath 方法然后再进行正则匹配，如下所示： table { margin: auto; font-size: 80%; } "},{"title":"django入门-环境搭建","date":"2021-06-07T07:56:49.000Z","url":"/2021/06/07/django%E5%85%A5%E9%97%A8%E5%8F%8A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","tags":[["python","/tags/python/"],["django","/tags/django/"]],"categories":[["Django","/categories/Django/"]],"content":"为什么要使用Django?Django是一个用python编写的Web框架。Web框架是一种软件，基于web框架可以开发动态网站，各种应用程序以及服务。它提供了一系列工具和功能，可以解决许多与Web开发相关的常见问题，比如∶ 安全功能，数据库访问，会话，模板处理，URL路由，国际化，本地化，等等。 使用诸如 Django 之类的网络框架，使我们能够以标准化的方式快速开发安全可靠的Web应用程序，而无需重新发明轮子。 那么，Django有什么特别之处呢? 对于初学者来说，它是一个Python Web框架，这意味着你可以受益于各种各样的开源库包。如果当你想要解决一个特定的问题的时候，可能有人已经为它实现了一个库来供你使用。Django是用python编写的最流行的web框架之一。它绝对是最完整的，提供了各种各样的开箱即用的功能，比如用于开发和测试的独立Web服务器，缓存，中间件系统，ORM，模板引擎，表单处理，基于Python单元测试的工具接口。Django还自带内部电池，提供内置应用程序，比如一个认证系统，一个可用于 cRuD （增删改查）操作并且自动生成页面的后台管理界面，生成订阅文档（RSS/Atom），站点地图等。甚至在django中内建了一个地理信息系统（GIS）框架。 安装通过虚拟环境安装，先安装虚拟环境。 创建虚拟环境并激活，在虚拟环境下安装django。 如果要退出虚拟环境，运行deactive就退出了虚拟环境。 创建一个项目 执行上面的命令后，会在目录下生成基础文件夹结构。 最初项目由五个文件构成： manage.py ：使用 django-admin 命令行工具得快捷方式。用于运行与项目相关得管理命令。 __init__.py：告诉python这个文件夹是一个python包 seetings.py：这个文件包含了所有得项目配置。 urls.py：这个文件负责映射项目中得路由和路径。 wsgi.py：该文件是用于部署得简单网关接口。 django自带了一个简单得网络服务器。在开发过程中无需安装其他软件即可在本地运行项目，通过执行命令测试一些。 访问，你可以看到下面页面。 使用ctrl+c来终止服务器。 Django应用在Django得哲学中，有两个重要概念： app：是一个可以完成做某件事情得web应用程序。一个应用程序通常由一组models（数据库表），views（视图），templates（模板），tests（测试）组成。 project：是配置和应用程序得集合。一个项目可以由多个应用程序或一个应用程序组成。 创建一个应用： 执行上面的命令后，会在myproject/目录下生成boards文件夹。 文件作用介绍： migrations/：在这个文件夹里，Django会存储一些文件跟踪models文件中创建得变更，用来保持数据库和models.py的同步。 admin.py：这个文件为一个django内置的应用程序Django Admin的配置文件。 apps.py：应用程序本身的配置文件。 models.py：定义web应用程序数据实列的地方，models会由Django自动转为数据库表。 tests.py：这个文件用来写当前应用程序的单元测试。 views.py：处理web应用程序请求（request）/响应（response）周期的文件。 现在创建了第一个应用程序，来配置下项目以便启用这个应用程序。 在settings文件中INSTALLED_APPS中添加这个应用： 创建视图现在来写第一个视图（view），打开boards应用程序中的views.py文件，并添加以下代码： 视图是接收httprequest对象并返回一个httpresponse对象的python函数。接收request作为参数并返回response作为结果。 这里定义了一个简单的视图，命名为home，它只是简单返回一个信息，一个字符串hello world。 接下来需要告诉Django什么时候会调用这个view。在boards文件夹下创建urls.py文件并添加以下内容中添加： 下一步是要在根 URLconf 文件中指定我们创建的 boards.urls模块。在 myproject/urls.py 文件的 urlpatterns 列表里插入一个 include()， 如下： 函数 include() 允许引用其它 URLconfs。每当 Django 遇到 include() 时，它会截断与此项匹配的 URL 的部分，并将剩余的字符串发送到 URLconf 以供进一步处理。 访问，就会返回hello world。 table { margin: auto; font-size: 80%; } "},{"title":"XPath库解析","date":"2021-06-06T06:44:22.000Z","url":"/2021/06/06/XPath%E5%BA%93%E8%A7%A3%E6%9E%90/","tags":[["python","/tags/python/"],["xpath","/tags/xpath/"],["lxml","/tags/lxml/"]],"categories":[["Python标准库","/categories/Python%E6%A0%87%E5%87%86%E5%BA%93/"]],"content":"XPath，全称 XML Path Language，即 XML 路径语言，它是一门在XML文档中查找信息的语言。XPath 最初设计是用来搜寻XML文档的，HTML是XML的子集，它同样适用于HTML文档的搜索。 安装 XPath 术语节点（Node）在 XPath 中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档节点（或称为根节点）。 请看下面这个 XML 文档： 上面的XML文档中的节点例子： 基本值（或称原子值，Atomic value）基本值是无父或无子的节点。 基本值的例子： 项目（Item）项目是基本值或者节点。 节点关系父（Parent）每个元素以及属性都有一个父。 在下面的例子中，book 元素是 title、author、year 以及 price 元素的父： 子（Children）元素节点可有零个、一个或多个子。 在下面的例子中，title、author、year 以及 price 元素都是 book 元素的子： 同胞（Sibling）拥有相同的父的节点 在下面的例子中，title、author、year 以及 price 元素都是同胞： 先辈（Ancestor）某节点的父、父的父，等等。 在下面的例子中，title 元素的先辈是 book 元素和 bookstore 元素： 后代（Descendant）某个节点的子，子的子，等等。 在下面的例子中，bookstore 的后代是 book、title、author、year 以及 price 元素： XPath 语法目标实列： 选取节点XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。 下面列出了最有用的路径表达式： 表达式 描述 nodename 选取此节点的所有子节点。 / 从根节点选取。 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 . 选取当前节点。 .. 选取当前节点的父节点。 @ 选取属性。 实例 在下面的表格中，列出了一些路径表达式以及表达式的结果： 路径表达式 结果 bookstore 选取 bookstore 元素的所有子节点。 /bookstore 选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ bookstore/book 选取属于 bookstore 的子元素的所有 book 元素。 //book 选取所有 book 子元素，而不管它们在文档中的位置。 bookstore//book 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。 //@lang 选取名为 lang 的所有属性。 谓语（Predicates）谓语用来查找某个特定的节点或者包含某个指定的值的节点。 谓语被嵌在方括号中。 实例在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果： 路径表达式 结果 /bookstore/book[1] 选取属于 bookstore 子元素的第一个 book 元素。 /bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素。 /bookstore/book[last()-1] 选取属于 bookstore 子元素的倒数第二个 book 元素。 /bookstore/book[position()&lt;3] 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。 //title[@lang] 选取所有拥有名为 lang 的属性的 title 元素。 //title[@lang=’eng’] 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。 /bookstore/book[price&gt;35.00] 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 /bookstore/book[price&gt;35.00]/title 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 选取未知节点XPath 通配符可用来选取未知的 XML 元素。 通配符 描述 * 匹配任何元素节点。 @* 匹配任何属性节点。 node() 匹配任何类型的节点。 实例 在下面的表格中，列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 /bookstore/* 选取 bookstore 元素的所有子元素。 //* 选取文档中的所有元素。 //title[@*] 选取所有带有属性的 title 元素。 选取若干路径通过在路径表达式中使用“|”运算符，您可以选取若干个路径。 实例在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果： 路径表达式 结果 //book/title | //book/price 选取 book 元素的所有 title 和 price 元素。 //title | //price 选取文档中的所有 title 和 price 元素。 /bookstore/book/title | //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 XPath 实例对xml文档的匹配。 对html文档的匹配。 引入xlxml模块， 从文件中导入html。 通过字符串转换成html格式利用etree.HTML解析字符串，将字符串解析从html格式的文件， 经过处理后，部分缺失的节点可以自动修复，并且还自动添加了 body、html 节点。 所有节点用以 // 开头的 XPath 规则来选取所有符合要求的字符串： 绝对路径查找 获取某个标签的内容 注意，获取a标签的所有内容，a后面就不用再加正斜杠，否则报错。 或者 打印指定路径下a标签的属性 这里可以通过遍历拿到某个属性的值，查找标签的内容，通过@属性名获取。 获取指定标签对应属性值的内容 使用xpath拿到得都是一个个的ElementTree对象，如果需要查找内容的话，还需要遍历拿到数据的列表。 相对路径查找（常用） 查找所有li标签下的a标签内容 查找一下l相对路径下li标签下的a标签下的href属性的值 注意，a标签后面需要双//。 查找a标签下属性href值为link5.html的内容。 查找a标签下属性href值为””的内容。 子节点通过 / 或 // 即可查找元素的子节点或子孙节点，选择 li 节点的所有直接 a 子节点xpath为：//li/a。 父节点知道子节点，查询父节点可以用 .. 来实现。 属性匹配匹配时可以用@符号进行属性过滤，匹配li下属性class为item-5的内容。 文本获取有两种方法：一是获取文本所在节点后直接获取文本，二是使用 //。第二种方法会获取到补全代码时换行产生的特殊字符，推荐使用第一种方法，可以保证获取的结果是整洁的。 属性获取@符号相当于过滤器，可以直接获取节点的属性值。 属性多值匹配某些节点的某个属性可能有多个值： 多属性匹配 函数查找最后一个li标签里的a标签的href属性（last()函数） table { margin: auto; font-size: 80%; } "},{"title":"优美图库抓取","date":"2021-06-06T05:56:08.000Z","url":"/2021/06/06/%E4%BC%98%E7%BE%8E%E5%9B%BE%E5%BA%93%E6%8A%93%E5%8F%96/","tags":[["python","/tags/python/"],["bs4","/tags/bs4/"],["requests","/tags/requests/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"抓取思路1.拿到主页面的源代码，然后提取子页面的链接接地址，href2.通过href拿到子页面的内容，从子页面中找到图片的下载地址 img -&gt; src3.下载图片并保存 实现代码如下： 没有做进一步的爬取，只爬取了一个页面。 所有图片保存到文件夹images。 table { margin: auto; font-size: 80%; } "},{"title":"Beautiful Soup4库","date":"2021-06-05T08:15:33.000Z","url":"/2021/06/05/Beautiful-Soup%E5%BA%93/","tags":[["python","/tags/python/"],["bs4","/tags/bs4/"]],"categories":[["Python标准库","/categories/Python%E6%A0%87%E5%87%86%E5%BA%93/"]],"content":"Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库。 安装 Beautiful Soup支持Python标准库中的HTML解析器,还支持一些第三方的解析器,其中一个是 lxml .根据操作系统不同,可以选择下列方法来安装lxml: 使用示例HTML； 使用BeautifulSoup解析这段代码,能够得到一个 BeautifulSoup 的对象,并能按照标准的缩进格式的结构输出: output: 几个简单的浏览结构化数据的方法: 从文档中找到所有标签的链接: 从文档中获取所有文字内容: 对象的种类Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种: Tag , NavigableString , BeautifulSoup , Comment . TagTag 对象与XML或HTML原生文档中的tag相同: Name每个tag都有自己的名字,通过 .name 来获取: 如果改变了tag的name,那将影响所有通过当前Beautiful Soup对象生成的HTML文档: Attributes一个tag可能有很多个属性. tag &lt;b class=&quot;boldest&quot;&gt; 有一个 “class” 的属性,值为 “boldest” . tag的属性的操作方法与字典相同: 也可以直接”点”取属性, 比如: .attrs : tag的属性可以被添加,删除或修改. 再说一次, tag的属性操作方法与字典一样 多值属性还有一些属性 rel , rev , accept-charset , headers , accesskey . 在Beautiful Soup``中多值属性的返回类型是list: 如果某个属性看起来好像有多个值,但在任何版本的HTML定义中都没有被定义为多值属性,那么Beautiful Soup会将这个属性作为字符串返回。 将tag转换成字符串时,多值属性会合并为一个值。 如果转换的文档是XML格式,那么tag中不包含多值属性 table { margin: auto; font-size: 80%; } "},{"title":"re正则表达式操作","date":"2021-06-04T14:02:30.000Z","url":"/2021/06/04/re%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%93%8D%E4%BD%9C/","tags":[["python","/tags/python/"],["re","/tags/re/"]],"categories":[["Python标准库","/categories/Python%E6%A0%87%E5%87%86%E5%BA%93/"]],"content":"Regular Expression，正则表达式，使用表达式的方式对字符串进行匹配的语法规则。 正则表达式语法一个正则表达式指定了一集与之匹配的字符串；模块内的函数可以让你检查某个字符串是否跟给定的正则表达式匹配。 要看正则写得对不对，可以去测试网站测试匹配结果。 首先了解元字符的概念：具有固定含义的特殊符号。 特殊字符有： 模块内容模块定义了几个函数，常量，和一个例外。 re.compile re.compile将正则表达式的样式编译为一个 正则表达式对象（正则对象），可以用于匹配，通过这个对象的方法 match(), search() 。 比如： 使用 re.compile() 保存正则对象以便复用，可以让程序更加高效。 re.match 如果 string 开始的0或者多个字符匹配到了正则表达式样式，就返回一个相应的 匹配对象 。 如果没有匹配，就返回 None ；注意它跟零长度匹配是不同的。 re.search 扫描整个 字符串 找到匹配样式的第一个位置，并返回一个相应的 匹配对象 。如果没有匹配，就返回一个 None ； re.split 用 pattern 分开 string 。 如果在 pattern 中捕获到括号，那么所有的组里的文字也会包含在列表里。如果 maxsplit 非零， 最多进行 maxsplit 次分隔， 剩下的字符全部返回到列表的最后一个元素。 re.findall 对 string 返回一个不重复的 pattern 的匹配列表， string 从左到右进行扫描，匹配按找到的顺序返回。如果样式里存在一到多个组，就返回一个组合列表；就是一个元组的列表（如果样式里有超过一个组合的话）。空匹配也会包含在结果里。 re.finditer pattern 在 string 里所有的非重复匹配，返回为一个迭代器 iterator 保存了 匹配对象 。 string 从左到右扫描，匹配按顺序排列。空匹配也包含在结果里。 匹配对象匹配对象总是有一个布尔值 True。如果没有匹配的话 match() 和 search() 返回 None 所以你可以简单的用 if 语句来判断是否匹配 匹配对象支持以下方法和属性： Match.group 返回一个或者多个匹配的子组。如果只有一个参数，结果就是一个字符串，如果有多个参数，结果就是一个元组（每个参数对应一个项），如果没有参数，组1默认到0（整个匹配都被返回）。如果一个组N 参数值为 0，相应的返回值就是整个匹配字符串；如果它是一个范围 [1..99]，结果就是相应的括号组字符串。 如果正则表达式使用了 (?P&lt;name&gt;…) 语法， groupN 参数就也可能是命名组合的名字。 命名组合同样可以通过索引值引用。 如果一个组匹配成功多次，就只返回最后一个匹配。 Match.groups 返回一个元组，包含所有匹配的子组，在样式中出现的从1到任意多的组合。 default 参数用于不参与匹配的情况，默认为 None。 table { margin: auto; font-size: 80%; } "},{"title":"经典豆瓣TOP250爬取","date":"2021-06-04T13:34:51.000Z","url":"/2021/06/04/%E7%BB%8F%E5%85%B8%E8%B1%86%E7%93%A3TOP250%E7%88%AC%E5%8F%96/","tags":[["re","/tags/re/"],["requests","/tags/requests/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"经典案例每个学习爬虫的人都爬过的豆瓣电影TOP250，先看下长什么样。 开爬。 分析查看源代码，所有的数据都在页面源代码中，直接用requests请求拿过来页面源代码，再通过re匹配即可。 保存到data.csv "},{"title":"基于docker安装redis","date":"2021-06-04T11:11:27.000Z","url":"/2021/06/04/%E5%9F%BA%E4%BA%8Edocker%E5%AE%89%E8%A3%85redis/","tags":[["docker","/tags/docker/"],["redis","/tags/redis/"]],"categories":[["Docker","/categories/Docker/"]],"content":"获取redis镜像安装环境，腾讯云。 不加版本号默认获取最新版本，也可以使用 docker search redis 查看镜像来源。 查看本地镜像 配置官网获取redis配置文件: 修改默认的配置文件 使用xftp登录服务器，在/root/redis/data 创建好文件夹用于存放redis数据，这个文件夹位置可自己设定。然后将配置好的redis.conf文件复制入/root/redis/文件夹下。 运行 启动命令解释如下： "},{"title":"asyncio异步I/O模块","date":"2021-06-02T23:40:51.000Z","url":"/2021/06/03/asyncio%E5%BC%82%E6%AD%A5I-O%E6%A8%A1%E5%9D%97/","tags":[["python","/tags/python/"],["asyncio","/tags/asyncio/"]],"categories":[["Python标准库","/categories/Python%E6%A0%87%E5%87%86%E5%BA%93/"]],"content":"asyncio 是用来编写并发代码的库，使用 async/await 语法。 asyncio 被用作多个提供高性能 Python 异步框架的基础，包括网络和网站服务，数据库连接库，分布式任务队列等等。 asyncio 往往是构建 IO 密集型和高层级 结构化 网络代码的最佳选择。 asyncio 提供一组 高层级 API 用于: 并发地运行 Python 协程 并对其执行过程实现完全控制; 执行网络IO和IPC; 控制子进程; 通过队列 实现分布式任务; 同步并发代码; 此外，还有一些 低层级 API 以支持库和框架的开发者实现: 创建和管理事件循环，以提供异步 API 用于网络化, 运行子进程，处理 OS信号 等等; 使用 transports 实现高效率协议; 通过 async/await 语法 桥接 基于回调的库和代码。 协程与任务协程协程 通过 async/await 语法进行声明，是编写 asyncio 应用的推荐方式。 输出hello后停止1秒后输出world。 直接调用协程并不会执行，比如这里直接main()是运行不了的，吃屎的main不是函数而是一个协程对象。 运行协程，有三种机制： asyncio.run()函数来运行最高层级的入口点“main”函数。 等待一个协程，下面的代码等待1秒后输出hello，再等待2秒后输出world。 输出结果： asyncio.create_task()函数用来并发运行作为 asyncio 任务的多个协程。 输出结果，注意，输出显示代码段的运行时间比之前快了 1 秒:： 可等待对象如果一个对象可以在 await 语句中使用，那么它就是 可等待 对象。 可等待 对象有三种主要类型: 协程, 任务 和 Future。 协程Python 协程属于 可等待 对象，因此可以在其他协程中被等待: 重要 “协程” 可用来表示两个紧密关联的概念: 协程函数: 定义形式为 async def 的函数; 协程对象: 调用 协程函数 所返回的对象。 任务任务 被用来“并行的”调度协程 当一个协程通过 asyncio.create_task() 等函数被封装为一个 任务，该协程会被自动调度执行: 运行 asyncio 程序asyncio.run(coro, *, debug=False) 执行 coroutine coro 并返回结果。 此函数会运行传入的协程，负责管理 asyncio 事件循环，终结异步生成器，并关闭线程池。 当有其他 asyncio 事件循环在同一线程中运行时，此函数不能被调用。 如果 debug 为 True，事件循环将以调试模式运行。 此函数总是会创建一个新的事件循环并在结束时关闭之。它应当被用作 asyncio 程序的主入口点，理想情况下应当只被调用一次。 示例: 创建任务asyncio.create_task(coro, *, name=None) 将 coro 协程 封装为一个 Task 并调度其执行。返回 Task 对象。 name 不为 None，它将使用 Task.set_name() 来设为任务的名称。 该任务会在 get_running_loop() 返回的循环中执行，如果当前线程没有在运行的循环则会引发 RuntimeError。 此函数 在 Python 3.7 中被加入。在 Python 3.7 之前，可以改用低层级的 asyncio.ensure_future() 函数。 休眠coroutine asyncio.sleep(delay, result=None, *, loop=None) 阻塞 delay 指定的秒数。 如果指定了 result，则当协程完成时将其返回给调用者。 sleep() 总是会挂起当前任务，以允许其他任务运行。 将 delay 设为 0 将提供一个经优化的路径以允许其他任务运行。 这可供长期间运行的函数使用以避免在函数调用的全过程中阻塞事件循环。 输出结果： 并发运行任务awaitable asyncio.gather(*aws, loop=None, return_exceptions=False) 并发 运行 aws 序列中的 可等待对象。 如果 aws 中的某个可等待对象为协程，它将自动被作为一个任务调度。 如果所有可等待对象都成功完成，结果将是一个由所有返回值聚合而成的列表。结果值的顺序与 aws 中可等待对象的顺序一致。 如果 return_exceptions 为 False (默认)，所引发的首个异常会立即传播给等待 gather() 的任务。aws 序列中的其他可等待对象 不会被取消 并将继续运行。 如果 return_exceptions 为 True，异常会和成功的结果一样处理，并聚合至结果列表。 如果 gather() 被取消，所有被提交 (尚未完成) 的可等待对象也会 被取消。 如果 aws 序列中的任一 Task 或 Future 对象 被取消，它将被当作引发了 CancelledError 一样处理 – 在此情况下 gather() 调用 不会 被取消。这是为了防止一个已提交的 Task/Future 被取消导致其他 Tasks/Future 也被取消。 "},{"title":"Pyppeteer的使用","date":"2021-06-02T06:04:50.000Z","url":"/2021/06/02/Pyppeteer%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":[["python","/tags/python/"],["pypprteer","/tags/pypprteer/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"Selenium功能虽热强大，但还是有些不方便。 Pyppeteer介绍Puppeteer 是 Google 基于 Node.js 开发的一个工具，有了它我们可以通过 JavaScript 来控制 Chrome 浏览器的一些操作，当然也可以用作网络爬虫上，其 API 极其完善，功能非常强大。 Pyppeteer 是 Puppeteer 的 Python版本的实现，但它不是 Google 开发的，是一位来自于日本的工程师依据 Puppeteer 的一些功能开发出来的非官方版本。 Pyppeteer 是依赖于 Chromium这个浏览器来运行的。那么有了 Pyppeteer 之后，我们就可以免去那些烦琐的环境配置等问题。如果第一次运行的时候，Chromium浏览器没有安装，那么程序会帮我们自动安装和配置，就免去了烦琐的环境配置等工作。 安装运行环境python 3.7以上。 查看安装路径 output: 使用Pyppeteer 是一款非常高效的 web 自动化测试工具，由于 Pyppeteer 是基于 asyncio 构建的，它的所有属性和方法几乎都是 coroutine (协程) 对象，因此在构建异步程序的时候非常方便，天生就支持异步运行。 程序构建的基本思路是新建一个 browser 浏览器 和 一个 页面 page。 运行上面这段代码会发现并没有浏览器弹出运行，这是因为 Pyppeteer 默认使用的是无头浏览器，如果想要浏览器显示，需要在launch 函数中设置参数 “headless = False”，程序运行结束后在同一目录下会出现截取到的网页图片： 有个问题就是图片显示不全，网站内容没有办法完全铺满。 报错报错 OSError: Unable to remove Temporary User Data启动浏览器时指定参数 userDataDir 存放缓存，保证硬盘够大且不是系统盘。 移除Chrome正受到自动测试软件的控制 全屏 页面内容Page.content()或Page.evalute() 把整个页面的文字获取下来了。 异步运行asyncio.wait() 或 asyncio.gather()，建议只用在一次性读取的页面，需要滚动的不建议使用。 反爬虫检测访问 。 "},{"title":"logging日志模块","date":"2021-06-02T01:34:05.000Z","url":"/2021/06/02/logging%E6%97%A5%E5%BF%97%E6%A8%A1%E5%9D%97/","tags":[["logging","/tags/logging/"]],"categories":[["Python标准库","/categories/Python%E6%A0%87%E5%87%86%E5%BA%93/"]],"content":"基础教程日志级别 级别 使用场景 DEBUG 细节信息，仅当诊断问题时适用 INFO 程序按预期运行 WARNING 表明有已经或即将发生的意外 ERROR 由于严重的问题，程序的某些功能已经不能正常执行 CRITICAL 严重的错误，表明程序已经不能继续执行 默认是WARNING级别，意味着只会追踪该级别以及以上的事件，除非修改日志配置。 simple example: output: INFO信息并没有出现，因为默认级别是WARNING级别。 记录日志到文件 日志信息会记录到exampe.log中，3.9版本以后增加了encoding参数，在之前的版本或者没有指定时，编码会使用open()使用的默认值。 对 basicConfig() 的调用应该在 debug() ， info() 等的前面。因为它被设计为一次性的配置，只有第一次调用会进行操作，随后的调用不会产生有效操作。 如果多次运行上述脚本，则连续运行的消息将追加到文件 example.log 。 如果你希望每次运行重新开始，而不是记住先前运行的消息，则可以通过将上例中的调用更改为来指定 filemode 参数: 记录变量数据使用格式字符作为事件描述信息，附加传入变量数据作为参数。 output: 更改显示消息的格式example: output: 在消息中显示日期/时间显示事件的日期和时间，在格式字符串中使用&#39;%(asctime)s&#39;。 output: datafmt参数可以控制时间的格式。 output: datefmt 参数的格式与 time.strftime() 支持的格式相同。 进阶教程日志库采用模块化的方法，提供了几类组件：记录器、处理器、过滤器和格式器。 记录器暴露了应用程序代码直接使用的接口。 处理器将日志记录（由记录器创建）发送到适当的目标。 过滤器提供了更精细的附加功能，用于确定要输出的日志记录。 格式器指定最终输出中日志记录的样式。 通过调用 Logger 类（以下称为 loggers ， 记录器）的实例来执行日志记录。 每个实例都有一个名称，它们在概念上以点（句点）作为分隔符排列在命名空间的层次结构中。 例如，名为 ‘scan’ 的记录器是记录器 ‘scan.text’ ，’scan.html’ 和 ‘scan.pdf’ 的父级。 记录器名称可以是你想要的任何名称，并指示记录消息源自的应用程序区域。 良好实践是在使用日志记录模块中使用模块记录器，命名如下： 记录器层次结构的根称为根记录器。 这是函数 debug() 、 info() 、 warning() 、 error() 和 critical() 使用的记录器，它们就是调用了根记录器的同名方法。函数和方法具有相同的签名。根记录器的名称在输出中打印为 &#39;root&#39; 。 可以将消息记录到不同的地方。 软件包中的支持包含，用于将日志消息写入文件、 HTTP GET/POST 位置、通过 SMTP 发送电子邮件、通用套接字、队列或特定于操作系统的日志记录机制（如 syslog 或 Windows NT 事件日志）。 目标由 handler 类提供。 如果你有任何内置处理器类未满足的特殊要求，则可以创建自己的日志目标类。 默认情况下，没有为任何日志消息设置目标。 你可以使用 basicConfig() 指定目标（例如控制台或文件），如教程示例中所示。 如果你调用函数 debug() 、 info() 、 warning() 、 error() 和 critical() ，它们将检查是否有设置目标；如果没有设置，将在委托给根记录器进行实际的消息输出之前设置目标为控制台（ sys.stderr ）并设置显示消息的默认格式。 由 basicConfig() 设置的消息默认格式为： 可以通过使用 format 参数将格式字符串传递给 basicConfig() 来更改此设置。 记录流程记录器和处理器中的日志事件信息流程如下图所示。 记录器Logger 对象有三重任务。首先，它们向应用程序代码公开了几种方法，以便应用程序可以在运行时记录消息。其次，记录器对象根据严重性（默认过滤工具）或过滤器对象确定要处理的日志消息。第三，记录器对象将相关的日志消息传递给所有感兴趣的日志处理器。 这些是最常见的配置方法： Logger.setLevel() 指定记录器将处理的最低严重性日志消息，其中 debug 是最低内置严重性级别， critical 是最高内置严重性级别。 例如，如果严重性级别为 INFO ，则记录器将仅处理 INFO 、 WARNING 、 ERROR 和 CRITICAL 消息，并将忽略 DEBUG 消息。 Logger.addHandler() 和 Logger.removeHandler() 从记录器对象中添加和删除处理器对象。处理器在以下内容中有更详细的介绍 处理器 。 Logger.addFilter() 和 Logger.removeFilter() 可以添加或移除记录器对象中的过滤器。 过滤器对象 包含更多的过滤器细节。 处理器Handler 对象负责将适当的日志消息（基于日志消息的严重性）分派给处理器的指定目标。 Logger 对象可以使用 addHandler() 方法向自己添加零个或多个处理器对象。作为示例场景，应用程序可能希望将所有日志消息发送到日志文件，将错误或更高的所有日志消息发送到标准输出，以及将所有关键消息发送至一个邮件地址。 此方案需要三个单独的处理器，其中每个处理器负责将特定严重性的消息发送到特定位置。 常用的配置方法： setLevel() 方法，就像在记录器对象中一样，指定将被分派到适当目标的最低严重性。为什么有两个 setLevel() 方法？记录器中设置的级别确定将传递给其处理器的消息的严重性。每个处理器中设置的级别确定该处理器将发送哪些消息。 setFormatter() 选择一个该处理器使用的 Formatter 对象。 addFilter() 和 removeFilter() 分别在处理器上配置和取消配置过滤器对象。 格式器格式化器对象配置日志消息的最终顺序、结构和内容。与 logging.Handler 类不同，应用程序代码可以实例化格式器类，但如果应用程序需要特殊行为，则可能会对格式化器进行子类化定制。构造函数有三个可选参数 —— 消息格式字符串、日期格式字符串和样式指示符。 logging.Formatter.__init__(fmt=None, datefmt=None, style=&#39;%&#39;) 如果没有消息格式字符串，则默认使用原始消息。如果没有日期格式字符串，则默认日期格式为： 最后加上毫秒数。 style 是 ％，&#39;&#123;&#39; 或 &#39;$&#39; 之一。 如果未指定，则将使用 &#39;％&#39;。 在 3.2 版更改: 添加 style 形参。 以下消息格式字符串将以人类可读的格式记录时间、消息的严重性以及消息的内容，按此顺序: 格式器通过用户可配置的函数将记录的创建时间转换为元组。 默认情况下，使用 time.localtime()；要为特定格式器实例更改此项，请将实例的 converter 属性设置为与 time.localtime() 或 time.gmtime() 具有相同签名的函数。 配置日志记录开发者可以通过三种方式配置日志记录： 使用调用上面列出的配置方法的 Python 代码显式创建记录器、处理器和格式器。 创建日志配置文件并使用 fileConfig() 函数读取它。 创建配置信息字典并将其传递给 dictConfig() 函数。 具体可参考官方文档。 table { margin: auto; font-size: 80%; } "},{"title":"南瑞103协议详解","date":"2021-06-01T07:09:53.000Z","url":"/2021/06/01/%E5%8D%97%E7%91%9E103%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/","tags":[["pscada","/tags/pscada/"]],"categories":[["自动化","/categories/%E8%87%AA%E5%8A%A8%E5%8C%96/"]],"content":"前言IEC60870-5-103 传输规约是1999年10月1日实施的电力行业标准（国内称为DL/T667－1999继电保护设备信息接口配套标准），规约要求采用此规约的监控和保护装置必须符合该标准的所有强制性定义。 深圳所IEC-103是该标准的一个子集，主要目的在于明确应用层功能，满足IEC60870-5-103的所有强制性定义，如果该子集与IEC60870-5-103相冲突，则以IEC60870-5-103为准。 缩略语ASDU（Application Service Data Unit）应用服务数据单元COT（Cause Of Transmission）传动原因DCO（Double COmmand)双命令H（Hex）十六进制KOD（Kind Of Description）命令的描述类型TOO（Type Of Order）命令类型 IEC-103规约说明IEC60870-5-103规约描述了两种信息交换的方法，第一种方法是基于严格规定的应用服务数据单元和为传输 “标准化” 报文的应用过程， 第二种方法使用了通用分类服务， 以传输几乎所有可能的信息。IEC-103( 深圳所 )两种方法都采用。利用第一种方法来实现校时、信号复归、传输保护事件、告警、状态量和故障录波数据；利用第二种方法来实现定值（包括修改定值） 、测量值、软压板（控制字） （包括修改软压板（控制字））的传输。 下面是其详细列表 (括号中是涉及到的主要长帧报文)。 详细列表 保护动作事件 (ASDU2) 保护告警信息 (ASDU1) 保护状态信息 (ASDU1) 故障录波(ASDU23/ASDU24/ASDU25/ASDU26/ASDU27/ASDU28/ASDU29/ASDU30/ASDU31) 读取定值 (通用分类服务 ,ASDU10,ASDU21) 修改定值 (通用分类服务 ,ASDU10) 读取测量值 (通用分类服务 , ASDU10,ASDU21) 读取软压板（控制字） (通用分类服务 , ASDU10,ASDU21) 修改软压板（控制字） (通用分类服务 ,ASDU10) 校时 (ASDU6) 信号复归 (ASDU20) IEC-103 规约结构IEC 60870-5-103 规约基于三层参考模型（增强性能结构） ，这种模型用三层来表示：物理层、链路层、应用层。 IEC-103 物理层说明支持光纤或者 RS-485 方式。 IEC-103 链路层说明传输方式非平衡传输。控制系统组成主站，继电保护设备为子站，按照严格的Polling（轮询）方式进行通信。 传输速率、校验方式和重复帧传输的超时时间传输速率为 9600Kbit/s 或 19.2Kbit/s(可调)。8位数据位，1位停止位，校验方式为偶校验。 重复帧传输的超时时间间隔为50ms，控制系统中此值应当可以调节。 帧格式两种：固定长帧格式和非固定长帧格式。 固定长帧格式 固定帧长帧格式用于控制系统向继电保护设备传输询问帧或命令帧（复位数据单元或复位帧计数位），或继电保护设备向控制系统传输确认帧或响应帧。 帧校验和是控制字、地址的算术和（不考虑溢出位即 256 模和） 子站和主站在接收报文时，校验启动字符、帧校验和、结束字符，检出任何一个差错，该数据帧无效。 可变长帧格式 可变帧长帧格式用于控制系统向继电保护设备传输数据，或由继电保护设备向控制系统传输数据之用。 长度L包括控制域、地址域、链路用户数据长度的总和， L最大值为 255,子站和主站在组织报文时， 要控制链路用户数据的长度， 注意使控制域、 地址域、 链路用户数据长度之和不要超出 255。如果超出，则要考虑分包传送。 帧校验和是控制、地址、链路用户数据的算术和，（不考虑溢出位即 256 模和）。 子站和主站在接收报文时， 应校验两个启动字符、 两个 L值应一致，接收字符数为 L+6,帧校验和、结束字符，若检出任何一个差错，则舍弃此帧数据。 控制系统至保护设备报文控制域定义8位，一个字节。 备用位，始终为0。 启动报文位，始终为1，表示是由控制系统向继电保护设备传输，控制系统为启动站。 FCB（帧计数位），控制系统向同一个继电保护设备进行新一轮的发送/确认或请求/响应时，将 FCB 取反值。控制系统应为每一个继电保护设备保留一个帧计数位(FCB)的拷贝，若超时未从继电保护设备收到报文，或接收出现差错，则控制系统不改变帧计数位(FCB)的状态，重传原报文，重传次数大于等于 3 次，如果通信恢复，则继续正常通信过程，否则将复位该继电保护设备。继电保护设备在发送报文时，应该保留该发送报文的一个拷贝。在下一轮传输过程中如果控制系统的FCB有效且没有取反，则继电保护设备重新发送上一次发送报文的拷贝。在下一轮传输过程中如果控制系统的有效且 FCB 取反，则组织新报文发送，并保留此新报文的一个拷贝，并丢弃原来报文的拷贝。这样有效防止了报文丢失。复位命令的帧计数位 (FCB)为 0,帧计数有效位 (FCV) 为 0。广播校时报文不需要考虑报文丢失和重复重传，无需改变帧计数位 (FCB) 的状态，故在广播校时报文中帧计数有效位 (FCV) 为 0。 FCV(帧计数有效位)，0表示帧计数位（FCB）的变化无效，1表示帧计数位（FCB）的变化有效。 功能码： 功能码序号 帧类型 功能 FCV 0 发送/确认帧 复位通信单元 (CU) 0 1－2 —— 备用 — 3 发送/确认帧 传送数据 (总查询启动、命令报文、读写定值和软压板 （控制字），读取测量值 ) 1 4 发送/无回答帧 传送数据（广播校时） 0 5－6 —— 备用 — 7 复位帧计数位 (FCB) 传送数据 0 8 —— 备用 — 9 请求/响应帧 召唤链路状态 0 10 请求/响应帧 召唤1级用户数据 1 11 请求/响应帧 召唤2级用户数据 1 12－13 —— 备用 — 14－15 —— 备用 — table { margin: auto; font-size: 70%; } 保护设备至控制系统报文控制域的定义8位，一个字节。 备用位，始终为 0。 启动报文位，始终为 0：表示是由继电保护设备向控制系统传输。 ACD( 要求访问位 )，继电保护设备有 1 级用户用户数据，将 ACD 置 1 通知控制系统召唤 1 级用户数据，如果继电保护设备没有 1 级用户数据，则将 ACD 置 0。 DFC(数据流控制位 ) ，DFC=0表示继电保护设备可以接受数据。DFC=1表示继电保护设备的缓冲区已满，无法接受新数据。 本规约规定深圳所保护设备上送报文中 DFC 始终为 0，即可以接受新数据， 控制系统应能够处理 DFC=1 的报文。 功能码 功能码序号 帧类型 功能 0 确认帧 确认 1 确认帧 链路忙 2－5 —— 备用 6－7 —— 制造厂和用户协商定义 8 响应帧 以数据响应请求帧 9 响应帧 无所召唤的数据 10 —— 备用 11 响应帧 以链路状态或访问请求回答请求帧 12 备用 13 制造厂和用户协商定义 14 链路服务未工作 15 链路服务未工作 功能码 0 :继电保护设备从控制系统接收到复位、 读写数据报文时， 在传递 1 级用户数据之前， 都要发送一个短帧来通知控制系统继电保护设备已经接收到复位或读写数据报文， 且如果有 1级用户数据则将 ACD 置 1 来通知控制系统召唤 1 级用户数据。 功能码 1：如果控制系统向继电保护设备召唤 1 级用户 ,继电保护设备能够确定有需要发送的 1 级用户数据 ,但由于某种原因无法立即组织好 ,可以用此功能码来组织短帧10 21 01 22 16（假定地址为 1） 通知控制系统该继电保护设备链路忙，控制系统在接收到此报文后在下一个周期轮询到此继电保护设备时，继续召唤 1 级用户数据。 IEC-103应用层说明1级和2级用户1级用户数据（优先级从高到低）： 一次设备的工作状态和系统工作状态发生变化时形成的数据。 继电保护设备动作信号。 初始化过程中，继电保护设备发送的 ASDU5 报文。 控制系统发送总查询命令后，继电保护传送的状态量或动作信息，继电保护设备发送的总查询结束报文。 命令传输过程中，继电保护设备形成的肯定或否定认可报文。 在通用分类服务控制系统发送读或写命令后，继电保护设备发送的 ASDU10 报文。 继电保护设备在故障录波后形成扰动数据时所产生的如下内容： 扰动信息 扰动数据表 ASDU23 扰动数据传输准备就绪 ASDU26 被记录的通道传输准备就绪 ASDU27 带标志的状态变位传输准备就绪 ASDU28 传送带标志的状态变位的状态 ASDU29 传送扰动值 ASDU30 带标志的状态变位传输结束 ASDU31 2 级用户数据： 二级用户数据主要用来传输测量值， IEC－ 103(深圳所)规定深圳所继电保护设备测量值采用通用分类服务传输， 故无 2 级用户数据， 当主站召唤 2 级用户数据时， 用无所请求报文数据回答。 但要求在非深圳所继电保护设备利用 ASDU3 、ASDU9 上送测量值的情况下， 深圳所控制系统能够支持。 应用层系统介绍IEC－103（深圳所）是严格的 Polling 规约，按照非平衡方式传输，控制系统为主站，继电保护为子站。通信开始时，控制系统向继电保护设备发送服务命令使通信功能复位，然后是对保护进行广播校时，和总查询，总查询完毕后进入通常的召唤 2 级用户数据的过程。如果有突发事件，则子站首先通过ACD标志位置 1 通知主站本子站有 1 级用户数据， 主站然后根据ACD标志位为 1 来召唤 1 级用户数据。 对于控制系统主动发送的命令或通用分类服务读命令，子站首先响应一个短帧来确认收到此命令，并将ACD位置 1，通知主站有 1 级用户数据，主站然后根据ACD标志位为 1 来召唤 1 级用户数据， 如果此时子站尚未准备好 1 级用户数据， 则可发送链路忙报文并将ACD置 1 以通知主站子站忙，主站接收到此报文后应继续召唤 1 级用户数据，直到最后接收到 1 级用户数据或无所请求报文为止。 为了防止主站对某一个子站进行查询的时间过长，规定与某一个子站通信一次后应马上转入与下一个子站的通信过程， 主站应该记录与该子站的通信状态， 以便下一次轮询到此子站时能够继续正常通信。 应用层功能实现： 利用ASDU1传送告警、状态信息 利用ASDU2来传送事件 利用ASDU6来对保护装置校时 利用ASDU20来对保护装置信号复归 利用通用分类服务实现调取和修改定值、调取和修改测量值、调取测量值，其中定值支持调取多组定值。"},{"title":"异步爬虫的原理和解析","date":"2021-05-31T09:03:41.000Z","url":"/2021/05/31/%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E8%A7%A3%E6%9E%90/","tags":[["python","/tags/python/"],["asyncio","/tags/asyncio/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"实列引入看这么一个示例网站：，如图所示。 这个网站在内部实现返回响应的逻辑的时候特意加了 5 秒的延迟，也就是说如果我们用 requests 来爬取其中某个页面的话，至少需要 5 秒才能得到响应。 下面我们来用 requests 写一个遍历程序，直接遍历 1~100 部电影数据，代码实现如下： 运行结果如下： 每个页面都至少要等待 5 秒才能加载出来，因此 100 个页面至少要花费 500 秒的时间，总的爬取时间最终为 513.6 秒，将近 9 分钟。 基本了解阻塞阻塞状态指程序未得到所需计算资源时被挂起的状态。程序在等待某个操作完成期间，自身无法继续处理其他的事情，则称该程序在该操作上是阻塞的。 常见的阻塞形式有：网络 I/O 阻塞、磁盘 I/O 阻塞、用户输入阻塞等。阻塞是无处不在的，包括 CPU 切换上下文时，所有的进程都无法真正处理事情，它们也会被阻塞。如果是多核 CPU 则正在执行上下文切换操作的核不可被利用。 非阻塞非程序在等待某操作过程中，自身不被阻塞，可以继续处理其他的事情，则称该程序在该操作上是非阻塞的。 非阻塞并不是在任何程序级别、任何情况下都可以存在的。仅当程序封装的级别可以囊括独立的子程序单元时，它才可能存在非阻塞状态。 非阻塞的存在是因为阻塞存在，正因为某个操作阻塞导致的耗时与效率低下，才要把它变成非阻塞的。 同步不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，我们称这些程序单元是同步执行的。 例如购物系统中更新商品库存，需要用“行锁”作为通信信号，让不同的更新请求强制排队顺序执行，那更新库存的操作是同步的。 简言之，同步意味着有序。 异步为完成某个任务，不同程序单元之间过程中无需通信协调，也能完成任务的方式，不相关的程序单元之间可以是异步的。 例如，爬虫下载网页。调度程序调用下载程序后，即可调度其他任务，而无需与该下载任务保持通信以协调行为。不同网页的下载、保存等操作都是无关的，也无需相互通知协调。这些异步操作的完成时刻并不确定。 简言之，异步意味着无序。 多进程多进程就是利用 CPU 的多核优势，在同一时间并行地执行多个任务，可以大大提高执行效率。 协程协程，英文叫作 Coroutine，又称微线程、纤程，协程是一种用户态的轻量级线程。 协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此协程能保留上一次调用时的状态，即所有局部状态的一 个特定组合，每次过程重入时，就相当于进入上一次调用的状态。 协程本质上是个单进程，协程相对于多进程来说，无需线程上下文切换的开销，无需原子操作锁定及同步的开销，编程模型也非常简单。 可以使用协程来实现异步操作，比如在网络爬虫场景下，发出一个请求之后，需要等待一定的时间才能得到响应，但其实在这个等待过程中，程序可以干许多其他的事情，等到响应得到之后才切换回 来继续处理，这样可以充分利用 CPU 和其他资源，这就是协程的优势。 协程用法从 Python 3.4 开始，Python中加入了协程的概念，但这个版本的协程还是以生成器对象为基础的，在 Python 3.5 则增加了 async/await，使得协程的实现更加方便。 Python中使用协程最常用的库莫过于 asyncio，所以本文会以 asyncio 为基础来介绍协程的使用。 首先需要了解下面几个概念。 event_loop：事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，当满足条件发生的时候，就会调用对应的处理方法。 coroutine：中文翻译叫协程，在 Python中常指代为协程对象类型，可以将协程对象注册到时间循环中，它会被事件循环调用。可以使用 async 关键字来定义一个方法，这个方法在调用时不会立即 被执行，而是返回一个协程对象。 task：任务，它是对协程对象的进一步封装，包含了任务的各个状态。 future：代表将来执行或没有执行的任务的结果，实际上和 task 没有本质区别。 另外还需要了解 async/await 关键字，它是从 Python 3.5 才出现的，专门用于定义协程。其中，async 定义一个协程，await 用来挂起阻塞方法的执行。 定义协程 运行结果： async定义的方法无法直接执行，会变成一个协程对象，必须注册到事件循环中才可以执行。 协程对象可以进一步的被封装成task对象，如下： 运行结果： 可以看到定义了loop对象之后，调用它的create_task方法将coroutine对象转化成task对象，随后打印发现它是pending状态。接着将task对象添加到事件循环中执行，之后再打印输出一下task对象，它的状态就变成了finished，同时可以看到result变成了1，就是定义的execute方法返回的结果。 还有另外的方法可以定义task对象，直接通过ayncio的ensure_future方法，返回结果也是task对象。 运行结果： 其运行效果都是一样的。 绑定回调可以为某个task绑定回调函数。 运行结果： 调用add_done_task方法，将callback方法传递给封装好的task对象，当task对象执行完毕后就可以调用callback方法，同时task对象还会作为参数传递给callback方法，调用task对象的result方法就可以获取返回结果。 多任务协程如果要执行过次请求，可以顶一个task列表，然后使用asyncio的wait方法。 运行结果： 可以看到五个任务被顺次执行了，并得到了运行结果。 使用aiohttpaiohttp是一个支持异步请求的库，利用它和asyncio配合可以实现异步请求操作。 安装： 运行结果如下： 代码里面我们使用了await，后面跟了 get 方法，在执行这 10 个协程的时候，如果遇到了await，那么就会将当前协程挂起，转而去执行其他的协程，直到其他的协程也挂起或执行完毕，再进行下一个协程的执行。 开始运行时，时间循环会运行第一个task，针对第一个 task 来说，当执行到第一个 await 跟着的 get 方法时，它被挂起，但这个 get 方法第一步的执行是非阻塞的，挂起之后立马被唤醒，所以立即又进入执行， 创建了 ClientSession对象，接着遇到了第二个 await，调用了 session.get 请求方法，然后就被挂起了，由于请求需要耗时很久，所以一直没有被唤醒。 当第一个 task 被挂起了，那接下来该怎么办呢？事件循环会寻找当前未被挂起的协程继续执行，于是就转而执行第二个 task 了，也是一样的流程操作，直到执行了第十个 task 的 session.get 方法之后，全部的 task 都被挂起了。所有 task 都已经处于挂起状态，怎么办？只好等待了。几个请求几乎同时都有了响应，然后几个 task 也被唤醒接着执行，输出请求结果。 以百度为例，来测试下并发数量为 1、3、5、10、…、500 的情况下的耗时情况，代码如下： 运行结果如下： 可以看到，即使增加了并发数量，但在服务器能承受高并发的前提下，其爬取速度几乎不太受影响。"},{"title":"FastAPI教程-路径参数","date":"2021-05-30T14:47:15.000Z","url":"/2021/05/30/FastAPI%E6%95%99%E7%A8%8B-%E8%B7%AF%E5%BE%84%E5%8F%82%E6%95%B0/","tags":[["fastapi","/tags/fastapi/"]],"categories":[["FastAPI","/categories/FastAPI/"]],"content":"可以使用与 Python 格式化字符串相同的语法来声明路径”参数”或”变量”： 路径参数 item_id 的值将作为参数 item_id 传递给你的函数。 所以，运行示例并访问 ，将会看到如下响应： 有类型的路径参数可以使用标准的 Python 类型标注为函数中的路径参数声明类型。 在这个例子中，item_id 被声明为 int 类型。 Check 这将为你的函数提供编辑器支持，包括错误检查、代码补全等等。 数据转换如果运行示例并打开浏览器访问，将得到如下响应： Check 注意函数接收（并返回）的值为 3，是一个 Python int 值，而不是字符串 “3”。 所以，FastAPI 通过上面的类型声明提供了对请求的自动”解析”。 数据校验但如果你通过浏览器访问，你会看到一个清晰可读的 HTTP 错误： 因为路径参数 item_id 传入的值为 &quot;foo&quot;，它不是一个 int。 如果你提供的是 float 而非整数也会出现同样的错误，比如： Check 所以，通过同样的 Python 类型声明，FastAPI 提供了数据校验功能。 注意上面的错误同样清楚地指出了校验未通过的具体原因。 在开发和调试与你的 API 进行交互的代码时，这非常有用。 文档打开浏览器访问，你将看到自动生成的交互式 API 文档： 路径操作的顺序在创建路径操作时，你会发现有些情况下路径是固定的。 比如 /users/me，我们假设它用来获取关于当前用户的数据. 然后，你还可以使用路径 /users/&#123;user_id&#125; 来通过用户 ID 获取关于特定用户的数据。 由于路径操作是按顺序依次运行的，你需要确保路径 /users/me 声明在路径 /users/&#123;user_id&#125;之前： 否则，/users/&#123;user_id&#125; 的路径还将与 /users/me 相匹配，”认为”自己正在接收一个值为 &quot;me&quot; 的 user_id 参数。 预设值如果你有一个接收路径参数的路径操作，但你希望预先设定可能的有效参数值，则可以使用标准的 Python Enum 类型。 创建一个 Enum 类导入 Enum 并创建一个继承自 str 和 Enum 的子类。 通过从 str 继承，API文档将能够知道这些值必须为 string 类型并且能够正确地展示出来。 然后创建具有固定值的类属性，这些固定值将是可用的有效值： 输出结果： 获取枚举值可以使用 model_name.value 或通常来说 your_enum_member.value 来获取实际的值（在这个例子中为 str）。 Tip 你也可以通过 ModelName.tony.value 来获取值 “tony”。 返回枚举成员你以从路径操作中返回枚举成员，即使嵌套在 JSON 结构中（例如一个 dict 中）。 包含路径的路径参数路径转换器可以使用直接来自 Starlette 的选项来声明一个包含路径的路径参数： 在这种情况下，参数的名称为 file_path，结尾部分的 :path 说明该参数应匹配任意的路径。 因此，可以这样使用它： 访问输出结果如下： Tip 可能会需要参数包含 /home/johndoe/myfile.txt，以斜杠（/）开头。 在这种情况下，URL 将会是 /files//home/johndoe/myfile.txt，在files 和 home 之间有一个双斜杠（//）。 "},{"title":"FastAPI教程-入门介绍","date":"2021-05-30T11:57:07.000Z","url":"/2021/05/30/FastAPI%E6%95%99%E7%A8%8B-%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97/","tags":[["python","/tags/python/"],["fastapi","/tags/fastapi/"]],"categories":[["FastAPI","/categories/FastAPI/"]],"content":"第一步最简单的 FastAPI 文件可能像下面这样： 最简单的 FastAPI 文件可能像下面这样： 将其复制到 main.py 文件中。 运行实时服务器： Note uvicorn main:app 命令含义如下: main：main.py 文件（一个 Python「模块」）。 app：在 main.py 文件中通过 app = FastAPI() 创建的对象。 --reload：让服务器在更新代码后重新启动。仅在开发时使用该选项。 分步概括步骤1：导入FastAPI FastAPI 是一个为 API 提供了所有功能的 Python 类。 技术细节 FastAPI 是直接从 Starlette 继承的类。你可以通过 FastAPI 使用所有的 Starlette 的功能。 步骤 2：创建一个 FastAPI「实例」 这里的变量 app 会是 FastAPI 类的一个「实例」。 这个实例将是创建你所有 API 的主要交互对象。 这个 app 同样在如下命令中被 uvicorn 所引用： 如果你像下面这样创建应用： 将代码放入 main.py 文件中，然后你可以像下面这样运行 uvicorn： 步骤 3：创建一个路径操作路径这里的「路径」指的是 URL 中从第一个 / 起的后半部分。 所以，在一个这样的 URL 中： …路径会是： info 「路径」也通常被称为「端点」或「路由」。 开发 API 时，「路径」是用来分离「关注点」和「资源」的主要手段。 操作这里的「操作」指的是一种 HTTP「方法」。 下列之一： POST GET PUT DELETE …以及更少见的几种： OPTIONS HEAD PATCH TRACE 在 HTTP 协议中，你可以使用以上的其中一种（或多种）「方法」与每个路径进行通信。 在开发 API 时，通常使用特定的 HTTP 方法去执行特定的行为。 通常使用： POST：创建数据。 GET：读取数据。 PUT：更新数据。 DELETE：删除数据。 因此，在 OpenAPI 中，每一个 HTTP 方法都被称为「操作」。 定义一个路径操作装饰器 @app.get(&quot;/&quot;)告诉 FastAPI 在它下方的函数负责处理如下访问请求： 请求路径为 / 使用 get 操作 @decorator Info @something语法在 Python 中被称为「装饰器」。 装饰器接收位于其下方的函数并且用它完成一些工作。 在上面的例子中，这个装饰器告诉 FastAPI 位于其下方的函数对应着路径 / 加上 get 操作。 它是一个「路径操作装饰器」。 步骤 4：定义路径操作函数这是「路径操作函数」： 路径：是 /。 操作：是 get。 函数：是位于「装饰器」下方的函数（位于@app.get(&quot;/&quot;)下方）。 这是一个 Python 函数。 每当 FastAPI 接收一个使用 GET 方法访问 URL「/」的请求时这个函数会被调用。 在这个例子中，它是一个 async 函数。当然也可以定义为常规函数。 步骤 5：返回内容 你可以返回一个 dict、list，像 str、int 一样的单个值，等等。 你还可以返回 Pydantic 模型（稍后你将了解更多）。 还有许多其他将会自动转换为 JSON 的对象和模型（包括 ORM 对象等）。"},{"title":"FastAPI安装和使用","date":"2021-05-30T07:51:17.000Z","url":"/2021/05/30/FastAPI%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","tags":[["python","/tags/python/"],["fastapi","/tags/fastapi/"]],"categories":[["FastAPI","/categories/FastAPI/"]],"content":"简介FastAPI 是一个用于构建 API 的现代、快速（高性能）的 web 框架，使用 Python 3.6+ 并基于标准的 Python 类型提示。 关键特性: 快速：可与 NodeJS 和 Go 比肩的极高性能（归功于 Starlette 和 Pydantic）。最快的 Python web 框架之一。 高效编码：提高功能开发速度约 200％ 至 300％。* 更少 bug：减少约 40％ 的人为（开发者）导致错误。* 智能：极佳的编辑器支持。处处皆可自动补全，减少调试时间。 简单：设计的易于使用和学习，阅读文档的时间更短。 简短：使代码重复最小化。通过不同的参数声明实现丰富功能。bug 更少。 健壮：生产可用级别的代码。还有自动生成的交互式文档。 标准化：基于（并完全兼容）API 的相关开放标准：OpenAPI (以前被称为 Swagger) 和 JSON Schema。 依赖Python 3.6及更高版本 FastAPI站在以下巨人的肩膀之上： Starlette 负责 web 部分。 Pydantic 负责数据部分。 安装 你还会需要一个 ASGI 服务器，生产环境可以使用 Uvicorn 或者 Hypercorn。 示例创建创建一个main.py文件并写入以下内容: 或者使用 如果你的代码里会出现 async / await，请使用 async def： 运行通过以下命令运行服务器： 检查使用浏览器访问。 你将会看到如下JSON响应： 你已经创建了一个具有以下功能的 API： 通过 路径 / 和 /items/&#123;item_id&#125; 接受 HTTP 请求。 以上 路径 都接受 GET 操作（也被称为 HTTP 方法）。 /items/&#123;item_id&#125; 路径 有一个路径参数 item_id 并且应该为 int 类型。 /items/&#123;item_id&#125; 路径 有一个可选的 str 类型的 查询参数 q。 交互式 API 文档访问，你会看到自动生成的交互式 API 文档（由 Swagger UI生成）： 示例升级现在修改main.py文件来从 PUT 请求中接收请求体。 我们借助 Pydantic 来使用标准的 Python 类型声明请求体。 交互式 API 文档升级访问。 交互式 API 文档将会自动更新，并加入新的请求体： 点击「Try it out」按钮，之后你可以填写参数并直接调用 API： 然后点击「Execute」按钮，用户界面将会和 API 进行通信，发送参数，获取结果并在屏幕上展示： "},{"title":"Flask快速上手","date":"2021-05-30T06:56:17.000Z","url":"/2021/05/30/Flask%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/","tags":[["flask","/tags/flask/"]],"categories":[["Flask","/categories/Flask/"]],"content":"一个最小的应用一个最小的Flask应用如下: 那么，这些代码是什么意思呢？ 首先我们导入了Flask类。 该类的实例将会成为我们的WSGI应用。 接着我们创建一个该类的实例。第一个参数是应用模块或者包的名称。如果你使用一个单一模块（就像本例），那么应当使用__name__，因为名称会根据这个模块是按应用方式使用还是作为一个模块导入而发生变化（可能是 ‘__main__’ ， 也可能是实际导入的名称）。这个参数是必需的，这样Flask才能知道在哪里可以找到模板和静态文件等东西。更多内容详见Flask文档。 使用route()装饰器来告诉Flask触发函数的URL。 函数名称被用于生成相关联的URL 。函数最后返回需要在用户浏览器中显示的信息。 可以使用flask命令或者python的-m开关来运行这个应用。在运行应用之前，需要在终端里导出FLASK_APP环境变量: 如果是在Windows下，那么导出环境变量的语法取决于使用的是哪种命令行解释器。 在Command Prompt下: 还可以使用python -m flask: 这样就启动了一个非常简单的内建的服务器。这个服务器用于测试应该是足够了，但是用于生产可能是不够的。 路由现代web应用都使用有意义的URL，这样有助于用户记忆，网页会更得到用户的青睐， 提高回头率。 使用route()装饰器来把函数绑定到URL: "},{"title":"Flask安装","date":"2021-05-30T06:24:17.000Z","url":"/2021/05/30/Flask%E5%AE%89%E8%A3%85/","tags":[["python","/tags/python/"],["flask","/tags/flask/"]],"categories":[["Flask","/categories/Flask/"]],"content":"安装Python版本推荐使用最新版本的Python3 。 Flask支持Python 3.5及更高版本的Python 3 、Python 2.7和PyPy。 依赖当安装Flask时，以下配套软件会被自动安装。 Werkzeug用于实现WSGI，应用和服务之间的标准Python接口。 Jinja用于渲染页面的模板语言。 MarkupSafe与Jinja共用，在渲染页面时用于避免不可信的输入，防止注入攻击。 ItsDangerous保证数据完整性的安全标志数据，用于保护Flask的session cookie. Click是一个命令行应用的框架。用于提供flask命令，并允许添加自定义管理命令。 可选依赖以下配套软件不会被自动安装。如果安装了，那么Flask会检测到这些软件。 Blinker为信号提供支持。 SimpleJSON是一个快速的JSON实现，兼容Python’s json模块。如果安装了这个软件，那么会优先使用这个软件来进行JSON操作。 python-dotenv当运行flask命令时为通过dotenv设置环境变量 提供支持。 Watchdog为开发服务器提供快速高效的重载。 虚拟环境建议在开发环境和生产环境下都使用虚拟环境来管理项目的依赖。 为什么要使用虚拟环境？随着你的Python项目越来越多，你会发现不同的项目会需要不同的版本的Python库。同一个Python库的不同版本可能不兼容。 虚拟环境可以为每一个项目安装独立的Python库，这样就可以隔离不同项目之间的Python库，也可以隔离项目与操作系统之间的Python库。 Python3内置了用于创建虚拟环境的venv模块。如果你使用的是较新的Python版本，那么可以按照下面的步骤创建虚拟环境。 创建一个虚拟环境创建一个项目文件夹，然后创建一个虚拟环境。创建完成后项目文件夹中会有一个venv文件夹： 在Windows下： 在老版本的Python中要使用下面的命令创建虚拟环境： 在Windows下： 激活python环境在开始工作前，先要激活相应的虚拟环境： 在Windows下： 激活后，你的终端提示符会显示虚拟环境的名称。 如果你使用的VS code，要在cmd命令行下激活虚拟环境，如果是power shell则不可以。 安装Flask在已激活的虚拟环境中可以使用如下命令安装Flask： Flask现在已经安装完毕。"},{"title":"scrapy的基本使用","date":"2021-05-29T14:49:55.000Z","url":"/2021/05/29/scrapy%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":[["scrapy","/tags/scrapy/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"抓取目标要完成的任务如下。 创建一个Scrapy项目。 创建一个Spider来抓取站点和处理数据。 通过命令行将抓取的内容导出。 将抓取的内容保存到MongoDB数据库。 抓取的目标站点为。 准备工作安装好scrapy框架、MongoDB和PyMongo库。 创建项目创建一个Scrapy项目，项目文件可以直接用Scrapy命令生成，命令如下所示： 这个命令将会创建一个名为tutorial的文件夹，文件夹结构如下所示： 创建SpiderSpider是自己定义的类，Scrapy用它从网页里抓取内容，并解析抓取的结果。不过这个类必须继承Scrapy提供的Spider类scrapy.Spider，还要定义Spider的名称和起始请求，以及怎样处理爬取后的结果的方法。 也可以使用命令行创建一个Spider。比如要生成Quotes这个Spider，可以执行如下命令： 进入刚才创建的tutorial文件夹，然后执行genspider命令。第一个参数是Spider的名称，第二个参数是网站域名。执行完毕之后，spiders文件夹中多了一个quotes.py，它就是刚刚创建的Spider，内容如下所示： 这里有三个属性——name、allowed_domains和start_urls，还有一个方法parse。 name：它是每个项目唯一的名字，用来区分不同的Spider。 allowed_domains：它是允许爬取的域名，如果初始或后续的请求链接不是这个域名下的，则请求链接会被过滤掉。 start_urls：它包含了Spider在启动时爬取的url列表，初始请求是由它来定义的。 parse：它是Spider的一个方法。默认情况下，被调用时start_urls里面的链接构成的请求完成下载执行后，返回的响应就会作为唯一的参数传递给这个函数。该方法负责解析返回的响应、提取数据或者进一 步生成要处理的请求。 创建ItemItem是保存爬虫数据的容器，它的使用方法和字典类似，不过相比字典，Item多了额外的保护机制，可以避免拼写错误或者定义字段错误。 创建Item需要继承scrapy.Item类，并且定义类型为scrapy.Field的字段。观察目标网站，可以获取到的内容有text、author、tags。 定义Item，此时将items.py修改如下： 这里定义了三个字段，将类的名称修改为QuoteItem，接下来爬取时会使用到这个Item。 解析Responseparse方法的参数response是start_urls里面的链接爬取后的结果。所以在parse方法中，可以直接对response变量包含的内容进行解析，比如浏览请求结果的网页源代码，或者进一步分析源代码内容，或者找出结果中的链接而得到下一个请求。 首先看看网页结构，如图所示。每一页都有多个class为quote的区块，每个区块内都包含text、author、tags。那么先找出所有的quote，然后提取每一个quote中的内容。 提取的方式可以是CSS选择器或XPath选择器。在这里使用CSS选择器进行选择，parse方法的改写如下所示： 对于text，获取结果的第一个元素即可，所以使用extract_first方法，对于tags，要获取所有结果组成的列表，所以使用extract方法。 使用Item上文定义了Item，接下来就要使用它了。Item可以理解为一个字典，不过在声明的时候需要实例化。然后依次用刚才解析的结果赋值Item的每一个字段，最后将Item返回即可。 QuotesSpider的改写如下所示： 如此一来，首页的所有内容被解析出来，并被赋值成了一个个QuoteItem。 后续Request接下来就需要从当前页面中找到信息来生成下一个请求，然后在下一个请求的页面里面找到信息再构造下一个请求。这样循环往复迭代，从而实现整站的爬取。 将刚才的页面拉到最底部，如图所示。 有一个Next按钮，查看一下源代码，可以发现它的链接是/page/2/，实际上全链接就是：，通过这个链接我们就可以构造下一个请求。 构造请求时需要用到scrapy.Request。这里传递两个参数——url和callback，这两个参数的说明如下。 url：它是请求链接。 callback：它是回调函数。当指定了该回调函数的请求完成之后，获取到响应，引擎会将该响应作为参数传递给这个回调函数。回调函数进行解析或生成下一个请求，回调函数如上文的parse()所示。 由于parse就是解析text、author、tags的方法，而下一页的结构和刚才已经解析的页面结构是一样的，所以可以再次使用parse方法来做页面解析。 接下来要做的就是利用选择器得到下一页链接并生成请求，在parse方法后追加如下的代码： 第一句代码首先通过CSS选择器获取下一个页面的链接，即要获取a超链接中的href属性。这里用到了::attr(href)操作。然后再调用extract_first方法获取内容。 第二句代码调用了urljoin方法，urljoin()方法可以将相对URL构造成一个绝对的URL。例如，获取到的下一页地址是/page/2，urljoin方法处理后得到的结果就是：。 第三句代码通过url和callback变量构造了一个新的请求，回调函数callback依然使用parse方法。这个请求完成后，响应会重新经过parse方法处理，得到第二页的解析结果，然后生成第二页的下一页，也就是第三页的请求。这样爬虫就进入了一个循环，直到最后一页。dont_filter设置为Ture不进行域名过滤，这样就能继续爬取。 通过几行代码，就轻松实现了一个抓取循环，将每个页面的结果抓取下来了。现在，改写之后的整个Spider类如下所示： 运行进入目录，运行如下命令： 爬虫一边解析，一边翻页，直至将所有内容抓取完毕，然后终止。 最后，Scrapy输出了整个抓取过程的统计信息，如请求的字节数、请求次数、响应次数、完成原因等。 保存到文件Scrapy提供的Feed Exports可以轻松将抓取结果输出。例如，想将上面的结果保存成JSON文件，可以执行如下命令： 命令运行后，项目内多了一个quotes.json文件，文件包含了刚才抓取的所有内容，内容是JSON格式。 另外还可以每一个Item输出一行JSON，输出后缀为jl，为jsonline的缩写，命令如下所示： 或 输出格式还支持很多种，例如csv、xml、pickle、marshal等，还支持ftp、s3 等远程输出，另外还可以通过自定义ItemExporter来实现其他的输出。 例如，下面命令对应的输出分别为csv、xml、pickle、marshal格式以及ftp远程输出： 其中，ftp输出需要正确配置用户名、密码、地址、输出路径，否则会报错。通过Scrapy提供的Feed Exports，可以轻松地输出抓取结果到文件。对于一些小型项目来说，这应该足够了。不过如果想要更复杂的输出，如输出到数据库等，可以使用ItemPileline来完成。 使用ItemPileline如果想进行更复杂的操作，如将结果保存到MongoDB数据库，或者筛选某些有用的Item，则可以定义ItemPipeline来实现。 ItemPipeline为项目管道。当Item生成后，它会自动被送到ItemPipeline进行处理，常用ItemPipeline来做如下操作。 清洗HTML数据； 验证爬取数据，检查爬取字段； 查重并丢弃重复内容； 将爬取结果储存到数据库。 要实现ItemPipeline很简单，只需要定义一个类并实现process_item方法即可。启用ItemPipeline后，ItemPipeline会自动调用这个方法。process_item方法必须返回包含数据的字典或Item对象，或者抛出DropItem异常。 process_item方法有两个参数。一个参数是item，每次Spider生成的Item都会作为参数传递过来。另一个参数是spider，就是Spider的实例。 接下来，实现一个ItemPipeline，筛掉text长度大于50的Item，并将结果保存到MongoDB。 修改项目里的pipelines.py文件，之前用命令行自动生成的文件内容可以删掉，增加一个TextPipeline类，内容如下所示： 这段代码在构造方法里定义了限制长度为50，实现了process_item方法，其参数是item和spider。首先该方法判断item的text属性是否存在，如果不存在，则抛出DropItem异常；如果存在，再判断长度是否大于50，如果大于，那就截断然后拼接省略号，再将item返回即可。 将处理后的item存入MongoDB，定义另外一个Pipeline。同样在pipelines.py中，实现另一个类MongoPipeline，内容如下所示： MongoPipeline类实现了API定义的另外几个方法。 from_crawler：这是一个类方法，用@classmethod标识，是一种依赖注入的方式，方法的参数就是crawler，通过crawler这个参数我们可以拿到全局配置的每个配置信息，在全局配置settings.py中可以定义MONGO_URI和MONGO_DB来指定MongoDB连接需要的地址和数据库名称，拿到配置信息之后返回类对象即可。所以这个方法的定义主要是用来获取settings.py中的配置的。 open_spider：当Spider被开启时，这个方法被调用。在这里主要进行了一些初始化操作。 close_spider：当Spider被关闭时，这个方法会调用，在这里将数据库连接关闭。 最主要的process_item方法则执行了数据插入操作。 定义好TextPipeline和MongoPipeline这两个类后，需要在settings.py中使用它们。MongoDB的连接信息还需要定义。 赋值ITEM_PIPELINES字典，键名是Pipeline的类名称，键值是调用优先级，是一个数字，数字越小则对应的Pipeline越先被调用。再重新执行爬取，命令如下所示： 爬取结束后，MongoDB中创建了一个tutorial的数据库、QuoteItem的表。"},{"title":"scrapy框架介绍","date":"2021-05-29T10:13:04.000Z","url":"/2021/05/29/scrapy%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/","tags":[["python","/tags/python/"],["scrapy","/tags/scrapy/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"Scrapy介绍Scrapy是一个基于Twisted的异步处理框架，是纯Python实现的爬虫框架，其架构清晰，模块之间的耦合程度低，可扩展性极强，可以灵活完成各种需求。只需要定制开发几个模块就可以轻松实现一个爬虫。 首先来看下Scrapy框架的架构，如图所示： 它可以分为如下的几个部分。 Engine（引擎）：用来处理整个系统的数据流处理、触发事务，是整个框架的核心。 Item（项目）：定义了爬取结果的数据结构，爬取的数据会被赋值成该对象。 Scheduler（调度器）：用来接受引擎发过来的请求并加入队列中，并在引擎再次请求的时候提供给引擎。 Downloader（下载器）：用于下载网页内容，并将网页内容返回给蜘蛛。 Spiders（蜘蛛）：其内定义了爬取的逻辑和网页的解析规则，它主要负责解析响应并生成提取结果和新的请求。 ItemPipeline（项目管道）：负责处理由蜘蛛从网页中抽取的项目，它的主要任务是清洗、验证和存储数据。 Downloader Middlewares（下载器中间件）：位于引擎和下载器之间的钩子框架，主要是处理引擎与下载器之间的请求及响应。 Spider Middlewares（蜘蛛中间件）：位于引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛输入的响应和输出的结果及新的请求。 数据流了解了架构，就是要了解它是怎么进行数据爬取和处理的，所以接下来介绍Scrapy的数据流机制。 Scrapy中的数据流由引擎控制，其过程如下： Engine首先打开一个网站，找到处理该网站的Spider并向该Spider请求第一个要爬虫的URL。 Engine从Spider中获取到第一个要爬取的URL并通过Scheduler以Request的形式调度。 Engine向Scheduler请求下一个要爬取的URL。 Scheduler返回下一个要爬取的URL给Engine，Engine将URL通过Downloader Middlewares转发给Downloader下载。 一旦页面下载完毕， Downloader生成一个该页面的Response，并将其通过Downloader Middlewares发送给Engine。 Engine从下载器中接收到Response并通过Spider Middlewares发送给Spider处理。 Spider处理Response并返回爬取到的Item及新的Request给Engine。 Engine将Spider返回的Item给ItemPipeline，将新的Request给Scheduler。 重复第二步到最后一步，直到Scheduler中没有更多的Request，Engine关闭该网站，爬取结束。 通过多个组件的相互协作、不同组件完成工作的不同、组件对异步处理的支持，Scrapy最大限度地利用了网络带宽，大大提高了 数据爬取和处理的效率。 安装使用官方文档推荐的方式安装就可以了： 项目结构安装完成后，就可以使用Scrapy预先配置好的很多可用的组件和编写爬虫时所用的脚手架。 创建项目的命令如下： 执行完成之后，在当前运行目录下便会出现一个文件夹，叫作demo，这就是一个Scrapy项目框架，可以基于这个项目框架来编写爬虫。 项目文件结构如下所示： 在此要将各个文件的功能描述如下： scrapy.cfg：它是Scrapy项目的配置文件，其内定义了项目的配置文件路径、部署相关信息等内容。 items.py：它定义Item数据结构，所有的Item的定义都可以放这里。 pipelines.py：它定义ItemPipeline的实现，所有的ItemPipeline的实现都可以放这里。 settings.py：它定义项目的全局配置。 middlewares.py：它定义Spider Middlewares和Downloader Middlewares的实现。 spiders：其内包含一个个Spider的实现，每个Spider都有一个文件。 "},{"title":"内篇（二）——齐物论","date":"2021-05-29T05:12:36.000Z","url":"/2021/05/29/%E5%86%85%E7%AF%87%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E9%BD%90%E7%89%A9%E8%AE%BA/","tags":[["读书","/tags/%E8%AF%BB%E4%B9%A6/"]],"categories":[["庄子","/categories/%E5%BA%84%E5%AD%90/"]],"content":" 南郭子綦隐机而坐，仰天而嘘，荅焉似丧其耦。颜成子游立侍乎前，曰：“何居乎？形固可使如槁木，而心固可使如死灰乎？今之隐机者，非昔之隐机者也？”子綦曰：“偃，不亦善乎，而问之也！今者吾丧我，汝知之乎？汝闻人籁而未闻地籁，汝闻地籁而不闻天籁夫！” 南郭子綦靠几静坐，仰面朝天缓缓地吐气，茫然若失，就像魂魄离开了躯体。南郭子綦的学生颜成子游见状马上前来侍候，问南郭子綦说：“您刚才是处于一种什么样的境界呢？一个人的肢体形貌在打坐时固然可以使它像枯槁之木一样毫无生气，难道一个人的心神在打坐时也可以使它像死灰一样毫无生气吗？您在今天所表现出来的安稳境界，跟以前的安稳境界绝对不一样。”南郭子綦回答说：“偃，你问此事，不是问得很好吗？今天我遗忘了形体之我，你知道这一点吗？你只听到人吹箫管发出的声音，而没听听风吹众窍所发出的声音，你只听到风吹众窍所发出的声音，而没有听到天地间万物的自鸣之声！” 子游曰：“敢问其方。”子綦曰：“夫大块噫气，其名为风。是唯无作，作则万窍怒呺。而独不闻之翏翏乎？山林之畏隹，大木百围之窍穴，似鼻，似口，似耳，似枅，似圈，似臼，似洼者，似污者。激者、謞者、叱者、吸者、叫者、譹者、宎者，咬者，前者唱于而随者唱喁，泠风则小和，飘风则大和，厉风济则众窍为虚。而独不见之调调之刁刁乎？” 子游说：“能不能问一问大地的声音与天的声音是怎么回事呢？”南郭子綦回答说：“那个地籁就是无边无际的造物之作用所发出的能量之气息啊，发出来之后就被叫作风。这种能量要么就是内在蕴含着不发出来，一旦发出来就会万窍怒号，难道你耳边就没有听到过这种‘呼呼’的风声吗！山林的险峻、大树周身的窍穴，有的形状像鼻子，有的形状像张开的嘴，有的形状像耳朵，有的形状”像扁长的发簪，有的形状像凸出来的圈筒，有的形状像凹下去的浅坑，有的浅一些，有的深一些；于是所发的声响就有的快促，有的像响箭，有的刺耳，有的发自往里吸，有的发自往外出，有的像嚎哭声，有的像狗叫，有的像悲哀声。能量流的运动原本就很舒缓动听，怪就怪哉随着各种物体所发出的众口乱叫而嘈杂了；当徐徐之风时万物就会轻微地唱和，当风大时万物就会大点儿地唱和，当劲风厉厉时反而所有能发响者皆欲发而无声了，难道你没有从风响的音调中听出来各种声响的发声原因、从风的响声上听出其后面的那个不发声的东西吗？” 子游曰：“地籁则众窍是已，人籁则比竹是已，敢问天籁。”子綦曰：“夫吹万不同，为而使其自己也。咸其自取，怒者其谁邪？” 子游又问：“大地的本意可以借助万物之窍所发出的唱和声中表达出来，人所的本意也可以从清悠、消沉、谐美的各种丝竹的或条畅或激昂的声中表达出来，那么天的本意是怎么表达出来的呢？”南郭子綦说：“所谓天籁的音响万变，而又能使其自行息止，这完全都是出于自然，有什么东西主使着它呢？” 大知闲闲，小知閒閒。大言炎炎，小言詹詹。其寐也魂交，其觉也形开。与接为构，日以心斗。缦者、窖者、密者。小恐惴惴，大恐缦缦。其发若机栝，其司是非之谓也；其留如诅盟，其守胜之谓也；其杀如秋冬，以言其日消也；其溺之所为之，不可使复之也；其厌也如缄，以言其老洫也；近死之心，莫使复阳也。喜怒哀乐，虑叹变慹[zhé]，姚佚启态。乐出虚，蒸成菌。日夜相代乎前而莫知其所萌。已乎，已乎！旦暮得此，其所由以生乎！ 大智者看上去显得非常广博，小智者却十分琐细；高论者盛气凌人，争论者小辩不休。辩士睡时，精神与梦境交错在一起，醒后疲于与外物接触纠缠。每天与外物相接，其心有如经历了一场又一场的战斗一样疲惫。有的心机柔奸，有的善设陷阱，有的潜机不露。小的惧怕表现为忧惧不安，大的惧怕表现为惊恐失神。三灾八难的降临就像箭在弦上一样地随时而发，这全是由于行为人的心机所伺机行是非之行径所造成的；三灾八难的不降临也可以像发愿一样的通过履践其愿誓而消失，那就要看行为人是否践履自己的愿誓而坚持无思无为的行径来决定了。神情衰沮就像秋风冬雪肃杀万木一样无情，这就说明道心逍遥之本性的消亡是渐进的过程；如果行为人沉迷于自己之心机的经营里而不醒悟的话，也就很难再回到那个逍遥的本性去了。然而行为人对恢复道心的修行却厌恶得像自行封闭起来的蚕子一样的听不进去，这就说明行为人的执着意识的积习太严重了；像这样的几乎等于死亡的心，实在是没法使它再恢复生气。所谓的喜怒哀乐，都是认取了事物对自己所引发的感受而逐步形成多忧惧的意识，正在这个意识之执着的支配下才形成了轻狂而放任的或喜或怒或哀或乐的情绪了。喜怒哀乐发出来之后就过去了，接着而来的就是又孕育出了无明之烦恼，如此的一天到晚的不是喜乐就是烦恼的交替着出现在眼前，而从来不知道它们是怎么产生的。不可救药啊，不可救药！一天到晚地如此活着，三灾八难也就由此产生了啊！ 非彼无我，非我无所取。是亦近矣，而不知其所为使。若有真宰，而特不得其眹。可行己信，而不见其形，有情而无形。百骸、九窍、六藏、赅而存焉，吾谁与为亲？汝皆说之乎？其有私焉？如是皆有为臣妾乎？其臣妾不足以相治乎？其递相为君臣乎？其有真君存焉！如求得其情与不得，无益损乎其真。一受其成形，不亡以待尽。与物相刃相靡，其行尽如驰而莫之能止，不亦悲乎！终身役役而不见其成功，苶然疲役而不知其所归，可不哀邪！人谓之不死，奚益！其形化，其心与之然，可不谓大哀乎？人之生也，固若是芒乎？其我独芒，而人亦有不芒者乎？ 如果没有了我之内外的任何对立面的观念也就没有我的观念了，如果没有了我的观念也就没有了博取之心了，如此也就基本上接近于那个道心的行径了，然而却不知道是什么东西在支配着这个道心的行径。好像其中有一个迫使万物循势而动的主宰在，然而偏偏找不到那个主宰的迹象在什么地方，当你恢复了道心之后就会彻底明了那个迫使万物循势而动的主宰了，但它却又没有一个具体的形状可见，它仅仅是道心的广大无缘之悲性而没有具体形状而已。一个人身上有百骸、九窍、六脏，当这些脏器都完备了之后才能决定一个人的生存，那么你认为哪一个脏器最重要而倍加爱护呢？还不是都喜欢而都倍加爱护吗？这个问题的背后岂不牵扯到一个“私我”的观念了吗？如果有一个“私我”之观念的话，那么各脏器岂不就成了分别为“私我”提供专职服务的臣下与妻妾了吗？这种势必缺乏协调性的臣下与妻妾岂不就不足以担当全面治理整个机体的重任了吗？难道它们是逐个轮流着担当君的职责而协调各位臣妾的工作吗？这就证明里面有一个真正的主宰在后面起着决定作用吗？如果实证到了那个广大无缘之慈悲性的道心原来是没有具体形状可得的时候，那时候就会明白这个真宰的作用无论怎么做都既不会益于也不会损于那个真宰者的不生不灭、不增不减、不垢不净性了。然而自从那个人人都有的真主秉承了执着之意识以投胎成人之后，还没有到该死的寿数时就在哪里等着寿尽的一天了，这是因为内外界事物所诱发的爱、恶、欲、的心行在相争斗以至于相消耗的缘故；明明知道生命的寿终就像飞驰一样，但却不知道如何止住这种迈向寿终的步伐，对于自许聪明的人们来说难道不是一种悲哀吗！只知道一辈子地役使自己的心身去赴爱、恶、欲的劳役但对于恢复道心却毫无一点儿进展，整天精神不振地疲于劳役但却不知道究竟是为了什么，对于自许聪明的人们来说难道不是一种痛心吗！即使能够达到人们所说的千万岁的长生不老，对于道心的恢复又有什么益处呢？即使其形体逐渐衰败枯萎，也只是其执着心使其具有了这种变化而已，对于那个真宰的不生不死、不增不减、不垢不净性而言怎能不是最大的痛心呢！人生在世，本来就如此糊涂吗？ 夫随其成心而师之，谁独且无师乎？奚必知代而心自取者有之？愚者与有焉！未成乎心而有是非，是今日适越而昔至也。是以无有为有。无有为有，虽有神禹且不能知，吾独且奈何哉！ 还是只有我糊涂，而别人也有不糊涂的呢？世人如果都以自己的偏见作为判别是非的标准，那么谁没有一个标准呢？何必是懂得事物更替而心有见地的人才有是非标准呢？即使愚蠢的人也是有的。在未有成见存于心中之前已经就因自己的好恶之感受而得出对与错的心得了，这个心得之现象的得出就像今天刚到了越国而实际上原本就在越国一样地早就待在那个地方了。所以才从道心里凭空产生出心得乃至执着了。对于通过作意而产生出神奇的事物来的行为而言，即使有着禹王那样的神通，也不能有一点儿故意去知道的心在，客观世界的任何事物又怎能奈何于超然物外的那个道心之吾的神奇作用呢！ 夫言非吹也，言者有言。其所言者特未定也。果有言邪？其未尝有言邪？其以为异于鷇音，亦有辩乎？其无辩乎？道恶乎隐而有真伪？言恶乎隐而有是非？道恶乎往而不存？言恶乎存而不可？道隐于小成，言隐于荣华。故有儒墨之是非，以是其所非而非其所是。欲是其所非而非其所是，则莫若以明。 言论出于机心，与无心而吹的“天籁”是不同的。发言者知持一端，他们的话并不能作为衡量是非的标准。为了表述某事物果真就需要言辞吗？人们最初始时的思想交流并没有使用言辞啊！人们的言辞之所以能听得懂而雏鸟的叫声却听不懂，这个能听懂的背后是有一个思辨的审明作用在呢？还是没有思辨的审明作用在呢？那个大道为什么未被真正地认识到而产生了见解上的真与假呢？言辞所表述的意义为什么未被真正地认识到而产生了判断上的是与非呢？那个大道为什么必须去参悟才能实证到它而不能通过想象去找到它呢？言辞所表述的意义为什么必须通过思辨才能听懂而不能一听就能确定其真意呢？原因就在于那个大道被不深刻、不全面的小见解给遮蔽住了，就在于言辞所表述的意义被言辞本身的巧妙之比喻或夸张给遮蔽住了。所以才产生了儒墨两家之学术思想的我对你错的争鸣，世人都是用自己所认定的正确去评判人家的不正确，都是用自己所认定的人家的不正确来宣扬自己的正确。与其想用自己所认为的正确去推定人家的不正确，与其想用人家的不正确来证明自己所认为的正确，那就不如静下心来认认真真地修身以达到内明为好。 物无非彼，物无非是。自彼则不见，自知则知之。故曰：彼出于是，是亦因彼。彼是方生之说也。虽然，方生方死，方死方生；方可方不可，方不可方可；因是因非，因非因是。是以圣人不由而照之于天，亦因是也。是亦彼也，彼亦是也。彼亦一是非，此亦一是非，果且有彼是乎哉？果且无彼是乎哉？彼是莫得其偶，谓之道枢。枢始得其环中，以应无穷。是亦一无穷，非亦一无穷也。故曰：莫若以明。 天地万物在道体上不存在你我它的分别，天地万物在道体上也不存在你对及它错的差别。如果从万物存在的现象去看就不会认识到万物是道体之妙用的那个本质，只有从内明的大智慧上去看才能认识到万物同是道体之妙用的那个本质。所以说人们之所以会有万物的个体皆有差异以及你对而它错的分别心乃是从没有分别心的物我同胞的实相中产生的，于是那个没有分别心的物我同胞之实相的提法也就对应于人们的分别心而提出来了。人们之所以会产生分别心，乃是由于当下一念的喜爱心所造成的虽然都是因为当下一念的喜爱心所造成，但当下一念的喜爱心刚刚生起便又马上消失了，前念刚刚消失便又马上生起了后念的喜爱心；于是人们的分别心刚刚形成执着又紧接着遇到新的事物而须要去分别了，于是对新事物的分别心又形成另一个新的执著心了；因为先前的执着心而产生了对新遇到之事物的不认同，对新遇到之事物的不认同经过喜爱心的分别又形成了新的另一个执着。所以每一位恢复了道心的圣人无不鉴于当下之一念的迁流不断而不由得时刻注意着当下一念的似起未起之时，也就是因为这个道理啊！在先前的那个执着的作用下产生了新的另一个执着，新的另一个执着又变成了产生新执着的旧执着。先前的执着是喜爱心所认为的对与错而形成的，新执着也是喜爱心所认为的对与错而形成的。果真存在着执着心所认为的那个正确吗，果真存在着执着心所认为的那个不正确吗？无论是执着心所认为的正确还是所认为的不正确皆是失去了道心中庸之用的结果，这就是所说的形成执着的关键之所在。正是由于形成执着的整个关键作用发生在旧执着与新执着的环环相产生的现象中，于是无始劫以来所形成的无穷的执着也就变成了遮蔽道心的“五蕴”了。道心的中庸之大用是无穷无尽的，意识心的执着之妄用也是无穷无尽的。所以说要想了知道心的全体之大用还是静下心来认认真真地修身以达到内明为好。 以指喻指之非指，不若以非指喻指之非指也；以马喻马之非马，不若以非马喻马之非马也。天地一指也，万物一马也。 用自己的手指来说明人家的手指不是手指，不如不用自己的手指来说明人家的手指不是手指为好；用马来说明白马不是马，不如不用马来说明白马不是马为好。从道通为一的观点看，天地与一指，万物与一马，都是没有区别的。 可乎可，不可乎不可。道行之而成，物谓之而然。恶乎然？然于然。恶乎不然？不然于不然。物固有所然，物固有所可。无物不然，无物不可。故为是举莛与楹，厉与西施，恢诡谲怪，道通为一。 人家认为可，我也跟着认为可；人家认为不可，我也跟着认为不可。于是那个道的生发万物的妙用也就在因缘聚合而生的条件下形成了，于是万物也就在遇到了所对应的因缘聚合的条件下而形成了各具特色的形物了。有的万物之个体能适应因缘聚合而生的各种不同的环境条件而生，有的万物之个体不能适应因缘聚合而生的各种不同的环境条件而只能待其适应的环境条件时而生；所以有的万物之个体也就顺势而形成了具有新特色的形体了，有的万物之个体则未能顺势形成新特色的形体而是保持了始祖的面目乃至被淘汰。为什么有的万物之个体能顺势而形成具有新特色的形体呢？那是因为它具有能顺势而生的适应性所以才形成了具有新特色的形体。为什么有的万物之个体不能顺势而形成具有新特色的形体呢？那是因为它不具有能顺势而生的适应性所以才不能形成具有新特色的形体。为什么有的万物之个体能具有顺势而生的适应性呢？那是因为它具有适应因缘聚合而生的各种不同环境条件的天性。为什么有的万物之个体不具有顺势而生的适应性呢？那是因为它不具有适应因缘聚合而生的各种不同环境条件的天性。万物原本就具有顺势而形成的具有新特色的适应性，万物原本就具有适应因缘聚合而生的各种不同环境的天性。没有哪一个万物之个体不是因为这种适应性而生，也没有哪一个万物之个体不是因为这种天性而生。所以才产生了诸如轻空的莚秆或坚实的楹柱之类的各种不同物类，才产生了诸如丑陋或美丽之类的各种不同形色，才产生了诸如中正、阴险、诡秘、古怪之类的各种不同性格，当你通达了那个道心的大用后就会明白它们都是无二无别的实相了。 其分也，成也；其成也，毁也。凡物无成与毁，复通为一。唯达者知通为一，为是不用而寓诸庸。庸也者，用也；用也者，通也；通也者，得也；适得而几矣。因是已。已而不知其然，谓之道。劳神明为一而不知其同也，谓之“朝三”。何谓“朝三”？狙公赋芧，曰：“朝三而暮四。”众狙皆怒。曰：“然则朝四而暮三。”众狙皆悦。名实未亏而喜怒为用，亦因是也。是以圣人和之以是非而休乎天钧，是之谓两行。 万物之生命的死亡与分解的过程，就是又形成新的生命的过程；形成新的生命的过程，就是其生命的下一次死亡与分解的过程。万物根本就没有什么生与死的截然不同，或生活死的现象也只是同时发生着的道心之大用而已；正是由于实证到了道境界的人彻底明白了或生活死的现象都是同时发生着的，所以也就没有了意识分别心地将一切行为都隐含在恰到好处的无之用中了。当一切行为都隐含在恰到好处的无之用中了的时候，才能遵循道心的全体之大用；当能遵循道心的全体之大用的时候，才能通达一切现象皆是无二无别的实相；当通达了一切现象皆是无二无别之实相的时候，才能驾驭道心的全体之大用；当驾驭了道心的全体之大用的时候，也就差不多恢复了那个道心了。遵循这个过程也就恢复那个道心了。恢复了那个道心也就凡事只是无为地去做而不去问为什么了，这才叫作真正的道境界。被喜怒爱恶欲所驱使的心意之妄动与其内在的思辨觉知之作用是一个本原，然而却不知道此二者都是一个本原，这就叫作只知朝四而不知暮三的猴子意识。为什么叫作只知朝四而不知暮三的猴子意识呢？管理猴子的人给猴子喂食橡实时说：“早晨喂三个晚上喂四个。”众猴子闻之则大怒；管理猴子的人又说：“既然大家都不同意，那就还按早晨喂四个晚上喂三个的老法子。”众猴子们则闻之则大喜。名字之类的名相与名相所代表的那个实物的本性原本就没有什么不同，然而在人们的喜怒哀乐恶欲的心理作用下却变成了各种不同的理解，这就是因为那个只知朝四而不知暮三的意识在作怪啊。所以恢复了道心的圣人们既能在外行上将各种矛盾对立的意识调节控制在中和的状态，又能在内行上安闲自在于道心大用之遍行的无二无别的精神境界中，这就叫作一个道心境界而同时有着内外两种行径的圣行。 古之人，其知有所至矣。恶乎至？有以为未始有物者，至矣，尽矣，不可以加矣！其次以为有物矣，而未始有封也。其次以为有封焉，而未始有是非也。是非之彰也，道之所以亏也。道之所以亏，爱之所以成。果且有成与亏乎哉？果且无成与亏乎哉？有成与亏，故昭氏之鼓琴也；无成与亏，故昭氏之不鼓琴也。昭文之鼓琴也，师旷之枝策也，惠子之据梧也，三者之知几乎！皆其盛者也，故载之末年。唯其好之也，以异于彼，其好之也，欲以明之。彼非所明而明之，故以坚白之昧终。而其子又以文之纶终，终身无成。若是而可谓成乎，虽我亦成也；若是而不可谓成乎，物与我无成也。是故滑疑之耀，圣人之所图也。为是不用而寓诸庸，此之谓“以明”。 古时候的人，他们所具足的大智慧任何地方都能无所不至啊！都是打倒了什么样的无所不至呢？凡所做出的任何行为从根本上就没有做事之动机的存在，就是具足了无所不至的大智慧的境界了，当具足了这种无所不至的大智慧后，就再也不会被喜怒爱恶欲的习气所污染了。差一点的境界则仅仅才有了做事的动机，但却没有达到“这是我的，那是你的”的私我意识；再差一点的境界也只是有了“这是我的，那是你的”的私我意识，但却没有形成以我见而划分对错是非的唯我意识。后来我人便有了对错是非之心而且越来越严重了，于是道心也就亏失了；道心之所以亏失了，完全是心里的那个喜欢之所用所导致的。难道那个道心果真会圆满也会亏失吗，还是果真不会圆满也不会亏失呢？那个道心确实会圆满也会亏失，所以当大琴师昭氏鼓琴时便能激荡起人们心神的喜悦而造成道心之静的亏失了啊；那个道心确实不会圆满也不会亏失，所以当大琴师昭氏不鼓琴时人们的心神便恢复了道心之静的常态了。大琴师昭文的鼓琴之技艺，大乐师师旷持策以击乐器，大逻辑家惠子的诡辩之造诣，就这三个人对于各自爱好之事所达到的极其微妙的程度而言，都是各自领域里的顶尖高手，所以才能千古流传下来。由此三人在琴、诊、论上有着超乎寻常的爱好心，所以才使得在琴、诊、论的成就上大大超出了其他方面的技能；他们之所以爱好其所好的东西，其目的就是为了彻底研究明白所爱好之东西的内在机制。所爱好之东西的内在机制本来就不是靠研究所能研究明白的事反而执意地要去研究明白它，所以才犯了坚石非坚、白马非马之逻辑式的错误以至于愚昧终生。像他们那样地达到了极其微妙的程度就算是真正的最高成就了吗？那么我们每一个意识心用事的人也就可以说是成道的圣人了。像他们那样地达到了极其微妙的程度不算是真正的最高成就吗？那么万物与我人也就不会成为个别的个体了。所以，只有这样地将那个大道的玄机深究到大生疑惑的境地才能不知不觉地过渡到恍然大悟的境界，这正是圣人所希望的啊！只有达到了恍然大悟的境界才能没有是非对错之意识而一切行为都隐含在道心的无为之大用中，所以也就没有了意识分别心地将一切行为都隐含在恰到好处的无之用中了。 今且有言于此，不知其与是类乎？其与是不类乎？类与不类，相与为类，则与彼无以异矣。虽然，请尝言之：有始也者，有未始有始也者，有未始有夫未始有始也者；有有也者，有无也者，有未始有无也者，有未始有夫未始有无也者。俄而有无矣，而未知有无之果孰有孰无也。今我则已有有谓矣，而未知吾所谓之其果有谓乎？其果无谓乎？ 刚才已将“是非之彰也，道之所以亏也”的道理说过了，那么它与“为是不用而寓诸庸”的道理是一回事呢，还是与“为是不用而寓诸庸”的道理不是一回事呢？就以人们所认为的是一回事与不是一回事的那个认识而言，认识本身的那个思维所产生的心行就是同一回事，如此也就理解“是非之彰也，道之所以亏也”的道理与“为是不用而寓诸庸”的道理没有什么不同了。尽管如此，也需要试着将其道理说明如下。有的心行是从意识中突然产生的，有的心行是从末那识中迁流过来的，有的心行是从藏识中迁流过来的。来源于意识中的心行有的正存在着，有的已经过去了，来源于末那识的心行有的出于旧有的执着、有的出于最近才有的心得，来源于藏识中的心行乃是既没有形成执着，又没有形成心得的无为之举。心行就是这样地一会儿是从意识中产生出来，一会儿是从藏识中迁流出来，却不知道这两张心行的现象，它是不是真实的存在呢？现在用哪个会思会想的意识心反思一下自己就会发现所谓的“我”是不真实的假象了，而那个不会思不会想的道心之吾则是真实存在的了。道心之吾果真是真实的存在啊！意识心之假我果真是不真实的存在啊！ 夫天下莫大于秋毫之末，而太山为小；莫寿乎殇子，而彭祖为夭。天地与我并生，而万物与我为一。既已为一矣，且得有言乎？既已谓之一矣，且得无言乎？一与言为二，二与一为三。自此以往，巧历不能得，而况其凡乎！故自无适有，以至于三，而况自有适有乎！无适焉，因是已！ 整个的天下在实相上不会比秋毫的尖头大，相形之下那有形的大山却是渺小得很；我们的寿命在实相上不会比夭折的小儿寿长，相形之下那八百寿的彭祖却是夭亡的小儿；这就证明天地与我们都是道心之大用的妙用所生，万物与我们都是一个本体而没有人我万物之别。既然天地人我万物都是一个本体的“一”而没有差别，为什么会有天地人我万物之不同的认识存在呢？既然认为它们都是一个一个的个体而有差别，又怎会没有天地人我万物之不同的认识存在呢？道之实相用言辞说出来就变成第二重的言辞之比喻象了，将道的比喻象再用言辞说出来就变成第三重的意识之妄见象了。自从道之实相变成言辞之比喻象再变成意识之妄见象以来所代代递增的妄见象，即使技术最高超的人来推算也不能计算清楚它，更何况还有以后所必然会形成的意识之妄见象呢！从道的不可言说的实相上就能变成言辞之比喻象以至于再变成意识之妄见象，更何况将意识之妄见象再变成妄见的言辞之比喻象呢！由此而知，意识之妄见再变成意识之妄见的沿流永远没有一个尽头，原因就在于妄见的沿流层出不穷。 夫道未始有封，言未始有常，为是而有畛也。请言其畛：有左有右，有伦有义，有分有辩，有竞有争，此之谓八德。六合之外，圣人存而不论；六合之内，圣人论而不议；春秋经世先王之志，圣人议而不辩。 对于那个道心的大用而言在最初并没有“这是我的，那是你的”的私我之意识，对于言辞而言却从一开始便不存在永久不变，正是因为意识之妄见的沿流永远没有一个尽头而从道心之大用中形成了人我万物的各个个体及各种思想意识了。请让我说说这些界限：有上下、尊卑之序，有亲疏之理、贵贱之仪，有剖析万物、分别彼此，有角逐胜负、对辩是非，这就是儒、墨等派所争辩的八种界限。对于宇宙之外的不可知，圣人只是保留其说而不加以讲解；对于宇宙之内的事物，圣人只是讲解而不加以评说；对于以天道中正为喻的《春秋》所记载的经纶天下的先王的济世之道，圣人只是评说而不加以争辩。 故分也者，有不分也；辩也者，有不辩也。曰：“何也？”“圣人怀之，众人辩之以相示也。故曰：辩也者，有不见也。”夫大道不称，大辩不言，大仁不仁，大廉不嗛，大勇不忮。道昭而不道，言辩而不及，仁常而不成，廉清而不信，勇忮而不成。五者圆而几向方矣！故知止其所不知，至矣。孰知不言之辩，不道之道？若有能知，此之谓天府。注焉而不满，酌焉而不竭，而不知其所由来，此之谓葆光。 由圣人的做法可知，既然有了是非对错的认识之分别，这本身就说明道心原本就是一而没有认识之分别；既然有了是非对错的争辩，这本身就说明道心原本就是一而没有争辩之实质。这证明了什么呢？证明了圣人是思虑于心，而众人是妄议于言为了将这个道理开示出来。所以告诉大家，“之所以存在着争辩的现象，乃是还没有达到内明的缘故。”真正的大道没有任何名相之称谓，真正的大辩没有任何说道之言辞，真正的大仁没有任何仁慈之目的，真正的大廉是不自需锋芒的，大勇是不自逞血气之勇的。如果还有有道的样子显现出来则恰恰证明没有道，言语过于辩察，就不能达到真理，仁者滞于一偏之爱就不能周遍，过分表示廉洁就不够真实，自逞血气之勇就不成器大勇。不称、不言、不仁、不嗛、不忮这五个方面，本来是圆通混成的；如果涉及昭、辩、常、清、忮等形迹，就变成四方之物了。所以道心之知的明了作用若能停留在只管去做而不去动其心的境界上，那么修心的境界也就到家了。怎能又去管它什么没有言辞才是真正的论辩，不认为有道才是真正的有道呢？如果保持住了那个能知而又不去动心的境界，也就叫作修心阶段的最高成就了。只有达到了像大海一样地诸如多少也不会满溢，舀取多少也不会枯竭，而且也不去思考为什么会这样的境界时，这才叫作包藏光亮而不露。 故昔者尧问于舜曰：“吾欲伐宗脍、胥、敖，南面而不释然。其故何也？”舜曰：“夫三子者，犹存乎蓬艾之间。若不释然何哉！昔者十日并出，万物皆照，而况德之进乎日者乎！” 在过去的时候尧帝曾考问于大位的接班人舜帝说：“我真想消灭宗、脍、胥这三个屡犯边庭的小国，但是朕想到天子的责任又放不下对他们的牵挂，这个原因是什么呢？”舜帝回答说：“这三个小国的国君，竟然还能够在偏小卑微之地生存下来也真是不容易，你感到心情不怡悦。这说明了什么呢？过去曾有十个太阳同时普照大地，万物皆能普受恩惠，更何况您的道德境界胜过十个太阳的普照呢？” 啮缺问乎王倪曰：“子知物之所同是乎？”曰：“吾恶乎知之！”“子知子之所不知邪？”曰：“吾恶乎知之！”“然则物无知邪？”曰：“吾恶乎知之！虽然，尝试言之：庸讵知吾所谓知之非不知邪？庸讵知吾所谓不知之非知邪？且吾尝试问乎女：民湿寝则腰疾偏死，鳅然乎哉？木处则惴栗恂惧，猨猴然乎哉？三者孰知正处？民食刍豢，麋鹿食荐，蝍蛆甘带，鸱鸦耆鼠，四者孰知正味？猨猵狙以为雌，麋与鹿交，鳅与鱼游。毛嫱丽姬，人之所美也；鱼见之深入，鸟见之高飞，麋鹿见之决骤，四者孰知天下之正色哉？自我观之，仁义之端，是非之涂，樊然淆乱，吾恶能知其辩！”啮缺曰：“子不利害，则至人固不知利害乎？”王倪曰：“至人神矣！大泽焚而不能热，河汉冱而不能寒，疾雷破山、飘风振海而不能惊。若然者，乘云气，骑日月，而游乎四海之外，死生无变于己，而况利害之端乎！” 啮缺问于王倪说：“先生知道万物所共同的那个根本吗？”王倪回答说：“道心之吾为什么要去知道呢！”啮缺又问道：“先生知道您那个为什么不去知道吗？”王倪回答说：“道心之吾为什么要去知道呢！”啮缺进一步问道：“那么万物是否也都是一样的什么也不去知道吗？”王倪回答道：道心之吾为什么要去知道呢！尽管如此，也需要试着将其道理说明。怎么能知道我所说的道心的大用止于“知”，也就是道心之吾的那个“知”并不是不能知道啊？道心的大用止于“知”，则正是道心之吾的那个“知”明明就在却不去知啊！现在吾且试着问于你：我们如果在潮湿的地方安歇就会腰痛乃至偏瘫，泥鳅会这样吗？我们如果在树木上安歇就会担心掉下来而心惊恐惧，猿猴会这样吗？对于人、泥鳅、猿猴这三种动物的安歇方式而言哪一种是正确的呢？我人喜欢食食草的及喂养的动物，麋鹿喜欢食草类植物，蜈蚣喜欢食蛇之类的爬行动物，鹞鹰喜欢食鼠类动物，对于人、麋鹿、蜈蚣、鹞鹰这四种动物的口味而言哪一种是正确的呢？濑猴与猴子互相吸引，狍子与鹿互相倾情，泥鳅与鱼互相交欢。毛嫱与丽姬，在我们眼里都是最具吸引力的大美女；然而鱼见了她们却恶心地躲入深水中，飞鸟见到她们却鄙视地高飞而去，麋鹿见到她们却厌恶地急速地跑走。对于猿狙、麋鹿、鳅鱼、人这四种动物的审美观点而言哪一种是正确的呢？从我们的意识心上分析，讲究仁义的开始，便是是非涂抹道心的开始，像这样互相缠绕得杂乱的意识行为，道心之吾又怎能分清仁义与是非的观念是谁对谁错呢！啮缺又问道：“看来现实您是不知道凡事对自己的利与害了，难道心身行为最完美的人也是原本就不知道凡事对自己的利与害吗？”王倪回答道：“心身行为最完美的人可就神通广大了，即使四大海都变得干涸而燃烧起来他也不会感到热，即使宇宙都冻得成冰了他也不会感到冷，即使霹雳能击破山岳、狂风能掀起海啸也不会吓着他。如果达到了这种境界的话，就能够乘云气、骑日月而逍遥地游乎环宇之内了；即使人们所认为的死生也不会改变自己的道心之大用了，更何况是人们所认为的利与害的小事呢！” 瞿鹊子问乎长梧子曰：“吾闻诸夫子：圣人不从事于务，不就利，不违害，不喜求，不缘道，无谓有谓，有谓无谓，而游乎尘垢之外。夫子以为孟浪之言，而我以为妙道之行也。吾子以为奚若？” 瞿鹊子问于长梧子说：我听孔子说：“圣人在对待立身处世之心身行为的培养上没有必须要如何如何的想法，既没有获取利益之心，也没有躲避害处之心，既没有必得的执着之心，也没有等待外缘具备时再去做的懒散之心，只有心里真正达到了无所谓时才算是有了道心的成就了，若是心里有一点儿所谓的影子那就还没有真正达到道心的境界，如此才能使自己的心行既在世俗中生活又不受各种世俗之心对道心的污染。”先生您却认为这是好高骛远之言，而我却认为是最好的修道宗旨，作为得道的先生您认为咱俩的看法谁对呢？ 长梧子曰：“是黄帝之所听荧也，而丘也何足以知之！且女亦大早计，见卵而求时夜，见弹而求鸮炙。予尝为女妄言之，女以妄听之。奚旁日月，挟宇宙，为其脗合，置其滑涽，以隶相尊？众人役役，圣人愚芚，参万岁而一成纯。万物尽然，而以是相蕴。予恶乎知说生之非惑邪！予恶乎知恶死之非弱丧而不知归者邪！ 长梧子回答说：孔子所说的境界，恰恰是轩辕皇帝毫无意识心地任由那个道心的作用慢慢地明亮后的境界，单凭听了孔丘所说的几句道理又何足以真正地知道那个境界呢！而且你也太过于听风即是雨了，似乎见到鸡蛋就得到了会即时报更的公鸡了，见到弹弓就得到了香喷喷的烤鸮了。我试着为你妄言之，你也妄听之。如何才能与日月为邻，驾驭宇宙呢，唯一的办法就是必须完全做到与孔子所说的道理相吻合，对当下的心念不管是明白地知道还是糊里糊涂都不去管它，这才能符合孔子所说的道理。不要像众人役使自己的身心为自己的欲望服劳役那样地往外求，要像圣人傻傻地待在葆光的境界里那样，如此才能参究透一万年也只不过纯属当下的一念所变现而已；万物也都是如此，也都是以这个当下一念的作用在积蓄着生命的能量。我怎么知道喜欢活着的心理并不是内心不明之迷惑呢？我怎么知道厌恶死亡的心理并不是不害怕死亡呢？我怎么知道人们害怕死亡就像弱孩迷失在他乡而不知回归故乡呢！ 丽之姬，艾封人之子也。晋国之始得之也，涕泣沾襟。及其至于王所，与王同匡床，食刍豢，而后悔其泣也。予恶乎知夫死者不悔其始之蕲生乎？梦饮酒者，旦而哭泣；梦哭泣者，旦而田猎。方其梦也，不知其梦也。梦之中又占其梦焉，觉而后知其梦也。且有大觉而后知此其大梦也，而愚者自以为觉，窃窃然知之。“君乎！牧乎！”固哉！丘也与女皆梦也，予谓女梦亦梦也。是其言也，其名为吊诡。万世之后而一遇大圣知其解者，是旦暮遇之也。 丽这个地方的美女丽姬，原是丽地一个名叫“艾”的城邑之长官的女儿；晋国的国君刚选她入宫时，她不知道这是好运气来了反而吓得哭成了泪人儿，等到她到了王宫之后，没想到竟然是与国君同吃同住，而且餐餐都是食草的及喂养的动物肉，于是就后悔当初的无知而暗自落泪。从这类事上难道我们还不明白“那些怕死的人岂不应该后悔当初怕死”的无知之心理吗！夜里梦到席间饮宴娱乐的梦，第二天往往会遇到伤心而哭泣的事；夜里梦到伤心而哭泣的梦，第二天往往会遇到驰骋田猎而心旷神怡的事。当时再梦中的时候，并不知道自己是在做梦。而且还在梦中将所梦达成了真实的事，只到醒了之后才知道那是虚幻的梦境而非真实。还有一种情况是，彻底恢复了道心之内明而后明白了夜里的梦境与白天的所作所为，它们都是非常虚幻的。然而我们未恢复道心的所有凡夫却自以为所捞到的财官名利皆是自己最聪明的结果，且暗自为自己的聪明而高兴，真不知道自己是人生的主人，还是被人生所放牧的犟牛，这可是多累劫以来所形成的根深蒂固的虚妄习气所致使的啊！孔子给你所讲的那些道理，皆都是梦话；就连我所说的“孔子给你所讲的那些道理都是梦话”的话，也同样是梦话啊！我刚才的说法，名字就叫作为了归于道用之实相而截断各种妄见的接引后学的吊诡之法。假若万世之后能够遇到一位恢复了道心的大圣人，那也是在这种接引之法的接引下，起到了早晨听了道而晚上也就明白了的作用而已。 既使我与若辩矣，若胜我，我不若胜，若果是也？我果非也邪？我胜若，若不吾胜，我果是也？而果非也邪？其或是也？其或非也邪？其俱是也？其俱非也邪？我与若不能相知也。则人固受其黮暗，吾谁使正之？使同乎若者正之，既与若同矣，恶能正之？使同乎我者正之，既同乎我矣，恶能正之？使异乎我与若者正之，既异乎我与若矣，恶能正之？使同乎我与若者正之，既同乎我与若矣，恶能正之？然则我与若与人俱不能相知也，而待彼也邪？” 假使我与你再论辩，如果你胜了我，那么我就胜不了你，难道你就果真对了，我就果真错了吗？如果我胜了你，那么你就胜不了我，难道我就果真对了，你就果真错了吗？无论你认为是你果真对了还是我果真对了，岂不皆证明你的认为是出于意识心的了吗？如果你认为你与我的论辩都是对的，岂不证明你与我的论辩皆是出于意识的了吗？既然你与我之间不能论辩得明白，那么我人就会一直处于自己所证得的绝非道心内明的黑暗之中，那么我人的道心又怎能通过纠正自己的虚妄性而得到恢复呢？如果用与你的见地相同的人来纠正你的虚妄性，既然与你的见地相同，又怎能纠正得了呢？如果用与我的见地相同的人来纠正你的虚妄性，既然与我的见地相同，又怎能纠正得了你呢？如果用与你我的见地皆不相同的人来纠正你的虚妄性，既然与你我的见地皆不相同，又怎能纠正你的虚妄性呢？如果用与你我的见地皆相同的人来纠正你的虚妄性，既然与你我的意见皆相同，又怎能纠正你的虚妄性呢？然而反过来看我与你，也像世人一样的都是不明白自己的人，还必须有赖于吊诡之辩才能最终明白自己啊！ 化声之相待，若其不相待。和之以天倪，因之以曼衍，所以穷年也。“何谓和之以天倪？”曰：“是不是，然不然。是若果是也，则是之异乎不是也亦无辩；然若果然也，则然之异乎不然也亦无辩。忘年忘义，振于无竟，故寓诸无竟。” 怎能将极细微的起心动念控制在中和的程度呢？长梧子回答道：主观上所认定的对了，与道用上的客观事实一定是不相符的；主观上所认定的模样，与道用上的客观模样一定是相矛盾的。主观上所认定的对了，如果与道用上的客观事实是相符的，这个主观上所认定的也就达到非思维、言语所能辩的道用之实相的境界了。主观上所认定的模样，如果与道用上的客观模样是一致的，这个主观上所认定的模样也就达到非思维、言语所能辩的道用之实相的境界了。是与非纠缠在一起，如果那个审明辨知不起作用了，那就必须将起审明辨知之作用的那个极微细的起心动念时控制在中和的程度了，如果让那个极微的起心动念游衍自得，那就永远也不会恢复道心的明性而丧失审明辨知的作用了。当自己的心理上没有了时间及情义的概念时，才能像大鹏怒而飞一样地振作于不可预料的尘世生活环境中，于是也就能在各种各样的尘世生活中燕处超然了。 罔两问景曰：“曩子行，今子止；曩子坐，今子起。何其无特操与？”景曰：“吾有待而然者邪？吾所待又有待而然者邪？吾待蛇蚹蜩翼邪？恶识所以然？恶识所以不然？” 影子的影子问影子说：“正当我刚刚跟着你走的时候，你却突然又停了下来；正当我刚刚跟着你坐下来的时候，你却突然又站了起来；你怎么就如此地没有独立的志操呢？”影子回答说：“我后面还有一个对应地管束我的东西在如此地做吗？我后面的那个对应地管束我的东西的后面还有一个对应地管束它的东西在如此地做吗？对应的我只是像蛇腹下的横鳞或蝉的翼一样地被动地随之而动而已吗？我怎么能知道我为什么会一会儿被这样？又怎么能知道为什么一会儿会不被这样呢？” 昔者庄周梦为胡蝶，栩栩然胡蝶也。自喻适志与！不知周也。俄然觉，则蘧蘧然周也。不知周之梦为胡蝶与？胡蝶之梦为周与？周与胡蝶则必有分矣。此之谓物化。 过去庄周我曾经梦到自己变成了蝴蝶，活灵活现的一只真蝴蝶的样子，当时自己心里很明白自己就是这只蝴蝶而且非常惬意啊，已经完全不知道自己原本是庄周了；一会儿梦醒了，也就又恢复成了现实中的庄周我了。真不知道是庄周我在梦里化为了蝴蝶呢，还是蝴蝶在梦里化为了庄周我呢！对于庄周我与蝴蝶在梦中互化的现象而言，必然是在不同的心力下所产生的不同的物类及个体而已，这就是形身在心力的作用下从一个物类个体又转化为另一个物类个体的物化现象。"},{"title":"内篇（一）——逍遥游","date":"2021-05-29T04:56:11.250Z","url":"/2021/05/29/%E5%86%85%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E9%80%8D%E9%81%A5%E6%B8%B8/","tags":[["读书","/tags/%E8%AF%BB%E4%B9%A6/"]],"categories":[["庄子","/categories/%E5%BA%84%E5%AD%90/"]],"content":" 北冥有鱼，其名为鲲。鲲之大，不知其几千里也。化而为鸟，其名为鹏。鹏之背，不知其几千里也。怒而飞，其翼若垂天之云。是鸟也，海运则将徙于南冥。南冥者，天池也。 北方的大海里有一条鱼，它的名字叫鲲。鲲的体积，不知道大到有几千里。变化成为鸟，它的名字就叫做鹏。鹏的脊背，真不知道长到有几千里；当它奋起而飞的时候，那展开的翅膀就好像天边的云。这只鹏鸟，当海动风气的时候就要迁徙到南方的大海去了。南方的大海是一个天然的大池子。 《齐谐》者，志怪者也。《谐》之言曰：“鹏之徙于南冥也，水击三千里，抟扶摇而上者九万里，去以六月息者也。”野马也，尘埃也，生物之以息相吹也。天之苍苍，其正色邪？其远而无所至极邪？其视下也，亦若是则已矣。 《齐谐》是一部专门记载怪异事情的书。这本书上记载：“鹏往南方的大海迁徙的时候，翅膀拍打水面，能激起三千里的浪涛，环绕着旋风飞上了九万里的高空，乘着六月的风离开了北海。”像野马奔腾一样的游气，飘飘扬扬的尘埃，都是活动着的生物的气息相互吹拂所致。天空是那么湛蓝湛蓝的，难道就是它真正的颜色吗？还是因为天空高远而看不到尽头呢？鹏鸟从高空往下看的时候，也不过就像这个样子罢了。 且夫水之积也不厚，则其负大舟也无力。覆杯水于坳堂之上，则芥为之舟。置杯焉则胶，水浅而舟大也。风之积也不厚，则其负大翼也无力。故九万里则风斯在下矣，而后乃今培风；背负青天而莫之夭阏者，而后乃今将图南。 如果聚集的水不深，那么它就没有负载一艘大船的力量了。在堂前低洼的地方倒上一杯水，一棵小草就能被当作是一艘船，放一个杯子在上面就会被粘住，这是水浅而船却大的原因。如果聚集的风不够强大的话，那么负载一个巨大的翅膀也就没有力量了。因此，鹏在九万里的高空飞行，风就在它的身下了，凭借着风力，背负着青天毫无阻挡，然后才开始朝南飞。 蜩与学鸠笑之曰：“我决起而飞，抢榆枋而止，时则不至，而控于地而已矣，奚以之九万里而南为？”适莽苍者，三餐而反，腹犹果然；适百里者，宿舂粮；适千里者，三月聚粮。之二虫又何知！ 蝉和小斑鸠讥笑鹏说：“我们奋力而飞，碰到榆树和檀树就停止，有时飞不上去，落在地上就是了。何必要飞九万里到南海去呢？”到近郊去的人，只带当天吃的三餐粮食，回来肚子还是饱饱的；到百里外的人，要用一整夜时间舂米准备干粮；到千里外的人，要聚积三个月的粮食。蝉和小斑鸠这两只小虫、鸟又知道什么呢。 小知不及大知，小年不及大年。奚以知其然也？朝菌不知晦朔，蟪蛄不知春秋，此小年也。楚之南有冥灵者，以五百岁为春，五百岁为秋；上古有大椿者，以八千岁为春，八千岁为秋，此大年也。而彭祖乃今以久特闻，众人匹之，不亦悲乎！ 小智比不上大智，短命比不上长寿。怎么知道是这样的呢？朝生暮死的菌类不知道是一天。春生夏死、夏生秋死的寒蝉，不知道一年的时光，这就是短命。楚国的南方有一种大树（灵龟），它把五百年当作一个春季，五百年当作一个秋季。上古时代有一种树叫做大椿，它把八千年当作一个春季，八千年当作一个秋季，这就是长寿。可是彭祖到如今还是以年寿长久而闻名于世，人们与他攀比，岂不可悲可叹！ 汤之问棘也是已：穷发之北，有冥海者，天池也。有鱼焉，其广数千里，未有知其修者，其名为鲲。有鸟焉，其名为鹏，背若泰山，翼若垂天之云，抟扶摇羊角而上者九万里，绝云气，负青天，然后图南，且适南冥也。 商汤问棘的话也是这样的：“在草木不生的极远的北方，有个很深的大海，那就是天池。里面有条鱼，它的身子有几千里宽，没有人知道它有多长，它的名字叫做鲲。有一只鸟，它的名字叫做鹏。鹏的背像泰山，翅膀像天边的云；借着旋风盘旋而上九万里，超越云层，背负青天，然后向南飞翔，将要飞到南海去。 斥鴳笑之曰：“彼且奚适也？我腾跃而上，不过数仞而下，翱翔蓬蒿之间，此亦飞之至也，而彼且奚适也？”此小大之辩也。 小泽里的麻雀讥笑鹏说：‘它要飞到哪里去呢？我一跳就飞起来，不过数丈高就落下来，在蓬蒿丛中盘旋，这也是极好的飞行了。而它还要飞到哪里去呢？’”这就是小和大的不同了。 故夫知效一官，行比一乡，德合一君，而征一国者，其自视也，亦若此矣。而宋荣子犹然笑之。且举世而誉之而不加劝，举世而非之而不加沮，定乎内外之分，辩乎荣辱之境，斯已矣。彼其于世，未数数然也。虽然，犹有未树也。 所以，那些才智能胜任一官的职守，行为能够庇护一乡百姓的，德行能投合一个君王的心意的，能力能够取得全国信任的，他们看待自己，也像上面说的那只小鸟一样。而宋荣子对这种人加以嘲笑。宋荣子这个人，世上所有的人都称赞他，他并不因此就特别奋勉，世上所有的人都诽谤他，他也并不因此就感到沮丧。他认定了对自己和对外物的分寸，分辨清楚荣辱的界限，就觉得不过如此罢了。他对待人世间的一切，都没有拼命去追求。即使如此，他还是有未达到的境界。 夫列子御风而行，泠然善也，旬有五日而后反。彼于致福者，未数数然也。此虽免乎行，犹有所待者也。 列子乘风而行，飘然自得，驾轻就熟。十五天以后返回；他对于求福的事，没有拼命去追求。这样虽然免了步行，还是有所凭借的。 若夫乘天地之正，而御六气之辩，以游无穷者，彼且恶乎待哉！故曰：至人无己，神人无功，圣人无名。 倘若顺应天地万物的本性，驾驭着六气的变化，邀游于无穷的境地，他还要凭借什么呢？所以说：修养最高的人能任顺自然、忘掉自己，修养达到神化不测境界的人无意于求功，有道德学问的圣人无意于求名。 尧让天下于许由，曰：“日月出矣，而爝火不息，其于光也，不亦难乎！时雨降矣，而犹浸灌，其于泽也，不亦劳乎！夫子立而天下治，而我犹尸之，吾自视缺然。请致天下。”许由曰：“子治天下，天下既已治也，而我犹代子，吾将为名乎？名者，实之宾也，吾将为宾乎？鹪鹩巢于深林，不过一枝；偃鼠饮河，不过满腹。归休乎君，予无所用天下为！庖人虽不治庖，尸祝不越樽俎而代之矣。” 尧要把天下让给许由，说：“太阳月亮出来了，而小火把还不熄灭，它的亮度，要和日月相比不是太难了吗！及时雨降下了，还要灌溉田地，对于滋润禾苗，不是徒劳吗！你如果成了君王，天下一定大治，而我还徒居其位，我自己感到惭愧极了，请允许我把天下交给你。”许由说：“你治理天下，天下已经治理好了，而我再接替你，我岂不是为名而来吗？名，是依附于实的客体，我难道要做有名无实的客体吗？鹪鹩在深林中筑巢，只要一根树枝；鼹鼠饮河水，只要肚子喝饱。请你回去吧，天下对于我没有什么用！厨子虽然不下厨，主祭的人却不应该超越权限而代行厨子的职事。” 肩吾问于连叔曰：“吾闻言于接舆，大而无当，往而不返。吾惊怖其言犹河汉而无极也，大有径庭，不近人情焉。”连叔曰：“其言谓何哉？”“曰‘藐姑射之山，有神人居焉。肌肤若冰雪，淖约若处子；不食五谷，吸风饮露；乘云气，御飞龙，而游乎四海之外；其神凝，使物不疵疠而年谷熟。’吾以是狂而不信也。”连叔曰：“然，瞽者无以与乎文章之观，聋者无以与乎钟鼓之声。岂唯形骸有聋盲哉？夫知亦有之。是其言也，犹时女也。之人也，之德也，将旁礴万物以为一，世蕲乎乱，孰弊弊焉以天下为事！之人也，物莫之伤，大浸稽天而不溺，大旱金石流、土山焦而不热。是其尘垢粃糠，将犹陶铸尧舜者也，孰肯以物为事！” 肩吾向连叔求教：“我从接舆那里听到谈话，大话连篇没有边际，一说下去就回不到原来的话题上。我十分惊恐他的言谈，就好像天上的银河没有边际，跟一般人的言谈差异甚远，确实是太不近情理了。”连叔问：“他说的是些什么呢？”肩吾转述道：“‘在遥远的姑射山上，住着一位神人，皮肤润白像冰雪，体态柔美如处女，不食五谷，吸清风饮甘露，乘云气驾飞龙，遨游于四海之外。他的神情那么专注，使得世间万物不受病害，年年五谷丰登。’我认为这全是虚妄之言，一点也不可信。”连叔听后说：“是呀！对于瞎子没法同他们欣赏花纹和色彩，对于聋子没法同他们聆听钟鼓的乐声。难道只是形骸上有聋与瞎吗？思想上也有聋和瞎啊！这话似乎就是说你肩吾的呀。那位神人，他的德行，与万事万物混同一起，以此求得整个天下的治理，谁还会忙忙碌碌把管理天下当成回事！那样的人哪，外物没有什么能伤害他，滔天的大水不能淹没他，天下大旱使金石熔化、土山焦裂，他也不感到灼热。他所留下的尘埃以及瘪谷糠麸之类的废物，也可造就出尧舜那样的圣贤仁君来，他怎么会把忙着管理万物当作己任呢！ 宋人资章甫而适越，越人断发文身，无所用之。 北方的宋国有人贩卖帽子到南方的越国，越国人不蓄头发满身刺着花纹，没什么地方用得着帽子。 尧治天下之民，平海内之政。往见四子藐姑射之山，汾水之阳，窅然丧其天下焉。 尧治理好天下的百姓，安定了海内的政局，到姑射山上、汾水北面，去拜见四位得道的高士，不禁怅然若失，忘记了自己居于治理天下的地位。” 惠子谓庄子曰：“魏王贻我大瓠之种，我树之成而实五石。以盛水浆，其坚不能自举也。剖之以为瓢，则瓠落无所容。非不呺然大也，吾为其无用而掊之。”庄子曰：“夫子固拙于用大矣。宋人有善为不龟手之药者，世世以洴澼絖为事。客闻之，请买其方百金。聚族而谋之曰：‘我世世为澼絖，不过数金。今一朝而鬻技百金，请与之。’客得之，以说吴王。越有难，吴王使之将。冬，与越人水战，大败越人，裂地而封之。能不龟手一也，或以封，或不免于澼絖，则所用之异也。今子有五石之瓠，何不虑以为大樽而浮乎江湖，而忧其瓠落无所容？则夫子犹有蓬之心也夫！” 惠子对庄子说：“魏王送给我大葫芦的种子，我种下后结出的葫芦大得可以容纳五石。用它来盛水，它却因质地太脆无法提举。切开它当瓠，又大而平浅无法容纳东西。我不是嫌它不大，只是因为它无用，我把它砸了。”庄子说：“你真不善于使用大的物件。宋国有个人善于制作防止手冻裂的药，他家世世代代都以漂洗丝絮为职业。有个客人听说了，请求用一百金来买他的药方。这个宋国人召集全家商量说：‘我家世世代代靠这种药从事漂洗丝絮，一年所得不过数金；现在一旦卖掉这个药方马上可得百金，请大家答应我卖掉它。’这个客人买到药方，就去游说吴王。那时正逢越国有难，吴王就命他为将，在冬天跟越国人展开水战，大败越人，吴王就割地封侯来奖赏他。同样是一帖防止手冻裂的药方，有人靠它得到封赏，有人却只会用于漂洗丝絮，这是因为使用方法不同啊。现在你有可容五石东西的大葫芦，为什么不把它系在身上作为腰舟而浮游于江湖呢？却担忧它大而无处可容纳，可见你的心地过于浅陋狭隘了！” 惠子谓庄子曰：“吾有大树，人谓之樗。其大本臃肿而不中绳墨，其小枝卷曲而不中规矩。立之涂，匠者不顾。今子之言，大而无用，众所同去也。”庄子曰：“子独不见狸狌乎？卑身而伏，以候敖者；东西跳梁，不避高下；中于机辟，死于罔罟。今夫斄牛，其大若垂天之云。此能为大矣，而不能执鼠。今子有大树，患其无用，何不树之于无何有之乡，广莫之野，彷徨乎无为其侧，逍遥乎寝卧其下。不夭斤斧，物无害者，无所可用，安所困苦哉！” 惠子对庄子说：“我有一棵大树，人家把它叫做臭椿；它那树干上有许多赘瘤，不合绳墨，它那枝权弯弯曲曲，不合规矩。它长在路边，木匠都不看它一眼。现在你说的那段话，大而没有用，大家都不相信。”庄子说：“你难道没见过野猫和黄鼠狼吗？屈身伏在那里，等待捕捉来来往往的小动物；它捉小动物时东跳西跃，不避高下；但是一踏中捕兽的机关陷阱，就死在网中。再看那牦牛，它大如天边的云；这可以说够大的了，但是却不能捕鼠。现在你有一棵大树，担忧它没有用处，为什么不把它种在虚无之乡，广阔无边的原野，随意地徘徊在它的旁边，逍遥自在地躺在它的下面；这样大树就不会遭到斧头的砍伐，也没有什么东西会伤害它。它没有什么用处，又哪里会有什么困苦呢？”"},{"title":"python的内置类型","date":"2021-05-28T15:52:16.000Z","url":"/2021/05/28/python%E7%9A%84%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/","tags":[["python","/tags/python/"]],"categories":[["Python","/categories/Python/"]],"content":"字符串与字节Python3中只有一种能够保存文本信息的数据类型，就是str(string，字符串)。它是不可变的序列，保存的Unicode码位(code point)。 字符串可以保存的数据类型有非常明确的限制，就是Unicode文本。 bytes以及可变的bytearray与str不同，只能用字节作为序列值，即0&lt;=x&lt;256范围内的整数。 对于bytes和bytearray，在转换为另一种序列类型（例如list或tuple）时可以显示出其本来面目： 从Python3.0开始，所有没有前缀的字符串都是Unicode。因此，所有用单引号（’）、双引号（”）或成组的3个引号（单引号或双引号）包围且没有前缀的值都表示str数据类型： 字节也被单引号、双引号或三引号包围，但必须有一个b或B前缀： Python字符串是不可变的。字节序列也是如此。bytearray是bytes的可变版本，不存在这样的问题。字节数组可以通过元素赋值来进行原处修改（无需创建新对象），其大小也可以像列表一样动态地变化（利用append、pop、inseer等方法）。 字符串拼接用str.join()方法。它接受可迭代的字符串作为参数，返回合并后的字符串。 因为join()方法速度更快（对于大型列表来说更是如此），并不意味着在所有需要拼接两个字符串的情况下都应该使用这一方法。虽然这是一种广为认可的做法，但并不会提高代码的可读性。可读性是很重要的！在某些情况下，join()的性能可能还不如利用加法的普通拼接。 集合类型列表和元组列表是动态的，其大小可以改变；而元组是不可变的，一旦创建就不能修改。 tuple是不可变的（immutable），因此也是可哈希的（hashable） 从细节上来看，Python中的列表是由对其他对象的引用组成的的连续数组。指向这个数组的指针及其长度被保存在一个列表头结构中。这意味着，每次添加或删除一个元素时，由引用组成的数组需要改变大小（重新分配）。幸运的是，Python在创建这些数组时采用了指数过分配（exponential over-allocation），所以并不是每次操作都需要改变数组大小。这也是添加或取出元素的平摊复杂度较低的原因。不幸的是，在普通链表中“代价很小”的其他一些操作在Python中的计算复杂度却相对较高： 利用list.insert方法在任意位置插入一个元素——复杂度为O(n)。 利用list.delete或del删除一个元素——复杂度为O(n)。 列表推导 这种写法可能适用于C语言，但在Python中的实际运行速度很慢，原因如下。 解释器在每次循环中都需要判断序列中的哪一部分需要修改。 需要用一个计数器来跟踪需要处理的元素。 由于append()是一个列表方法，所以每次遍历时还需要额外执行一个查询函数。 列表推导正是解决这个问题的正确方法。它使用编排好的功能对上述语法的一部分做了自动化处理： 这种写法除了更加高效之外，也更加简短，涉及的语法元素也更少。在大型程序中，这意味着更少的错误，代码也更容易阅读和理解。 其他习语Python习语的另一个典型例子是使用enumerate（枚举）。在循环中使用序列时，这个内置函数可以很方便地获取其索引。以下面这段代码为例： 它可以替换为下面这段更短的代码： 序列解包（sequence unpacking）这种方法并不限于列表和元组，而是适用于任意序列类型（甚至包括字符串和字节序列）。只要赋值运算符左边的变量数目与序列中的元素数目相等，你都可以用这种方法将元素序列解包到另一组变量中： 解包还可以利用带星号的表达式获取单个变量中的多个元素，只要它的解释没有歧义即可。还可以对嵌套序列进行解包。特别是在遍历由序列构成的复杂数据结构时，这种方法非常实用。下面是一些更复杂的解包示例 字典字典是Python中最通用的数据结构之一。dict可以将一组唯一键映射到对应的值，如下所示： 可以用和前面列表推导类似的推导来创建一个新的字典。这里有一个非常简单的例子如下所示： 在许多情况下，字典推导要更加高效、更加简短、更加整洁。对于更复杂的代码而言，需要用到许多if语句或函数调用来创建一个字典，这时最好使用简单的for循环，尤其是它还提高了可读性。 对于python3，字典的keys()、values()和items()3个方法的返回值类型不再是列表。此外，与之对应的iterkeys()、itervalues()和iteritems()本来返回的是迭代器，而Python3中并没有这3个方法。现在keys()、values()和items()返回的是视图对象（viewobjects）。 keys()：返回dict_keys对象，可以查看字典的所有键。 values()：返回dict_values对象，可以查看字典的所有值。 items()：返回dict_items对象，可以查看字典所有的(key, value)二元元组。 视图对象可以动态查看字典的内容，因此每次字典发生变化时，视图都会相应改变，见下面这个例子： 视图对象既有旧的keys()、values()和items()方法返回的列表的特性，也有旧的iterkeys()、itervalues()和iteritems()方法返回的迭代器的特性。视图无需冗余地将所有值都保存在内存里（像列表那样），但你仍然可以获取其长度（使用len），也可以测试元素是否包含其中（使用in子句）。当然，视图是可迭代的。最后一件重要的事情是，在keys()和values()方法返回的视图中，键和值的顺序是完全对应的。 集合集合是一种鲁棒性很好的数据结构，当元素顺序的重要性不如元素的唯一性和测试元素是否包含在集合中的效率时，大部分情况下这种数据结构是很有用的。它与数学上的集合概念非常类似。Python的内置集合类型有两种。 set()：一种可变的、无序的、有限的集合，其元素是唯一的、不可变的（可哈希的）对象。 frozenset()：一种不可变的、可哈希的、无序的集合，其元素是唯一的、不可变的（可哈希的）对象。 由于frozenset()具有不变性，它可以用作字典的键，也可以作为其他set()和frozenset()的元素。在一个set()或frozenset()中不能包含另一个普通的可变set()，因为这会引发TypeError： 下面这种集合初始化的方法是完全正确的： 创建可变集合方法有以下3种，如下所示。 调用set()，选择性地接受可迭代对象作为初始化参数，例如set([0, 1, 2])。 使用集合推导，例如{element for element in range(3)}。 使用集合字面值，例如{1, 2, 3}。 使用集合的字面值和推导要格外小心，因为它们在形式上与字典的字面值和推导非常相似。此外，空的集合对象是没有字面值的。空的花括号{}表示的是空的字典字面值。"},{"title":"go语言的下载和安装","date":"2021-05-26T02:37:38.000Z","url":"/2021/05/26/go%E8%AF%AD%E8%A8%80%E7%9A%84%E4%B8%8B%E8%BD%BD%E5%92%8C%E5%AE%89%E8%A3%85/","tags":[["go","/tags/go/"]],"categories":[["Go","/categories/Go/"]],"content":"下载安装包访问下载对应系统的安装包，这里以windows平台为例。 安装安装下载后的安装文件。 使用推荐的默认设置就可以了。 增加环境变量在系统环境变量PATH中添加go的安装路径，比如C:\\Program Files\\Go\\bin。 添加完成后调出cmd输入go version有下面的显示说明安装成功了。 设置代理设置为国内代理，方便下载go模块： 设置方式： 运行go env查看设置是否成功。"},{"title":"Selenium的基本使用","date":"2021-05-25T13:15:51.000Z","url":"/2021/05/25/Selenium%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":[["selenium","/tags/selenium/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"在很多情况下，Ajax请求的接口通常会包含加密的参数，如token、sign等，如：，它的Ajax接口是包含一个token参数的，如图所示。 由于接口的请求加上了token参数，如果不深入分析并找到token的构造逻辑，是难以直接模拟这些Ajax请求的。 Selenium是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击、下拉等操作，同时还可以获取浏览器当前呈现的页面源代码，做到可见即可爬。对于一些使用JavaScript动态渲染的页面来说，此种抓取方式非常有效。 准备工作确保已经正确安装好了Chrome浏览器并配置好了ChromeDriver。另外，还需要正确安装好Python的Selenium库。 Selenium安装 chrome driver安装查看安装的浏览器版本，在chrome搜索栏输入chrome://version/，会显示带当前浏览器的版本信息。 在下载对应版本的chrome driver，一定要对应，否则不能使用。 下载到本地后，将chromedriver.exe分别拷贝到chrome.exe和python安装路径下。 以上，安装就完成了！ 基本使用首先来看一下Selenium有一些怎样的功能。示例如下： 运行代码后会自动弹出一个Chrome浏览器，浏览器会跳转到百度，然后在搜索框中输入Python，接着跳转到搜索结果页，如图所示。 此时在控制台的输出结果如下： 源代码过长，在此省略。可以看到，当前得到的URL、Cookies和源代码都是浏览器中的真实内容。所以说，如果用Selenium来驱动浏览器加载网页的话，就可以直接拿到JavaScript渲染的结果了，不用担心使用的是什么加密系统。 了解一下Selenium的用法。 声明浏览器对象Selenium支持非常多的浏览器，如Chrome、Firefox、Edge等，还有Android、BlackBerry等手机端的浏览器。 可以用如下方式进行初始化： 这样就完成了浏览器对象的初始化并将其赋值为browser对象。接下来，要做的就是调用browser对象，让其执行各个动作以模拟浏览器操作。 访问页面可以用get方法请求页面，只需要把参数传入连接URL即可。比如，这里用get方法访问淘宝，代码如下： 运行后会弹出Chrome浏览器并且自动访问淘宝，然后控制台会输出淘宝页面的源代码，随后浏览器关闭。 查找节点Selenium可以驱动浏览器完成各种操作，比如填充表单、模拟点击等。举个例子，当想要完成向某个输入框输入文字的操作时，首先需要知道这个输入框在哪，而 Selenium提供了一系列查找节点的方法，可以用这些方法来获取想要的节点，以便执行下一步动作或者提取信息。 单个节点想要从淘宝页面中提取搜索框这个节点，首先要观察它的源代码。 可以发现，它的id是q，name也是q，此外还有许多其他属性。此时就可以用多种方式获取它了。比如，find_element_by_name代表根据name值获取，find_element_by_id则是根据id获取，另外，还有根据XPath、CSS选择器等获取的方式。 这里使用3种方式获取输入框，分别是根据id、CSS选择器和XPath获取，它们返回的结果完全一致。运行结果如下： 这3个节点的类型是一致的，都是WebElement。 这里列出所有获取单个节点的方法： 多个节点多如果在网页中只查找一个目标，那么完全可以用find_element方法。但如果有多个节点需要查找，再用find_element方法，就只能得到第1个节点了。如果要查找所有满足条件的节点，需要用find_elements这样的 方法。注意，在这个方法的名称中， 注element多了一个多s，注意区分。 举个例子，假如要查找淘宝左侧导航条的所有条目，就可以这样来实现： 结果如下： 得到的内容变成了列表类型，列表中的每个节点都是WebElement类型。 也就是说，如果用find_element方法，只能获取匹配的第一个节点，结果是WebElement类型。如果用find_elements方法，则结果是列表类型，列表中的每个节点是WebElement类型。 这里列出所有获取多个节点的方法： 当然，也可以直接用find_elements方法来选择，这时可以这样写： 结果是完全一致的。 节点交互Selenium可以驱动浏览器来执行一些操作，或者说可以让浏览器模拟执行一些动作。比较常见的用法有：输入文字时用send_keys方法，清空文字时用clear方法，点击按钮时用click方法。示例如下： 这里首先驱动浏览器打开淘宝，用find_element_by_id方法获取输入框，然后用send_keys方法输入iPhone文字，等待一秒后用clear方法清空输入框，接着再次调用send_keys方法输入iPad文字，之后再用find_element_by_class_name方法获取搜索按钮，最后调用click方法完成搜索动作。 动作链在上面的实例中，一些交互动作都是针对某个节点执行的。比如，对于输入框，调用它的输入文字和清空文字方法；对于按钮，调用它的点击方法。其实，还有另外一些操作，它们没有特定的执行对象，比如鼠标拖拽、键盘按键等，这些动作用另一种方式来执行，那就是动作链。 比如，现在要实现一个节点的拖拽操作，将某个节点从一处拖拽到另外一处，可以这样实现： 打开网页中的一个拖拽实例，依次选中要拖拽的节点和拖拽到的目标节点，接着声明ActionChains对象并将其赋值为actions变量，然后通过调用actions变量的drag_and_drop方法，再调用perform方法执行动作，此时就完成了拖拽操作，如图所示： 拖拽前页面： 执行执JavaScriptSeleniumAPI并没有提供实现某些操作的方法，比如，下拉进度条。但它可以直接模拟运行JavaScript，此时使用execute_script方法即可实现，代码如下： 这里利用execute_script方法将进度条下拉到最底部，然后弹出alert提示框。 有了这个方法，基本上API没有提供的所有功能都可以用执行JavaScript的方式来实现了。 获取节点信息Selenium已经提供了选择节点的方法，并且返回的是WebElement类型，那么它也有相关的方法和属性来直接提取节点信息，如属性、文本等。这样的话，就可以不用通过解析源代码来提取信息了，非常方便。 获取属性可以使用get_attribute方法来获取节点的属性，但是前提是得先选中这个节点，示例如下： 运行之后，程序便会驱动浏览器打开该页面，然后获取class为logo-image的节点，最后打印出它的src属性。 返回结果： 通过get_attribute方法，只需要传入想要获取的属性名，就可以得到它的值了。 获取文本值每个WebElement节点都有text属性，直接调用这个属性就可以得到节点内部的文本信息，这相当于pyquery的text方法，示例如下： 结果如下： 获取获ID、位置、标签名、大小WebElement节点还有一些其他属性，比如id属性可以获取节点id，location属性可以获取该节点在页面中的相对位置，tag_name属性可以获取标签名称，size属性可以获取节点的大小，也就是宽高，这些 属性有时候还是很有用的。示例如下： 这里首先获得class为logo-title这个节点，然后调用其id、location、tag_name、size属性来获取对应的属性值。 切换Frame网页中有一种节点叫作iframe，也就是子Frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。Selenium打开页面后，默认是在父级Frame里面操作，而此时如果页面中还有子Frame，Selenium是不能获取到子Frame里面的节点的。这时就需要使用switch_to.frame方法来切换Frame。示例如下： 控制台输出： 首先通过switch_to.frame方法切换到子Frame里面，然后尝试获取子Frame里的logo节点（这是不能找到的），如果找不到的话，就会抛出NoSuchElementException异常，异常被捕捉之后，就会输出No logo found。接下来，我们需要重新切换回父级Frame，然后再次重新获取节点，发现此时可以成功获取了。 所以，当页面中包含子Frame时，如果想获取子Frame中的节点，需要先调用switch_to.frame方法切换到对应的Frame，然后再进行操作。 延时等待在Selenium中，get方法会在网页框架加载结束后结束执行，此时如果获取page_source，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，在网页源代码中也不一定能成功获取 \\到。所以，这里需要延时等待一定时间，确保节点已经加载出来。 这里等待的方式有两种：一种是隐式等待，一种是显式等待。 隐式等待当使用隐式等待执行测试的时候，如果Selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则抛出找不到节点的异常。换句话说，隐式等待可以在我们查找节点而节点并没有立即出现的时候， 等待一段时间再查找DOM，默认的时间是0。示例如下： "},{"title":"基于docker部署code-server","date":"2021-05-25T09:21:25.000Z","url":"/2021/05/25/%E5%9F%BA%E4%BA%8Edocker%E9%83%A8%E7%BD%B2code-server/","tags":[["docker","/tags/docker/"],["vscode","/tags/vscode/"]],"categories":[["Docker","/categories/Docker/"]],"content":"拉取镜像 创建挂载目录 配置文件启动一个容器：-u表示使用root用户运行 拉出配置文件 修改你的密码： 启动服务 登录访问，输入密码登录。 "},{"title":"腾讯云主机CPU占用100%","date":"2021-05-25T08:32:01.000Z","url":"/2021/05/25/%E8%85%BE%E8%AE%AF%E4%BA%91%E4%B8%BB%E6%9C%BACPU%E5%8D%A0%E7%94%A8%E7%99%BE%E5%88%86%E7%99%BE/","tags":[["linux","/tags/linux/"],["git","/tags/git/"],["病毒","/tags/%E7%97%85%E6%AF%92/"]],"categories":[["Linux","/categories/Linux/"]],"content":"异常情况使用ssh登录发现连接很缓慢，排除网络问题。登陆后使用top查看进程，发现git用户的两个进程占用了95%的CPU。 网上查了下，kswapd0被植入挖矿病毒，立马kill这个进程，CPU的负荷马上降了下来。 查看进程端口 查看程序路径 定时器查看 查看到定时任务，这个就是病毒的执行路径。 临时目录查看 处理办法 直接杀掉进程 删除所有的crontab计划任务 删除用户和文件（前提是正规业务没有用到此用户） 删除/tmp目录下test用户的文件 Xmrig挖矿木马之暴力分析可以参考大神的分析。 密码一定要复杂！！！"},{"title":"Ajax爬取案例实战","date":"2021-05-24T12:13:23.000Z","url":"/2021/05/24/Ajax%E7%88%AC%E5%8F%96%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/","tags":[["ajax","/tags/ajax/"],["python","/tags/python/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"准备工作 安装好Python 3（最低为 3.6 版本），并能成功运行Python 3程序。 了解PythonHTTP请求库requests的基本用法。 了解Ajax的基础知识和分析Ajax的基本方法。 爬取目标其链接为：，页面如图所示。 需要完成的目标有： 分析页面数据的加载逻辑。 用requests实现Ajax数据的爬取。 将每部电影的数据保存成一个JSON数据文件。 爬取列表页打开浏览器开发者工具，切换到Network面板，勾选上「Preserve Log」并切换到「XHR」选项卡，如图所示。 点开第2个结果，观察到其Ajax接口请求的URL地址为：，这里有两个参数，一个是limit，其值为10，一个是offset，它的值也是10。 切换到Preview选项卡，结果如图所示。 可以看到结果是一些JSON数据，它有一个results字段，这是一个列表，列表的每一个元素都是一个字典。观察一下字典的内容，可以看到对应的电影数据的字段了，如name、alias、cover、 categories，对比下浏览器中的真实数据，各个内容是完全一致的，而且这个数据已经非常结构化了，完全就是我们想要爬取的数据。 导入一些所需的库并定义一些配置，代码如下： 引入了requests和logging库，并定义了logging的基本配置，接着定义INDEX_URL，这里把limit和offset预留出来变成占位符，可以动态传入参数构造成一个完整的列表页URL。 下面来实现一下列表页的爬取，先定义一个通用的爬取方法，代码如下： 定义一个scrape_api方法，和之前不同的是，这个方法专门用来处理JSON接口，最后的response调用的是json方法，它可以解析响应的内容并将其转化成JSON字符串。 在这个基础之上，定义一个爬取列表页的方法，代码如下： 定义了一个scrape_index方法，用来接收参数page，page代表列表页的页码。 先构造了一个URL，通过字符串的format方法，传入limit和offset的值。这里的limit直接使用了全局变量LIMIT的值，offset则是动态计算的，计算方法是页码数减1再乘以limit，比如第1页的offset值就是0，第2页的offset值就是10，以此类推。构造好URL之后，直接调用scrape_api方法并返回结果即可。 这样就完成了列表页的爬取，每次请求都会得到一页10部的电影数据。 由于这时爬取到的数据已经是JSON类型了，所以不用像之前一样去解析HTML代码来提取数据，爬到的数据就是想要的结构化数据，因此解析这一步这里就可以直接省略。 爬取详情页这时候已经可以拿到每一页的电影数据了，但是实际上这些数据还缺少一些想要的信息，如剧情简介等，所以需要进一步进入到详情页来获取这些内容。 这时候点击任意一部电影，如《教父》，进入到其详情页面，这时候可以发现页面的URL已经变成了，页面也成功展示了详情页的信息 先定义一个详情页的爬取逻辑吧，代码如下： 定义了一个scrape_detail方法，它接收参数id。这里的实现也非常简单，先根据定义好的DETAIL_URL加上id，构造一个真实的详情页Ajax请求的URL，然后直接调用scrape_api方法传入这个URL即可。接着，定义一个总的调用方法，将以上的方法串联调用起来，代码如下： 定义了一个main方法，首先遍历获取页码page，然后把page当成参数传递给scrape_index方法，得到列表页的数据。接着我遍历所有列表页的结果，获取每部电影的id，然后把id当作参数传递给scrape_detail方法，来爬取每部电影的详情数据，赋值为detail_data，输出即可。 保存数据定义一个数据保存的方法，代码如下： 首先定义了数据保存的文件夹RESULTS_DIR，注意，先要判断这个文件夹是否存在，如果不存在则需要创建。 接着，定义了保存数据的方法save_data，首先获取数据的name字段，即电影的名称，把电影名称作为JSON文件的名称，接着构造JSON文件的路径，然后用json的dump方法将数据保存成文本格式。dump的方法设置了两个参数，一个是ensure_ascii，将其设置为False，它可以保证中文字符在文件中能以正常的中文文本呈现，而不是unicode字符；另一个是indent，它的数值为2，这代表生成的JSON数据结果有两个空格缩进，让它的格式显得更加美观。 最后，main方法再调用下save_data方法即可，实现如下： 本地results文件夹下出现了各个电影的JSON文件，如图所示。 "},{"title":"Ajax的原理和解析","date":"2021-05-24T08:33:38.000Z","url":"/2021/05/24/Ajax%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E8%A7%A3%E6%9E%90/","tags":[["ajax","/tags/ajax/"],["javascript","/tags/javascript/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"什么是Ajax？Ajax，全称为Asynchronous JavaScriptand XML，即异步的JavaScript和XML。它不是一门编程语言，而是利用JavaScript在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。 传统的网页，如果你想更新其内容，那么必须要刷新整个页面。有了Ajax，便可以在页面不被全部刷新的情况下更新其内容。在这个过程中，页面实际上在后台与服务器进行了数据交互，获取到数据之后，再 利用JavaScript改变网页，这样网页内容就会更新了。 到W3School上体验几个Demo来感受一下：。 基本原理初步了解了Ajax之后，来详细了解它的基本原理。发送Ajax请求到网页更新的过程可以简单分为以下3步： 发送请求 解析内容 渲染网页 发送请求JavaScript可以实现页面的各种交互功能，Ajax也不例外，它是由JavaScript实现的，实际上执行了如下代码： 这是JavaScript对Ajax最底层的实现，这个过程实际上是新建了XMLHttpRequest对象，然后调用onreadystatechange属性设置监听，最后调用open()和send()方法向某个链接（也就是服务器）发送请求。 用Python实现请求发送之后，可以得到响应结果，但这里请求的发送由JavaScript来完成。由于设置了监听，所以当服务器返回响应时，onreadystatechange对应的方法便会被触发，在这个方法里面 解析响应内容即可。 解析内容解得到响应之后，onreadystatechange属性对应的方法会被触发，此时利用xmlhttp的responseText属性便可取到响应内容。这类似于Python中利用requests向服务器发起请求，然后得到响应的过程。 返回的内容可能是HTML，也可能是JSON，接下来只需要在方法中用JavaScript进一步处理即可。比如，如果返回的内容是JSON的话，可以对它进行解析和转化。 渲染网页JavaScript有改变网页内容的能力，解析完响应内容之后，就可以调用JavaScript针对解析完的内容对网页进行下一步处理。比如，通过document.getElementById().innerHTML这样的操作，对某个元素内的源代码进行更改，这样网页显示的内容就改变了，这种对Document网页文档进行如更改、删除等操作也被称作**DOM操作**。 上例中，document.getElementById(&quot;myDiv&quot;).innerHTML=xmlhttp.responseText这个操作便将ID为myDiv的节点内部的HTML代码更改为服务器返回的内容，这样myDiv元素内部便会呈现出服务器返回的新数据，网页的部分内容看上去就更新了。 可以看到，发送请求、解析内容和渲染网页这3个步骤其实都是由JavaScript完成的。 Ajax分析用浏览器打开微博链接，随后在页面中点击鼠标右键，从弹出的快捷菜单中选择“检查”选项，此时便会弹出开发者工具，如图所示： Ajax有其特殊的请求类型，它叫作xhr。在图中我们可以发现一个以getIndex开头的请求，其Type为xhr，这就是一个Ajax请求。用鼠标点击这个请求，可以查看这个请求的详细信息。 在右侧可以观察到Request Headers、URL和Response Headers等信息。Request Headers中有一个信息为X-Requested-With:XMLHttpRequest，这就标记了此请求是Ajax请求，如图所示： 点击Preview，即可看到响应的内容，它是JSON格式的。这里Chrome为我们自动做了解析，点击箭头即可展开和收起相应内容。 可以观察到，返回结果是我的个人信息，包括昵称、简介、头像等，这也是用来渲染个人主页所使用的数据。JavaScript接收到这些数据之后，再执行相应的渲染方法，整个页面就渲染出来了。 切换到Response选项卡，从中观察到真实的返回数据，如图所示： 所以说，我们看到的微博页面的真实数据并不是最原始的页面返回的，而是在执行 JavaScript 后再次向后台发送 Ajax请求，浏览器拿到数据后进一步渲染出来的。 过滤请求利用Chrome开发者工具的筛选功能筛选出所有的Ajax请求。在请求的上方有一层筛选栏，直接点击XHR，此时在下方显示的所有请求便都是Ajax请求了，如图所示： 接下来，不断滑动页面，可以看到页面底部有一条条新的微博被刷出，而开发者工具下方也不断地出现Ajax请求，这样我们就可以捕获到所有的Ajax请求了。 随意点开一个条目，都可以清楚地看到其Request URL、Request Headers、Response Headers、Response Body等内容，此时想要模拟请求和提取就非常简单了。 "},{"title":"request、pyquest和pymongodb案例实战","date":"2021-05-23T13:12:36.000Z","url":"/2021/05/23/request%E3%80%81pyquest%E5%92%8Cpymongodb%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/","tags":[["request","/tags/request/"],["pyquest","/tags/pyquest/"],["pymongodb","/tags/pymongodb/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"准备工作在本节课开始之前，我们需要做好如下的准备工作： 安装好Python3（最低为 3.6 版本），并能成功运行Python3程序。 了解Python多进程的基本原理。 了解PythonHTTP请求库requests的基本用法。 了解正则表达式的用法和Python中正则表达式库re的基本用法。 了解PythonHTML解析库pyquery的基本用法。 了解MongoDB并安装和启动MongoDB服务。 了解Python的MongoDB操作库PyMongo的基本用法。 爬虫目标一个基本的静态网站作为案例进行爬取，需要爬取的链接为：，这个网站里面包含了一些电影信息。 要完成的目标是： 用requests爬取这个站点每一页的电影列表，顺着列表再爬取每个电影的详情页。 用pyquery和正则表达式提取每部电影的名称、封面、类别、上映时间、评分、剧情简介等内容。 把以上爬取的内容存入MongoDB数据库。 使用多进程实现爬取的加速。 爬取列表页爬取的第一步肯定要从列表页入手，首先观察一下列表页的结构和翻页规则。在浏览器中访问，然后打开浏览器开发者工具，观察每一个电影信息区块对应的HTML，以及进入到详情页的URL是怎样的，如图所示： 每部电影对应的区块都是一个div节点，它的class属性都有el-card这个值。每个列表页有10个这样的div节点，也就对应着10部电影的信息。 再分析下从列表页是怎么进入到详情页的，选中电影的名称，看下结果： 这个名称实际上是一个h2节点，其内部的文字就是电影的标题。h2节点的外面包含了一个a节点，这个a节点带有href属性，这就是一个超链接，其中href的值为/detail/1，这是一个相对网站的根URL路径，加上网站的根URL就构成了，也就是这部电影详情页的URL。这样只需要提取这个href属性就能构造出详情页的URL并接着爬取了。 接下来分析下翻页的逻辑，拉到页面的最下方，可以看到分页页码，如图所示： 页面显示一共有100条数据，10页的内容，因此页码最多是10。接着我们点击第2页，如图所示： 可以看到网页的URL变成了，相比根URL多了/page/2这部分内容。网页的结构还是和原来一模一样，所以我们可以和第1页一样处理。 接着查看第3页、第4页等内容，可以发现有这么一个规律，每一页的URL最后分别变成了/page/3、/page/4。所以，/page后面跟的就是列表页的页码，当然第1页也是一样，在根URL后面加上/page/1也是能访问的，只不过网站做了一下处理，默认的页码是1，所以显示第1页的内容。 分析到这里，逻辑基本就清晰了。 如果要完成列表页的爬取，可以这么实现： 遍历页码构造10页的索引页URL。 从每个索引页分析提取出每个电影的详情页URL。 先定义一些基础的变量，并引入一些必要的库，写法如下： 引入requests用来爬取页面，logging用来输出信息，re用来实现正则表达式解析，pyquery用来直接解析网页，pymongo用来实现MongoDB存储，urljoin用来做URL的拼接。 接着定义日志输出级别和输出格式，完成之后再定义BASE_URL为当前站点的根URL，TOTAL_PAGE为需要爬取的总页码数量。 定义好了之后，来实现一个页面爬取的方法，实现如下： 考虑到不仅要爬取列表页，还要爬取详情页，在这里定义一个较通用的爬取页面的方法，叫作scrape_page，它接收一个url参数，返回页面的html代码。 首先判断状态码是不是200，如果是，则直接返回页面的HTML代码，如果不是，则会输出错误日志信息。另外，这里实现了requests的异常处理，如果出现了爬取异常，则会输出对应的错误日志信息。这时将logging的error方法的exc_info参数设置为True则可以打印出Traceback错误堆栈信息。 有了scrape_page方法之后，给这个方法传入一个url，正常情况下它就可以返回页面的HTML代码。 在这个基础上，来定义列表页的爬取方法吧，实现如下： 方法名称叫作scrape_index，这个方法会接收一个page参数，即列表页的页码，在方法里面实现列表页的URL拼接，然后调用scrape_page方法爬取即可得到列表页的HTML代码了。 获取了HTML代码后，下一步就是解析列表页，并得到每部电影的详情页的URL了，实现如下： 这里我们定义了parse_index方法，它接收一个html参数，即列表页的HTML代码。接着用pyquery新建一个PyQuery对象，完成之后再用.el-card .name选择器选出来每个电影名称对应的超链接节点。遍历这些节点，通过调用attr方法并传入href获得详情页的URL路径，得到的href就是上文所说的类似/detail/1这样的结果。这并不是一个完整的URL，所以需要借助urljoin方法把BASE_URL和href拼接起来，获得详情页的完整URL，得到的结果就是类似这样完整的URL了，最后yield返回即可。 通过调用parse_index方法传入列表页的HTML代码就可以获得该列表页所有电影的详情页URL了，接下来把上面的方法串联调用一下，实现如下： 定义了main方法来完成上面所有方法的调用，首先使用range方法遍历一下页码，得到的page是1~10，接着把page变量传给scrape_index方法，得到列表页的HTML，赋值为index_html变量。接下来再将index_html变量传给parse_index方法，得到列表页所有电影的详情页URL，赋值为detail_urls，结果是一个生成器，调用list方法就可以将其输出出来。 由于输出内容比较多，这里只贴了一部分。可以看到，在这个过程中程序首先爬取了第1页列表页，然后得到了对应详情页的每个URL，接着再接着爬第2页、第3页，一直到第10页，依次输出了每一页的详情页URL。这样，就成功获取到所有电影详情页URL。 爬取详情页首先观察一下详情页的HTML代码，如图所示： 经过分析，要提取的内容和对应的节点信息如下： 封面：是一个img节点，其class属性为cover。 名称：是一个h2节点，其内容便是名称。 类别：是span节点，其内容便是类别 内容，其外侧是button节点，再外侧则是class为categories的div节点。 上映时间：是span节点，其内容包含了上映时间，其外侧是包含了class为info的div节点。但注意这个div前面还有一个class为info的div节点，可以使用其内容来区分，也可以使用nth-child或nth- of-type这样的选择器来区分。另外提取结果中还多了「上映」二字，可以用正则表达式把日期提取出来。 评分：是一个p节点，其内容便是评分，p节点的class属性为score。 剧情简介：是一个p节点，其内容便是 剧情简介，其外侧是class为drama的div节点。 刚才已经成功获取了详情页的URL，接下来要定义一个详情页的爬取方法，实现如下： 定义了一个scrape_detail方法，它接收一个url参数，并通过调用scrape_page方法获得网页源代码。由于刚才已经实现了scrape_page方法，所以在这里不用再写一遍页面爬取的逻辑，直接调用即可，这就做到了代码复用。 单独定义一个scrape_detail方法在逻辑上会显得更清晰，而且以后如果想要对scrape_detail方法进行改动，比如添加日志输出或是增加预处理，都可以在 scrape_detail里面实现，而不用改动scrape_page方法，灵活性会更好。 详情页的爬取方法已经实现了，接着就是详情页的解析了，实现如下： 定义了parse_detail方法用于解析详情页，它接收一个html参数，解析其中的内容，并以字典的形式返回结果。每个字段的解析情况如下所述： cover：封面，直接选取class为cover的img节点，并调用attr方法获取src属性的内容即可。 name：名称，直接选取a节点的直接子节点h2节点，并调用text方法提取其文本内容即可得到名称。 categories：类别，由于类别是多个，所以这里首先用.categories button span选取了class为categories的节点内部的span节点，其结果是多个，所以这里进行了遍历，取出了每个span节点的文本内容，得到的便是列表形式的类别。 published_at：上映时间，由于pyquery支持使用:contains直接指定包含的文本内容并进行提取，且每个上映时间信息都包含了「上映」二字，所以这里就直接使用:contains(上映)提取了class为info的div节点。提取之后，得到的结果类似「1993-07-26 上映」这样，并不想要「上映」这两个字，所以又调用了正则表达式把日期单独提取出来了。当然这里也可以直接使用strip或replace方法把多余的文字去掉。 drama：直接提取class为drama的节点内部的p节点的文本即可。 score：直接提取class为score的p节点的文本即可，由于提取结果是字符串，所以我们需要把它转成浮点数，即``float`类型。 上述字段提取完毕之后，构造一个字典返回。这样，成功完成了详情页的提取和分析了。 将main方法稍微改写一下，增加这两个方法的调用，改写如下： 首先遍历了detail_urls，获取了每个详情页的URL，然后依次调用了scrape_detail和parse_detail方法，最后得到了每个详情页的提取结果，赋值为data并输出。 运行结果如下： 可以看到，已经成功提取出每部电影的基本信息，包括封面、名称、类别，等等。 保存到MongoDB请确保现在有一个可以正常连接和使用的MongoDB数据库。 将数据导入MongoDB需要用到PyMongo这个库，这个在最开始已经引入过了。那么接下来我们定义一下 MongoDB的连接配置，实现如下： 在这里声明了几个变量，介绍如下： MONGO_CONNECTION_STRING：MongoDB的连接字符串，里面定义了MongoDB的基本连接信息，如host、port，还可以定义用户名密码等内容。 MONGO_DB_NAME：MongoDB数据库的名称。 MONGO_COLLECTION_NAME：MongoDB的集合名称。 这里用MongoClient声明了一个连接对象，然后依次声明了存储的数据库和集合。接下来，再实现一个将数据保存到MongoDB的方法，实现如下： 声明了一个save_data方法，它接收一个data参数，也就是我们刚才提取的电影详情信息。在方法里面，调用了update_one方法，第1个参数是查询条件，即根据name进行查询；第2个参数是data对象本身，也就是所有的数据，这里用$set操作符表示更新操作；第3个参数很关键，这里实际上是upsert参数，如果把这个设置为 True，则可以做到存在即更新，不存在即插入的功能，更新会根据第一个参数设置的name字段，所以这样可以防止数据库中出现同名的电影数据。 注：实际上电影可能有同名，但该场景下的爬取数据没有同名情况，当然这里更重要的是实现MongoDB的去重操作。 接下来将main方法稍微改写一下就好了，改写如下： 重新运行，输出结果： 运行完毕之后我们可以使用MongoDB客户端工具可视化查看已经爬取到的数据，结果如下： 多进程加速由于整个的爬取是单进程的，而且只能逐条爬取，速度稍微有点慢，有没有方法来对整个爬取过程进行加速呢？ 在前面学习了多进程的基本原理和使用方法，下面就来实践一下多进程的爬取。 由于一共有10页详情页，并且这10页内容是互不干扰的，所以可以一页开一个进程来爬取。由于这10个列表页页码正好可以提前构造成一个列表，所以可以选用多进程里面的进程池Pool来实现这个过程。 这里需要改写下main方法的调用，实现如下： 这里首先给main方法添加一个参数page，用以表示列表页的页码。接着声明了一个进程池，并声明pages为所有需要遍历的页码，即1~10。最后调用map方法，第1个参数就是需要被调用的方法，第2个参数就是pages，即需要遍历的页码。 这样pages就会被依次遍历。把1~10这10个页码分别传递给main方法，并把每次的调用变成一个进程，加入到进程池中执行，进程池会根据当前运行环境来决定运行多少进程。 运行输出结果和之前类似，但是可以明显看到加了多进程执行之后，爬取速度快了非常多。可以清空一下之前的MongoDB数据，可以发现数据依然可以被正常保存到MongoDB数据库中。"},{"title":"MongoDB数据库的使用","date":"2021-05-23T06:46:16.000Z","url":"/2021/05/23/MongoDB%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":[["python","/tags/python/"],["mongodb","/tags/mongodb/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"准备工作开始之前，请确保你已经安装好了MongoDB并启动了其服务，同时安装好了Python的PyMongo库。 安装好之后，我们需要把MongoDB服务启动起来。启动完成之后，它会默认在本地localhost的27017端口上运行。 接下来我们需要安装PyMongo这个库，它是Python用来操作MongoDB的第三方库，直接用pip3安装即可： 连接MongoDB连接MongoDB时，需要使用PyMongo库里面的MongoClient。一般来说，向其传入MongoDB的IP及端口即可，其中第一个参数为地址host，第二个参数为端口 port（如果不给它传递参数，则默认是27017）： 另外，MongoClient的第一个参数host还可以直接传入MongoDB的连接字符串，它以mongodb开头，例如： 这样也可以达到同样的连接效果。 指定数据库MongoDB中可以建立多个数据库，接下来指定操作其中一个数据库。这里以test数据库作为下一步需要在程序中指定使用的例子： 这里调用client的test属性即可返回test数据库。当然，也可以这样指定： 这两种方式是等价的。 指定集合MongoDB的每个数据库又包含许多集合（collection），它们类似于关系型数据库中的表。 下一步需要指定要操作的集合，这里指定一个名称为students的集合。与指定数据库类似，指定集合也有两种方式： 或是 这样便声明了一个Collection对象。 插入数据接下来，便可以插入数据了。对students这个集合新建一条学生数据，这条数据以字典形式表示： 新建的这条数据里指定了学生的学号、姓名、年龄和性别。直接调用collection的insert方法即可插入数据，代码如下： 在MongoDB中，每条数据其实都有一个_id属性来唯一标识。如果没有显式指明该属性，MongoDB会自动产生一个ObjectId类型的_id属性。insert()方法会在执行后返回_id值。 运行结果如下： 当然，我们也可以同时插入多条数据，只需要以列表形式传递即可，示例如下： 返回结果是对应的_id的集合： 在PyMongo中，官方已经不推荐使用insert方法了。但是如果你要继续使用也没有什么问题。目前，官方推荐使用insert_one和insert_many方法来分别插入单条记录和多条记录，示例如下： 运行结果如下： 与insert方法不同，返回的是InsertOneResult对象，可以调用其inserted_id属性获取_id。 对于insert_many方法，可以将数据以列表形式传递，示例如下： 运行结果如下： 该方法返回的类型是InsertManyResult，调用inserted_ids属性可以获取插入数据的_id列表。 查询数据插入数据后，可以利用find_one或find方法进行查询，其中find_one查询得到的是单个结果，find则返回一个生成器对象。示例如下： 这里我们查询name为Holy的数据，它的返回结果是字典类型，运行结果如下： 查询结果： 可以发现，它多了_id属性，这就是MongoDB在插入过程中自动添加的。此外，也可以根据ObjectId来查询，此时需要调用bson库里面的objectid： 查询结果： 对于多条数据的查询，可以使用find方法。例如，这里查找性别为male的数据，示例如下： 运行结果如下： 返回结果是Cursor类型，它相当于一个生成器，需要遍历获取的所有结果，其中每个结果都是字典类型。 如果要查询年龄大于20的数据，则写法如下： 查询的条件键值已经不是单纯的数字了，而是一个字典，其键名为比较符号$gt，意思是大于，键值为20。 比较符号归纳如下： 符号 含义 示例 $lt 小于 {‘age’:{‘$lt’:20}} $gt 大于 {‘age’:{‘$gt’:20}} $lte 小于或等于 {‘age’:{‘$lte’:20}} $gte 大于或等于 {‘age’:{‘$gte’:20}} $ne 不等于 {‘age’:{‘$ne’:20}} $in 在范围内 {‘age’:{‘$in’:[20,23]}} $nin 不在范围内 {‘age’:{‘$nin’:[20, 23]}} table { margin: auto; font-size: 80%; } 另外，还可以进行正则匹配查询。例如，查询名字以H开头的学生数据，示例如下： 这里使用$regex来指定正则匹配，^M.*代表以M开头的正则表达式。 一些功能符号归类为下表： 符号 含义 示例 $regrx 匹配正则表达式 {‘name’:{‘$regex’:’^M.*’}} $exists 属性是否存在 {‘name’:{‘$exists’:True}} $type 类型判断 {‘age’:{‘$type’:’int’}} $mod 数字模操作 {‘age’:{‘$mod’:[5,0]}} $text 文本查询 {‘$text’:{‘$search’:’Holy’}} $where 高级条件查询 {‘$where’:’obj.fans_count==obj.follows_count’} 计数要统计查询结果有多少条数据，可以调用count方法。以统计所有数据条数为例： 我们还可以统计符合某个条件的数据： 运行结果是一个数值，即符合条件的数据条数。 排序排序时，可以直接调用sort方法，并在其中传入排序的字段及升降序标志。示例如下： 运行结果如下： 调用pymongo.ASCENDING指定升序。如果要降序排列，可以传入pymongo.DESCENDING。 偏移只需要取某几个元素，可以利用skip方法偏移几个位置，比如偏移2，就代表忽略前两个元素，得到第3个及以后的元素： 运行结果如下： 还可以用limit方法指定要取的结果个数，示例如下： 运行结果如下： 值得注意的是，在数据量非常庞大的时候，比如在查询千万、亿级别的数据库时，最好不要使用大的偏移量，因为这样很可能导致内存溢出。 更新数据更新，可以使用update方法，指定更新的条件和更新后的数据即可。例如： 更新name为Holy的数据的年龄：首先指定查询条件，然后将数据查询出来，修改年龄后调用update方法将原条件和修改后的数据传入。 运行结果如下： 返回结果是字典形式，ok代表执行成功，nModified代表影响的数据条数。 也可以使用$set操作符对数据进行更新，代码如下： 这样可以只更新student字典内存在的字段。如果原先还有其他字段，则不会更新，也不会删除。而如果不用$set的话，则会把之前的数据全部用student字典替换；如果原本存在其他字段，则会被删除。 update方法其实也是官方不推荐使用的方法。这里也分为update_one方法和update_many方法，用法更加严格，它们的第2个参数需要使用$类型操作符作为字典的键名，示例如下: 上面的例子中调用了update_one方法，使得第2个参数不能再直接传入修改后的字典，而是需要使用&#123;&#39;$set&#39;:student&#125;这样的形式，其返回结果是UpdateResult类型。然后分别调用matched_count和modified_count属性，可以获得匹配的数据条数和影响的数据条数。 运行结果如下： 删除删除操作比较简单，直接调用remove方法指定删除的条件即可，此时符合条件的所有数据均会被删除。 示例如下： 运行结果如下： 另外，这里依然存在两个新的推荐方法 ——delete_one和delete_many，示例如下： 运行结果如下： delete_one删除第一条符合条件的数据，delete_many即删除所有符合条件的数据。它们的返回结果都是DeleteResult类型，可以调用deleted_count属性获取删除的数据条数。"},{"title":"Pyquery的使用","date":"2021-05-22T16:49:30.000Z","url":"/2021/05/23/Pyquery%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":[["python","/tags/python/"],["pyquery","/tags/pyquery/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"准备工作pyquery是Python的第三方库，安装命令如下： 初始化解析HTML文本的时候，首先需要将其初始化为一个pyquery对象。它的初始化方式有多种，比如直接传入字符串、传入URL、传入文件名，等等。 字符串初始化可以直接把HTML的内容当作参数来初始化pyquery对象。 运行结果如下： 引入pyquery这个对象，取别名为pq，然后声明了一个长HTML字符串，并将其当作参数传递给pyquery类，这样就成功完成了初始化。 接下来，将初始化的对象传入CSS选择器。在这个实例中，传入li节点，这样就可以选择所有的li节点。 URL初始化初始化的参数不仅可以以字符串的形式传递，还可以传入网页的URL，只需要指定参数为url即可： pyquery对象会首先请求这个URL，然后用得到的HTML内容完成初始化。这就相当于将网页的源代码以字符串的形式传递给pyquery类来初始化。 它与下面的功能是相同的： 文件初始化除了传递一个URL，还可以传递本地的文件名，参数指定为filename即可： 这里需要有一个本地HTML文件demo.html，其内容是待解析的HTML字符串。这样它会先读取本地的文件内容，然后将文件内容以字符串的形式传递给 pyquery类来初始化。 以上3种方式均可初始化，当然最常用的初始化方式还是以字符串形式传递。 基本CSS选择器 运行结果： 初始化pyquery对象之后，传入CSS选择器#container .list li，它的意思是先选取id为container的节点，然后再选取其内部class为list的所有li节点，最后打印输出。成功获取到了符合条件的节点。将它的类型打印输出后发现，它的类型依然是pyquery类型。 直接遍历这些节点，然后调用text方法，就可以获取节点的文本内容，代码示例如下： 结果如下： 查找节点子节点查找子节点需要用到find方法，传入的参数是CSS选择器： 运行结果： 通过.list参数选取class为list的节点，然后调用find方法，传入CSS选择器，选取其内部的li节点，最后打印输出。可以发现，find方法会将符合条件的所有节点选择出来，结果的类型是pyquery类型。 find的查找范围是节点的所有子孙节点，而如果我们只想查找子节点，那可以用children方法： 运行结果： 如果要筛选所有子节点中符合条件的节点，比如想筛选出子节点中class为active的节点，可以向children方法传入CSS选择器.active，代码如下： 结果如下： 输出的结果做了筛选，留下了class为active的节点。 父节点可以用parent方法获取某个节点的父节点： 运行结果如下： 用.list选取class为list的节点，然后调用parent方法得到其父节点，其类型依然是pyquery类型。这里的父节点是该节点的直接父节点，也就是说，它不会再去查找父节点的父节点，即祖先节点。 如果你想获取某个祖先节点，该怎么办呢？可以用parents方法： 运行结果如下： 输出结果有两个：一个是class为wrap的节点，一个是id为container的节点。也就是说，使用parents方法会返回所有的祖先节点。 要筛选某个祖先节点的话，可以向parents方法传入CSS选择器，这样就会返回祖先节点中符合CSS选择器的节点： 结果如下： 输出结果少了一个节点，只保留了class为wrap的节点。 兄弟节点要获取兄弟节点，可以使用siblings方法。这里还是以上面的HTML代码为例： 结果如下： 先选择class为list的节点，内部class为item-0和active的节点，也就是第3个li节点。很明显，它的兄弟节点有4个，那就是第1、2、4、5 个li节点。结果显示的正是4个兄弟节点。 筛选某个兄弟节点，可以用siblings方法传入CSS选择器，这样就会从所有兄弟节点中挑选出符合条件的节点了： 筛选class为active的节点，从刚才的结果中可以观察到，class为active兄弟节点的是第4个li节点，所以结果应该是1个。运行结果： 遍历pyquery的选择结果既可能是多个节点，也可能是单个节点，类型都是pyquery类型，并没有返回列表。对于单个节点来说，可以直接打印输出，也可以直接转成字符串： 运行结果如下： 对于有多个节点的结果，我们就需要用遍历来获取了。例如，如果要把每一个li节点进行遍历，需要调用items方法： 运行结果如下： 调用items方法后，会得到一个生成器，遍历一下，就可以逐个得到li节点对象了，它的类型也是pyquery类型。每个li节点还可以调用前面所说的方法进行选择，比如继续查询子节点，寻找某个祖先节点等。 获取信息提取到节点之后，最终目的是提取节点包含的信息。比较重要的信息有两类，一是获取属性，二是获取文本。 获取属性提取到某个pyquery类型的节点后，就可以调用attr方法来获取属性： 运行结果如下： 首先选中class为item-0和active的li节点内的a节点，它的类型是pyquery类型。然后调用attr方法。在这个方法中传入属性的名称，就可以得到属性值了。此外，也可以通过调用attr属性来获取属性值，用法如下： 结果：link3.html，这两种方法的结果完全一样。 获取文本获取节点之后的另一个主要操作就是获取其内部文本了，此时可以调用text方法来实现： 运行结果如下： 首先选中一个a节点，然后调用text方法，就可以获取其内部的文本信息了。text会忽略节点内部包含的所有HTML，只返回纯文字内容。 如果想要获取这个节点内部的HTML文本，就要用html方法： 如果我们选中的结果是多个节点，text或html方法会返回什么内容？我们用实例来看一下： 运行结果如下： html方法返回的是第1个li节点的内部HTML文本，而text则返回了所有的li节点内部的纯文本，中间用一个空格分割开，即返回结果是一个字符串。 节点操作pyquery提供了一系列方法来对节点进行动态修改，比如为某个节点添加一个class，移除某个节点等，这些操作有时会为提取信息带来极大的便利。由于节点操作的方法太多，下面举几个典型的例子来说明它的用法。 addClass和removeClass 首先选中第3个li节点，然后调用removeClass方法，将li节点的active这class移除，第2步调用addClass方法，将class添加回来。每执行一次操作，就打印输出当前li节点的内容。 运行结果如下： 一共输出了3次。第2次输出时，li节点的active这个class被移除了，第3次class又添加回来了。addClass和removeClass方法可以动态改变节点的class属性。 attr、text、html除了操作class这个属性外，也可以用attr方法对属性进行操作。此外，还可以用text和html方法来改变节点内部的内容。示例如下： 首先选中li节点，然后调用attr方法来修改属性。该方法的第1个参数为属性名，第2个参数为属性值。最后调用text和html方法来改变节点内部的内容。3次操作后，分别打印输出当前的li节点。 运行结果如下： 调用attr方法后，li节点多了一个原本不存在的属性name，其值为link。接着调用text方法传入文本，li节点内部的文本全被改为传入的字符串文本。最后，调用html方法传入HTML文本，li节点内部又变为传入的HTML文本了。 使用attr方法时如果只传入第1个参数的属性名，则是获取这个属性值；如果传入第2个参数，可以用来修改属性值。使用text和html方法时如果不传参数，则是获取节点内纯文本和HTML文本，如果传入参数，则进行赋值。 removeremove方法就是移除，它有时会为信息的提取带来非常大的便利。下面有一段 HTML文本： 想提取“Hello, World”这个字符串，该怎样操作呢？ 这里先直接尝试提取class为wrap的节点的内容，看看是不是我们想要的。 运行结果如下： 这个结果还包含了内部的p节点的内容，也就是说text把所有的纯文本全提取出来了。 如果想去掉p节点内部的文本，可以选择再把p节点内的文本提取一遍，然后从整个结果中移除这个子串，但这个做法明显比较烦琐。 这时remove方法就可以派上用场了，可以接着这么做： 首先选中p节点，然后调用remove方法将其移除，这时wrap内部就只剩下“Hello, World”这句话了，最后利用text方法提取即可。 其实还有很多其他节点操作的方法，比如append、empty和prepend等方法，详细的用法可以参考官方文档：。 伪类选择器CSS选择器之所以强大，还有一个很重要的原因，那就是它支持多种多样的伪类选择器，例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等。示例如下： 在这个例子中使用了CSS3的伪类选择器，依次选择了第1个li节点、最后一个li节点、第2个li节点、第3个li之后的li节点、偶数位置的li节点、包含second文本的li节点。"},{"title":"正则表达式","date":"2021-05-21T13:08:59.000Z","url":"/2021/05/21/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","tags":[["python","/tags/python/"],["re","/tags/re/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"正则表达式正则表达式是处理字符串的强大工具，它有自己特定的语法结构。能实现字符串的检索、替换、匹配验证，对于爬虫来说，从HTML里提取想要的信息就非常方便。 实例引入下面用几个实例来看一下正则表达式的用法。 打开开源中国提供的正则表达式测试工具，输入待匹配的文本，然后选择常用的正则表达式，就可以得出相应的匹配结果了。 例如，输入下面这段待匹配的文本： 这段字符串中包含了一个电话号码和一个电子邮件，接下来就尝试用正则表达式提取出来，如图所示。 在网页右侧选择“匹配 Email地址”，就可以看到下方出现了文本中的E-mail。如果选择“匹配网址URL”，就可以看到下方出现了文本中的URL。 这里使用了正则表达式的匹配功能，也就是用一定规则将特定的文本提取出来。 比方说，电子邮件是有其特定的组成格式的：一段字符串 + @ 符号 + 某个域名。而URL的组成格式则是协议类型``+``冒号``+``双斜线``+``域名和路径。 可以用下面的正则表达式匹配URL： 用这个正则表达式去匹配一个字符串，如果这个字符串中包含类似URL的文本，那就会被提取出来。 下表中列出了常用的匹配规则： 模式 描述 \\w 匹配字母、数字及下划线 \\W 匹配不是字母、数字及下划线的字符 \\s 匹配任意空白字符，等价于 [\\t\\n\\r\\f] \\S 匹配任意非空字符 \\d 匹配任意数字，等价于 [0~9] \\D 匹配任意非数字的字符 \\A 匹配字符串开头 \\Z 匹配字符串结尾，如果存在换行，只匹配到换行前的结束字符串 \\z 匹配字符串结尾，如果存在换行，同时还会匹配换行符 \\G 匹配最后匹配完成的位置 \\n 匹配一个换行符 \\t 匹配一个制表符 ^ 匹配一行字符串的开头 $ 匹配一行字符串的结尾 . 匹配任意字符，除了换行符，当 re.DOTALL 标记被指定时，则可以匹配包括换行符的任意字符 […] 用来表示一组字符，单独列出，比如 [amk] 匹配 a、m 或 k [^…] 不在 [] 中的字符，比如 匹配除了 a、b、c 之外的字符 * 匹配 0 个或多个表达式 + 匹配 1 个或多个表达式 ? 匹配 0 个或 1 个前面的正则表达式定义的片段，非贪婪方式 {n} 精确匹配 n 个前面的表达式 {n, m} 匹配 n 到 m 次由前面正则表达式定义的片段，贪婪方式 a|b 匹配 a 或 b () 匹配括号内的表达式，也表示一个组 table { margin: auto; font-size: 80%; } Python的re库提供了整个正则表达式的实现，利用这个库，可以在Python中使用正则表达式。 match首先介绍一个常用的匹配方法——match，向它传入要匹配的字符串，以及正则表达式，就可以检测这个正则表达式是否匹配字符串。match方法会尝试从字符串的起始位置匹配正则表达式，如果匹配，就返回匹配成功的结果；如果不匹配，就返回None。 运行结果如下： 这里首先声明了一个字符串，其中包含英文字母、空白字符、数字等。接下来，我们写一个正则表达式： 用它来匹配这个长字符串。开头的^·匹配字符串的开头，也就是以Hello开头；\\s匹配空白字符，用来匹配目标字符串的空格；\\d匹配数字，3个\\d匹配123；再写1个\\s匹配空格；后面的4567，依然能用4个\\d来匹配，但是这么写比较烦琐，所以后面可以跟&#123;4&#125;代表匹配前面的规则4次，也就是匹配4个数字；后面再紧接1个空白字符，最后\\w&#123;10&#125;匹配10个字母及下划线。 在match方法中，第一个参数传入正则表达式，第二个参数传入要匹配的字符串。打印输出结果，可以看到结果是SRE_Match对象，这证明成功匹配。该对象有两个方法：group方法可以输出匹配的内容，结果是Hello 123 4567 World_This，这恰好是正则表达式规则所匹配的内容；span方法可以输出匹配的范围，结果是(0, 25)，这就是匹配到的结果字符串在原字符串中的位置范围。 在match方法中，第一个参数传入正则表达式，第二个参数传入要匹配的字符串。打印输出结果，可以看到结果是SRE_Match对象，这证明成功匹配。该对象有两个方法：group方法可以输出匹配的内容，结果是Hello 123 4567 World_This，这恰好是正则表达式规则所匹配的内容；span方法可以输出匹配的范围，结果是(0, 25)，这就是匹配到的结果字符串在原字符串中的位置范围。 匹配目标用match方法得到了匹配到的字符串内容，想从字符串中提取一部分内容，该怎么办呢？ 要从一段文本中提取出邮件或电话号码等内容。可以使用()括号将想提取的子字符串括起来。()实际上标记了一个子表达式的开始和结束位置，被标记的每个子表达式会依次对应每一个分组，调用group方法传入分组的索引即可获取提取的结果。 示例如下： 这里把字符串中的1234567提取出来，此时可以将数字部分的正则表达式用()括起来，然后调用了group(1)获取匹配结果。 运行结果如下： 可以看到，成功得到了1234567。这里用的是group(1)，它与group()有所不同，后者会输出完整的匹配结果，而前者会输出第一个被()包围的匹配结果。假如正则表达式后面还有()包括的内容，那么可以依次用group(2)、group(3)等来获取。 通用匹配刚才写的正则表达比较复杂，出现空白字符写\\s匹配，出现数字用\\d匹配，这样的工作量非常大。 可以用一个万能匹配来减少这些工作，那就是.*。其中.可以匹配任意字符（除换行符），*代表匹配前面的字符无限次，组合在一起可以匹配任意字符。 改写一下正则表达式： 运行结果： 贪婪与非贪婪使用通用匹配.*时，有时候匹配到的并不是我们想要的结果。 想获取中间的数字，所以中间依然写的是(\\d+)。由于数字两侧的内容比较杂乱，所以略写成.*。最后，组成^He.*(\\d+).*Demo$，看样子并没有什么问题。 运行结果： 奇怪的事情发生了，只得到了7这个数字，这是怎么回事呢？ 这里就涉及一个贪婪匹配与非贪婪匹配的问题了。在贪婪匹配下，.*会匹配尽可能多的字符。正则表达式中.*后面是\\d+，也就是至少一个数字，并没有指定具体多少个数字，因此，.*就尽可能匹配多的字符，这里就把123456匹配了，给\\d+留下一个可满足条件的数字7，最后得到的内容就只有数字7了。 这里需要使用非贪婪匹配，非贪婪匹配的写法是.*?。 将第一个.*改成了.*?，转变为非贪婪匹配。运行结果如下： 成功获取1234567，贪婪匹配是尽可能匹配多的字符，非贪婪匹配就是尽可能匹配少的字符。当.*?匹配到Hello后面的空白字符时，再往后的字符就是数字了，而\\d+恰好可以匹配，.*?就不再进行匹配，交给\\d+去匹配后面的数字。这样.*?匹配了尽可能少的字符，\\d+的结果就是1234567。 在做匹配的时候，字符串中间尽量使用非贪婪匹配，也就是用.*?来代替.*，以免出现匹配结果缺失的情况。 需要注意的是，如果匹配的结果在字符串结尾，.*?就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符。例如： 运行结果如下： 修饰符正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。 用实例来看一下： 字符串中加了换行符，正则表达式还是一样的，用来匹配其中的数字。看一下运行结果： 运行直接报错，也就是说正则表达式没有匹配到这个字符串，返回结果为None，又调用了group方法导致AttributeError。为什么加了一个换行符，就匹配不到了呢？这是因为我们匹配的是除换行符之外的任意字符，当遇到换行符时，.*?就不能匹配了，导致匹配失败。 这里只需加一个修饰符re.S，即可修正这个错误： 运行结果如下： 这个re.S在网页匹配中经常用到。因为HTML节点经常会有换行，加上它，就可以匹配节点与节点之间的换行了。 还有一些修饰符，在必要的情况下也可以使用，如表所示： 修饰符 描述 re.I 使匹配对大小写不敏感 re.L 做本地化识别（locale-aware）匹配 re.M 多行匹配，影响 ^ 和 $ re.S 使匹配包括换行在内的所有字符 re.U 根据 Unicode 字符集解析字符。这个标志影响 \\w、\\W、\\b 和 \\B re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解 "},{"title":"docker的安装","date":"2021-05-20T08:45:41.000Z","url":"/2021/05/20/docker%E7%9A%84%E5%AE%89%E8%A3%85/","tags":[["linux","/tags/linux/"],["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":"安装环境安装环境是Centos环境，要求版本是 CentOS 7 or 8以上。 卸载旧版本的docker 使用yum仓库安装安装方式有很多种，可以参考官方文档。 设置仓库源下载yum-utils工具包和稳固的安装源。 安装docker引擎 1.安装最新版本 2.安装特定版本 列出可安装的版本。 选择安装版本。 启动docker运行下列命令启动docker。 测试docker安装正确。 卸载docker卸载docker和下载的容器及包： 镜像等配置文件不会自动移除，需要手动删除。 "},{"title":"基于NextCloud搭建个人网盘","date":"2021-05-20T08:16:41.000Z","url":"/2021/05/20/%E5%9F%BA%E4%BA%8Ecentos%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%9B%98/","tags":[["docker","/tags/docker/"],["nextCloud","/tags/nextCloud/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"NextcloudNextcloud是一款开源免费的私有云存储网盘项目，可以让你快速便捷地搭建一套属于自己或团队的云同步网盘，从而实现跨平台跨设备文件同步、共享、版本控制、团队协作等功能。它的客户端覆盖了Windows、Mac、Android、iOS、Linux等各种平台，也提供了网页端以及WebDAV接口，所以你几乎可以在各种设备上方便地访问你的云盘。 安装 Nextcloud安装Mysql 运行mysql并且设置访问端口：3306，容器名称：mysql ,管理员密码：******** 进入容器bash。 登陆mysql 接着输入管理员密码：admin,回车。创建一个数据库 创建一个用户 创建一个用户名称为：nextcloud；‘%’：代表不限ip登陆，远程登陆; 密码为：admin。 授权。 给这个用户nextcloud授予 这个数据库nextcloud.*所有的权限，远程登陆，密码为admin； 基于Docker部署Nextcloud服务端选择以Docker的方式来部署nextcloud是因为Docker可以跨平台上运行，可以确保执行环境的一致性，有利于应用的迁移和管理。 服务端部署的基本流程是：安装Docker并启动 –&gt; 运行Nextcloud容器 –&gt; 访问Web端初始化。 安装的话不做过多说明，参考官方文档或本博客其他博文。 下载Nextcloud镜像 运行Nextcloud 参数说明： -d #容器后台运行 –name nextcloud #容器名 -v /data/nextcloud:/var/www/html #将宿主机的目录/data/nextcloud挂载到容器的/var/www/html -p 8000:80 #将宿主机的端口（此处以8000为例）映射到容器的80端口” 查看运行的容器。 停止运行的容器。 "},{"title":"requests库的基本使用","date":"2021-05-18T07:44:10.000Z","url":"/2021/05/18/requests%E5%BA%93%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":[["python","/tags/python/"],["requests","/tags/requests/"],["爬虫","/tags/%E7%88%AC%E8%99%AB/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"安装requests是一个第三方库，使用pip下载安装。 实例引入用Python写爬虫的第一步就是模拟发起一个请求，把网页的源代码获取下来。 在浏览器中输入一个URL并回车，实际上就是让浏览器帮我们发起一个GET类型的HTTP请求，浏览器得到源代码后，把它渲染出来就可以看到网页内容了。 那如果想用requests来获取源代码，应该怎么办呢？很简单，requests这个库提供了一个get方法，调用这个方法，并传入对应的URL就能得到网页的源代码。 比如这里有一个示例网站:，其内容如下： 这个网站展示了一些电影数据，如果想要把这个网页里面的数据爬下来，比如获取各个电影的名称、上映时间等信息，然后把它存下来的话，该怎么做呢？ 第一步当然就是获取它的网页源代码了。 可以用requests这个库轻松地完成这个过程，代码的写法是这样的： 输出结果如下： 由于网页内容比较多，这里省略了大部分内容。 不过看运行结果，我们已经成功获取网页的HTML源代码，里面包含了电影的标题、类型、上映时间，等等。 把网页源代码获取下来之后，下一步我们把想要的数据提取出来，数据的爬取就完成了。 请求HTTP中最常见的请求之一就是GET请求。 GET请求换一个示例网站，其URL为，如果客户端发起的是GET请求的话，该网站会判断并返回相应的请求信息，包括 Headers、IP等。 我们还是用相同的方法来发起一个GET请求，代码如下： 返回结果： 可以发现，成功发起了GET请求，也通过这个网站的返回结果得到了请求所携带的信息，包括Headers、URL、IP，等等。 对于GET请求，我们知道URL后面是可以跟上一些参数的，如果我们现在想添加两个参数，其中name是germey，age是25，URL就可以写成如下内容： 要构造这个请求链接，是不是要直接写成这样呢？ 这样也可以，但如果这些参数还需要手动拼接，未免有点不人性化。 一般情况下，这种信息我们利用params这个参数就可以直接传递了，示例如下： 返回结果： 把URL参数通过字典的形式传给get方法的params参数，通过返回信息可以判断，请求的链接自动被构造成了：。 网页的返回类型实际上是str类型，但是它很特殊，是JSON格式的。所以，如果想直接解析返回结果，得到一个JSON格式的数据的话，可以直接调用json方法。 示例如下： 结果如下： 调用json方法，就可以将返回结果是JSON格式的字符串转化为字典。 但需要注意的是，如果返回结果不是JSON格式，便会出现解析错误，抛出json.decoder.JSONDecodeError异常。 抓取网页上面的请求链接返回的是JSON形式的字符串，那么如果请求普通的网页，则肯定能获得相应的内容。下面以本课时最初的实例页面为例，我们再加上一点提取信息的逻辑，将代码完善成如下的样子： 运行结果： 抓取二进制数据抓取的是网站的一个页面，实际上它返回的是一个HTML文档。如果想抓取图片、音频、视频等文件，应该怎么办呢？ 图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以，想要抓取它们，就要拿到它们的二进制数据。 下面以 GitHub 的站点图标为例来看一下： 这里抓取的内容是站点图标，也就是在浏览器每一个标签上显示的小图标。 前者出现了乱码，后者结果前带有一个b，这代表是bytes类型的数据。 由于图片是二进制数据，所以前者在打印时转化为str类型，也就是图片直接转化为字符串，这当然会出现乱码。 上面返回的结果我们并不能看懂，它实际上是图片的二进制数据，没关系，将刚才提取到的信息保存下来就好了，代码如下： 这里用了open方法，它的第一个参数是文件名称，第二个参数代表以二进制的形式打开，可以向文件里写入二进制数据。 运行结束之后，可以发现在文件夹中出现了名为baidu.png的图标。 添加添headers在发起一个HTTP请求的时候，会有一个请求头Request Headers，那么这个怎么来设置呢？ 很简单，使用headers参数就可以完成了。 在刚才的实例中，是没有设置Request Headers信息的，如果不设置，某些网站会发现这不是一个正常的浏览器发起的请求，网站可能会返回异常的结果，导致网页抓取失败。 要添加Headers信息，比如添加一个User-Agent字段，可以这么写： 当然，我们可以在headers这个参数中任意添加其他的字段信息。 POST请求使用requests实现post请求，示例如下： 这里还是请求，该网站可以判断如果请求是POST方式，就把相关请求信息返回。 运行结果如下： 响应发送请求后，得到的就是响应，即Response。 在上面的实例中，使用text和content获取了响应的内容。此外，还有很多属性和方法可以用来获取其他信息，比如状态码、响应头、Cookies等。示例如下: 这里分别打印输出status_code属性得到状态码，输出headers属性得到响应头，输出cookies属性得到Cookies，输出url属性得到URL，输出history属性得到请求历史。 运行结果如下： headers和cookies这两个属性得到的结果分别是CaseInsensitiveDict和RequestsCookieJar类型。 状态码是用来表示响应状态的，比如返回200代表我们得到的响应是没问题的，上面的例子正好输出的结果也是200，所以可以通过判断Response的状态码来确认是否爬取成功。 requests还提供了一个内置的状态码查询对象requests.codes，用法示例如下： 这里通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用requests.codes.ok得到的是成功的状态码200。 这样的话，我们就不用再在程序里面写状态码对应的数字了，用字符串表示状态码会显得更加直观。 下面列出了返回码和相应的查询条件： 比如，如果想判断结果是不是404状态，可以用requests.codes.not_found来比对。 高级用法刚才，了解requests的基本用法，如基本的GET、POST请求以及Response对象。当然requests能做到的不仅这些，它几乎可以完成HTTP的所有操作。 下面来了解下requests的一些高级用法，如文件上传、Cookies设置、代理设置等。 文件上传requests可以模拟提交一些数据。假如有的网站需要上传文件，也可以用它来实现，示例如下： 要注意的是，baidu.png需要和当前脚本在同一目录下。如果有其他文件，当然也可以使用其他文件来上传，更改下代码即可。运行结果如下： 以上省略部分内容，这个网站会返回响应，里面包含files这个字段，而form字段是空的，这证明文件上传部分会单独有一个files字段来标识。 Cookies获取Cookies。 运行结果如下： 调用cookies属性即可成功得到Cookies，可以发现它是RequestCookieJar类型。然后用items 方法将其转化为元组组成的列表，遍历输出每一个Cookie的名称和值，实现Cookie的遍历解析。 可以直接用Cookie来维持登录状态，下面我们以GitHub为例来说明一下，首先我们登录GitHub，然后将Headers中的Cookie内容复制下来，如图所示： 可以替换成你自己的Cookie，将其设置到Headers里面，然后发送请求。 Session维持在requests中，如果直接利用get或post等方法的确可以做到模拟网页的请求，但是这实际上是相当于不同的Session，相当于你用两个浏览器打开了不同的页面。 设想这样一个场景，第一个请求利用post方法登录了某个网站，第二次想获取成功登录后的自己的个人信息，又用了一次get方法去请求个人信息页面。实际上，这相当于打开了两个浏览器，是两个完全不相关的Session，能成功获取个人信息吗？当然不能。 解决这个问题的主要方法就是维持同一个Session，相当于打开一个新的浏览器选项卡而不是新开一个浏览器。但我又不想每次设置Cookies，那该怎么办呢？这时候就有了新的利器——**Session对象**。 利用它，可以方便地维护一个Session，而且不用担心Cookies的问题，它会帮我们自动处理好。示例如下： 运行结果如下： 这并不行。再用 Session试试看： 运行结果如下： 利用Session，可以做到模拟同一个Session而不用担心Cookies的问题。它通常用于模拟登录成功之后再进行下一步的操作。 SSL证书验证现在很多网站都要求使用HTTPS协议，但是有些网站可能并没有设置好HTTPS证书，或者网站的HTTPS证书不被CA机构认可，这时候，这些网站可能就会出现SSL证书错误的提示。 比如示例网站：。 用浏览器打开这个URL，则会提示「您的连接不是私密连接」这样的错误，如图所示： 那如果我们一定要爬取这个网站怎么办呢？我们可以使用verify参数控制是否验证证书，如果将其设置为False，在请求时就不会再验证证书是否有效。如果不加verify参数的话，默认值是True，会自动验证。 改写代码如下： 这样就会打印出请求成功的状态码： 不过发现报了一个警告，它建议我们给它指定证书。可以通过设置忽略警告的方式来屏蔽这个警告： 或者通过捕获警告到日志的方式忽略警告： 当然，也可以指定一个本地证书用作客户端证书，这可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组： 上面的代码是演示实例，要有crt和key文件，并且指定它们的路径。另外注意，本地私有证书的key必须是解密状态，加密状态的key是不支持的。 超时时间在本机网络状况不好或者服务器网络响应延迟甚至无响应时，可能会等待很久才能收到响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错。这需要用到timeout参数。这个时间的计算是发出请求到服务器返回响应的时间。示例如下： 通过这样的方式将超时时间设置为1秒，如果1秒内没有响应，那就抛出异常。 实际上，请求分为两个阶段，即连接（connect）和读取（read）。 上面设置的timeout将用作连接和读取这二者的timeout总和。 如果要分别指定，就可以传入一个元组： 如果想永久等待，可以直接将timeout设置为None，或者不设置直接留空，因为默认是None。这样的话，如果服务器还在运行，但是响应特别慢，那就慢慢等吧，它永远不会返回超时错误的。其用法如下： 或直接不加参数： 身份认证在访问某些设置了身份认证的网站时，例如：，我们可能会遇到这样的认证窗口，如图所示： 如果遇到了这种情况，那就是这个网站启用了基本身份认证，英文叫作HTTP Basic Access Authentication，它是一种用来允许网页浏览器或其他客户端程序在请求时提供用户名和口令形式的身份凭证的一种登录验证方式。 如果遇到了这种情况，怎么用reqeusts来爬取呢，当然也有办法。 可以使用requests自带的身份认证功能，通过auth参数即可设置，示例如下： 成功的话，返回状态码200。 如果参数都传一个HTTPBasicAuth类，就显得有点烦琐了，所以requests提供了一个更简单的写法，可以直接传一个元组，它会默认使用HTTPBasicAuth这个类来认证。 上面的代码可以直接简写如下： 此外，requests还提供了其他认证方式，如OAuth认证，不过此时需要安装oauth包，安装命令如下： 使用OAuth1认证的方法如下： 更多详细的功能就可以参考requests_oauthlib的官方文档：，不再赘述。 代理设置某些网站在测试的时候请求几次，能正常获取内容。但是对于大规模且频繁的请求，网站可能会弹出验证码，或者跳转到登录认证页面，更甚者可能会直接封禁客户端的IP，导致一定时间段内无法访问。 为了防止这种情况发生，我们需要设置代理来解决这个问题，这就需要用到proxies参数。可以用这样的方式设置： 当然，直接运行这个实例或许行不通，因为这个代理可能是无效的，可以直接搜索寻找有效的代理并替换试验一下。 若代理需要使用上文所述的身份认证，可以使用类似ttp://user:password@host:port这样的语法来设置代理，示例如下： 除了基本的HTTP代理外，requests还支持SOCKS协议的代理。 首先，需要安装socks这个库： 然后就可以使用SOCKS协议代理了，示例如下： "},{"title":"Nginx配置详解","date":"2021-05-18T03:16:57.000Z","url":"/2021/05/18/Nginx%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/","tags":[["Nginx","/tags/Nginx/"],["负载均衡","/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"],["http","/tags/http/"]],"categories":[["Nginx","/categories/Nginx/"]],"content":"简介Nginx是lgor Sysoev为俄罗斯访问量第二的rambler.ru站点设计开发的。从2004年发布至今，凭借开源的力量，已经接近成熟与完善。 Nginx功能丰富，可作为HTTP服务器，也可作为反向代理服务器，邮件服务器。支持FastCGI、SSL、Virtual Host、URL Rewrite、Gzip等功能。并且支持很多第三方的模块扩展。 Nginx的稳定性、功能集、示例配置文件和低系统资源的消耗让他后来居上，在全球活跃的网站中有12.18%的使用比率，大约为2220万个网站。 Nginx常用功能1、Http代理，反向代理：作为web服务器最常用的功能之一，尤其是反向代理正向代理和反向代理： Nginx在做反向代理时，提供性能稳定，并且能够提供配置灵活的转发功能。Nginx可以根据不同的正则匹配，采取不同的转发策略，比如图片文件结尾的文件服务器，动态页面web服务器，只要你正则写的没问题，又有相对应的服务器解决方案，你就可以随心所欲的玩。并且Nginx对返回结果进行错误页跳转，异常判断等。如果被分发的服务器存在异常，他可以将请求重新转发给另外一台服务器，然后自动去除异常服务器。 2、负载均衡Nginx提供的负载均衡策略有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash。扩展策略，就天马行空，只有你想不到的没有他做不到的啦，你可以参照所有的负载均衡算法，给他一一找出来做下实现。 上3个图，理解这三种负载均衡算法的实现 Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 3、web缓存Nginx可以对不同的文件做不同的缓存处理，配置灵活，并且支持FastCGI_Cache，主要用于对FastCGI的动态程序进行缓存。配合着第三方的ngx_cache_purge，对制定的URL缓存内容可以的进行增删管理。 4、Nginx相关地址源码： 官网： Nginx配置文件结构如果你下载好啦，你的安装文件，不妨打开conf文件夹的nginx.conf文件，Nginx服务器的基础配置，默认的配置也存放在此。 在nginx.conf的注释符号为：# 默认的nginx配置文件nginx.conf内容如下： nginx 文件结构 1、全局块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。 2、events块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。 3、http块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。 4、server块：配置虚拟主机的相关参数，一个http中可以有多个server。 5、location块：配置请求的路由，以及各种页面的处理情况。 下面给大家上一个配置文件，作为理解。 上面是nginx的基本配置，需要注意的有以下几点： 1、几个常见配置项 2、惊群现象：一个网路连接到来，多个睡眠的进程被同时叫醒，但只有一个进程能获得链接，这样会影响系统性能。 3、每个指令必须有分号结束。 原文地址： "},{"title":"python多进程基本原理","date":"2021-05-17T12:04:49.000Z","url":"/2021/05/17/python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["python","/tags/python/"],["multiprocessing","/tags/multiprocessing/"]],"categories":[["Python","/categories/Python/"]],"content":"多进程的含义多进程（Process）是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是系统进行资源分配和调度的一个独立单位。 顾名思义，多进程就是启用多个进程同时运行。由于进程是线程的集合，而且进程是由一个或多个线程构成的，所以多进程的运 行意味着有大于或等于进程数量的线程在运行。 Python多进程的优势由于进程中GIL的存在，Python中的多线程并不能很好地发挥多核优势，一个进程中的多个线程，在同一时刻只能有一个线程运行。 对于多进程来说，每个进程都有属于自己的GIL，所以，在多核处理器下，多进程的运行是不会受GIL的影响的。因此，多进程能更好地发挥多核的优势。 当然，对于爬虫这种IO密集型任务来说，多线程和多进程影响差别并不大。对于计算密集型任务来说，Python的多进程相比多线程，其多核运行效率会有成倍的提升。 总的来说，Python的多进程整体来看是比多线程更有优势的。所以，在条件允许的情况下，能用多进程就尽量用多进程。 不过值得注意的是，由于进程是系统进行资源分配和调度的一个独立单位，所以各个进程之间的数据是无法共享的，如多个进程无法共享一个全局变量，进程之间的数据共享需要有单独的机制来实现到。 多进程的实现在Python中也有内置的库来实现多进程，它就是multiprocessing。 multiprocessing提供了一系列的组件，如Process（进程、Queue（队列）、Semaphore（信号量）、Pipe（管道）、Lock（锁）、Pool（进程池）等，接下来让我们来了解下它们的使用方法。 直接使用Process类在multiprocessing中，每一个进程都用一个Process类来表示。它的API调用如下： target表示调用对象，你可以传入方法的名字。 args表示被调用对象的位置参数元组，比如target是函数func，他有两个参数m，n，那么args就传入[m, n]即可。 kwargs表示调用对象的字典。 name是别名，相当于给这个进程取一个名字。 group分组。 先用一个实例来感受一下： 这是一个实现多进程最基础的方式：通过创建Process来新建一个子进程，其中target参数传入方法名，args是方法的参数，是以 元组的形式传入，其和被调用的方法process的参数是一一对应的。 注意：这里args 必须要是一个元组，如果只有一个参数，那也要在元组第一个元素后面加一个逗号，如果没有逗号则和单个元素本身没有区别，无法构成元组，导致参数传递出现问题。 创建完进程之后，我们通过调用start方法即可启动进程了。运行结果如下： 运行了5个子进程，每个进程都调用了process方法。process方法的index参数通过Process的args传入，分别是0~4这5个序号，最后打印出来，5个子进程运行结束。 由于进程是Python中最小的资源分配单元，因此这些进程和线程不同，各个进程之间的数据是不会共享的，每启动一个进程，都会独立分配资源。 在当前CPU核数足够的情况下，这些不同的进程会分配给不同的CPU核来运行，实现真正的并行执行。 运行结果如下： 通过cpu_count成功获取了CPU核心的数量：4个，不同的机器结果可能不同。通过 active_children获取到了当前正在活跃运行的进程列表。然后遍历每个进程，并将它们的名称和进程号打印出来了，这里进程号直接使用pid属性即可获取，进程名称直接通过name属性即可获取。 继承继Process类在上面的例子中，创建进程是直接使用Process这个类来创建的，这是一种创建进程的方式。不过，创建进程的方式不止这一 种，同样，也可以像线程Thread一样来通过继承的方式创建一个进程类，进程的基本操作我们在子类的run方法中实现即可。 通过一个实例来看一下： 声明了一个构造方法，这个方法接收一个loop参数，代表循环次数，并将其设置为全局变量。在run方法中，又使用这个loop变量循环了loop次并打印了当前的进程号和循环次数。 在调用时，用range方法得到了2、3、4三个数字，并把它们分别初始化了MyProcess进程，然后调用start方法将进程启动起来。 注意：这里进程的执行逻辑需要在run方法中实现，启动进程需要调用start方法，调用之后run方法便会执行。 运行结果如下： 三个进程分别打印出了2、3、4条结果，即进程13560打印了2次结果，进程9908 打印了3次结果，进程13728打印了4次结果。 注意，这里的进程pid代表进程号，不同机器、不同时刻运行结果可能不同。 通过上面的方式，非常方便地实现了一个进程的定义。为了复用方便，可以把一些方法写在每个进程类里封装好，在使用时直接初始化一个进程类运行即可。 守护进程在多进程中，同样存在守护进程的概念，如果一个进程被设置为守护进程，当父进程结束后，子进程会自动被终止，我们可以通过设置daemon属性来控制是否为守护进程。 还是原来的例子，增加了deamon属性的设置： 运行结果如下： 结果很简单，因为主进程没有做任何事情，直接输出一句话结束，所以在这时也直接终止了子进程的运行。 这样可以有效防止无控制地生成子进程。这样的写法可以让我们在主进程运行结束后无需额外担心子进程是否关闭，避免了独立子进程的运行。 进程等待上面的运行效果其实不太符合我们预期：主进程运行结束时，子进程（守护进程）也都退出了，子进程什么都没来得及执行。 能不能让所有子进程都执行完了然后再结束呢？当然是可以的，只需要加入join方法即可，可以将代码改写如下： 运行结果如下： 在调用start和join方法后，父进程就可以等待所有子进程都执行完毕后，再打印出结束的结果。 默认情况下，join是无限期的。也就是说，如果有子进程没有运行完毕，主进程会一直等待。这种情况下，如果子进程出现问题陷入了死循环，主进程也会无限等待下去。怎么解决这个问题呢？可以给join方法传递一个超时参数，代表最长等待秒数。如果子进程没有在这个指定秒数之内完成，会被强制返回，主进程不再会等待。也就是说这个参数设置了主进程等待该子进程的最长时间。 例如这里传入1，代表最长等待1秒，代码改写如下： 运行结果如下： 可以看到，有的子进程本来要运行3秒，结果运行1秒就被强制返回了，由于是守护进程，该子进程被终止了。 终止进程终止进程不止有守护进程这一种做法，我们也可以通过terminate方法来终止某个子进程，另外我们还可以通过is_alive方法判断进程是否还在运行。 下面看一个实例： 用Process创建了一个进程，接着调用start方法启动这个进程，然后调用terminate方法将进程终止，最后调用join方法。 另外，在进程运行不同的阶段，通过is_alive方法判断当前进程是否还在运行。 运行结果如下： 这里有一个值得注意的地方，在调用terminate方法之后，我们用is_alive方法获取进程的状态发现依然还是运行状态。在调用join方法之后，is_alive方法获取进程的运行状态才变为终止状态。 所以，在调用terminate方法之后，记得要调用一下join方法，这里调用join方法可以为进程提供时间来更新对象状态，用来反映出最终的进程终止效果。 进程互斥锁 在访问一些临界区资源时，使用Lock可以有效避免进程同时占用资源而导致的一些问题。 信号量｀multiprocessing库中的Semaphore`来实现信号量，实现多个进程共享资源，同时限制可访问的进程数量。 道管管道（Pipe）用来实现进程之间的通讯，管道可以是单向的，即half-duplex：一个进程负责发消息，另一个进程负责收消息；也可以是双向的duplex，即互相收发消息。 默认声明Pipe对象是双向管道，如果要创建单向管道，可以在初始化的时候传入deplex参数为False。 声明了一个默认为双向的管道，然后将管道的两端分别传给两个进程。两个进程互相收发。观察一下结果： 管道Pipe就像进程之间搭建的桥梁，利用它可以很方便地实现进程间通信。 进程池 假如现在我们遇到这么一个问题，我有10000个任务，每个任务需要启动一个进程来执行，并且一个进程运行完毕之后要紧接着 启动下一个进程，同时我还需要控制进程的并发数量，不能并发太高，不然CPU处理不过来（如果同时运行的进程能维持在一个 最高恒定值当然利用率是最高的）。 那么我们该如何来实现这个需求呢？ 用Process和Semaphore可以实现，但是实现起来比较烦琐。这种需求在平时又是非常常见的。此时，我们就可以派上进程池了，即multiprocessing中的Pool。 Pool可以提供指定数量的进程，供用户调用，当有新的请求提交到pool中时，如果池还没有满，就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行它。 用一个实例来实现一下，代码如下： 声明了一个大小为3的进程池，通过processes参数来指定，如果不指定，那么会自动根据处理器内核来分配进程数。接着我们使用apply_async方法将进程添加进去，args可以用来传递参数。 运行结果如下： 进程池大小为3，可以看到有3个进程同时执行，第4个进程在等待，在有进程运行完毕之后，第4个进程马上跟着运行，出现了如上的运行效果。 最后，我们要记得调用close方法来关闭进程池，使其不再接受新的任务，然后调用join方法让主进程等待子进程的退出，等子进程运行完毕之后，主进程接着运行并结束。 上面的写法多少有些烦琐，使用你进程池的map方法，可以将上述写法简化很多。 map方法是怎么用的呢？第一个参数就是要启动的进程对应的执行方法，第2个参数是一个可迭代对象，其中的每个元素会被传递给这个执行方法。 举个例子：现在有一个list，里面包含了很多URL，定义了一个方法用来抓取每个URL内容并解析，那么可以直接在map的第一个参数传入方法名，第2个参数传入URL数组。 用一个实例来感受一下： 运行结果： 这样，就可以实现3个进程并行运行。不同的进程相互独立地输出了对应的爬取结果。"},{"title":"python多线程基本原理","date":"2021-05-12T03:42:12.420Z","url":"/2021/05/12/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["python","/tags/python/"],["多线程","/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"],["threading","/tags/threading/"]],"categories":[["Python","/categories/Python/"]],"content":"多线程的含义说起多线程，就不得不先说什么是线程。然而想要弄明白什么是线程，又不得不先说什么是进程。 进程可以理解为是一个可以独立运行的程序单位，比如打开一个浏览器，这就开启了一个浏览器进程；打开一个文本编辑器，这就开启了一个文本编辑器进程。但一个进程中是可以同时处理很多事情的，比如在浏览器中，可以在多个选项卡中打开多个页面，有的页面在播放音乐，有的页面在播放视频，有的网页在播放动画，它们可以同时运行，互不干扰。为什么能同时做到同时运行这么多的任务呢？这里就需要引出线程的概念了，其实这一个个任务，实际上就对应着一个个线程的执行。 而进程呢？它就是线程的集合，进程就是由一个或多个线程构成的，线程是操作系统进行运算调度的最小单位，是进程中的一个最小运行单元。比如上面所说的浏览器进程，其中的播放音乐就是一个线程，播放视频也是一个线程，当然其中还有很多其他的线程在同时运行，这些线程的并发或并行执行最后使得整个浏览器可以同时运行这么多的任务。 了解了线程的概念，多线程就很容易理解了，多线程就是一个进程中同时执行多个线程，前面所说的浏览器的情景就是典型的多线程执行。 并发和并行并说到多进程和多线程，这里就需要再讲解两个概念，那就是并发和并行。知道，一个程序在计算机中运行，其底层是处理器通过运行一条条的指令来实现的。 并发，英文叫作concurrency。它是指同一时刻只能有一条指令执行，但是多个线程的对应的指令被快速轮换地执行。比如一个处理器，它先执行线程A的指令一段时间，再执行线程B的指令一段时间，再切回到线程A执行一段时间。 由于处理器执行指令的速度和切换的速度非常非常快，人完全感知不到计算机在这个过程中有多个线程切换上下文执行的操作，这就使得宏观上看起来多个线程在同时运行。但微观上只是这个处理器在连续不断地在多个线程之间切换和执行，每个线程的执行一定会占用这个处理器一个时间片段，同一时刻，其实只有一个线程在执行。 并行，英文叫作parallel。它是指同一时刻，有多条指令在多个处理器上同时执行，并行必须要依赖于多个处理器。不论是从宏观上还是微观上，多个线程都是在同一时刻一起执行的。 并行只能在多处理器系统中存在，如果的计算机处理器只有一个核，那就不可能实现并行。而并发在单处理器和多处理器系统中都是可以存在的，因为仅靠一个核，就可以实现并发。 举个例子，比如系统处理器需要同时运行多个线程。如果系统处理器只有一个核，那它只能通过并发的方式来运行这些线程。如果系统处理器有多个核，当一个核在执行一个线程时，另一个核可以执行另一个线程，这样这两个线程就实现了并行执行，当然其他的线程也可能和另外的线程处在同一个核上执行，它们之间就是并发执行。具体的执行方式，就取决于操作系统的调度了。 多线程适用场景多在一个程序进程中，有一些操作是比较耗时或者需要等待的，比如等待数据库的查询结果的返回，等待网页结果的响应。如果使用单线程，处理器必须要等到这些操作完成之后才能继续往下执行其他操作，而这个线程在等待的过程中，处理器明显是可以来执行其他的操作的。如果使用多线程，处理器就可以在某个线程等待的时候，去执行其他的线程，从而从整体上提高执行效率。 像上述场景，线程在执行过程中很多情况下是需要等待的。比如网络爬虫就是一个非常典型的例子，爬虫在向服务器发起请求之后，有一段时间必须要等待服务器的响应返回，这种任务就属于IO密集型任务。对于这种任务，如果启用多线程，处理器就可以在某个线程等待的过程中去处理其他的任务，从而提高整体的爬取效率。 但并不是所有的任务都是IO密集型任务，还有一种任务叫作计算密集型任务，也可以称之为CPU密集型任务。顾名思义，就是任务的运行一直需要处理器的参与。此时如果开启了多线程，一个处理器从一个计算密集型任务切换到切换到另一个计算密集型任务上去，处理器依然不会停下来，始终会忙于计算，这样并不会节省总体的时间，因为需要处理的任务的计算总量是不变的。如果线程数目过多，反而还会在线程切换的过程中多耗费一些时间，整体效率会变低。 所以，如果任务不全是计算密集型任务，可以使用多线程来提高程序整体的执行效率。尤其对于网络爬虫这种IO密集型任务来说，使用多线程会大大提高程序整体的爬取效率。 Python实现多线程实在Python中，实现多线程的模块叫作threading，是Python自带的模块。使用threading实现多线程的方法。 ###Thread直接创建子线程 首先，可以使用Thread类来创建一个线程，创建时需要指定target参数为运行的方法名称，如果被调用的方法需要传入额外的参数，则可以通过Thread的args参数来指定。示例如下： 运行结果如下： 在这里首先声明了一个方法，叫作target，它接收一个参数为second，通过方法的实现可以发现，这个方法其实就是执行了一个time.sleep休眠操作，second参数就是休眠秒数，其前后都print了一些内容，其中线程的名字通过threading.current_thread().name来获取出来，如果是主线程的话，其值就是MainThread，如果是子线程的话，其值就是Thread-*。 然后通过Thead类新建了两个线程，target参数就是刚才所定义的方法名，args以列表的形式传递。两次循环中，这里i分别就是1和5，这样两个线程就分别休眠1秒·和5秒，声明完成之后，调用start方法即可开始线程的运行。 观察结果可以发现，这里一共产生了三个线程，分别是主线程MainThread和两个子线程Thread-1、Thread-2。另外观察到，主线程首先运行结束，紧接着Thread-1、Thread-2才接连运行结束，分别间隔了1秒和4秒。这说明主线程并没有等待子线程运行完毕才结束运行，而是直接退出了，有点不符合常理。如果想要主线程等待子线程运行完毕之后才退出，可以让每个子线程对象都调用下join方法，实现如下： 运行结果如下： 主线程必须等待子线程都运行结束，主线程才继续运行并结束。 继承Thread类创建子线程另外，也可以通过继承Thread类的方式创建一个线程，该线程需要执行的方法写在类的run方法里面即可。上面的例子的等价改写为： 运行结果如下： 可以看到，两种实现方式，其运行效果是相同的。 守护进程在线程中有一个叫作守护线程的概念，如果一个线程被设置为守护线程，那么意味着这个线程是“不重要”的，这意味着，如果主线程结束了而该守护线程还没有运行完，那么它将会被强制结束。在Python中可以通过setDaemon方法来将某个线程设置为守护线程。 示例如下： 在这里通过 setDaemon方法将 t2 设置为了守护线程，这样主线程在运行完毕时，t2 线程会随着线程的结束而结束。 运行结果如下： 可以看到，没有Thread-2打印退出的消息，Thread-2随着主线程的退出而退出了。 这里并没有调用join方法，如果让t1和t2都调用join方法，主线程就会仍然等待各个子线程执行完毕再退出，不论其是否是守护线程。 互斥锁互在一个进程中的多个线程是共享资源的，比如在一个进程中，有一个全局变量count用来计数，声明多个线程，每个线程运行时都给count加1，代码实现如下： 在这里，声明了1000个线程，每个线程都是现取到当前的全局变量count值，然后休眠一小段时间，然后对count赋予新的值。 按照常理来说，最终的count值应该为1000。但其实不然。运行结果如下： 最后的结果居然只有15，而且多次运行或者换个环境运行结果是不同的.这是为什么呢？因为count这个值是共享的，每个线程都可以在执行temp = count这行代码时拿到当前count的值，但是这些线程中的一些线程可能是并发或者并行执行的，这就导致不同的线程拿到的可能是同一个count值，最后导致有些线程的count的加1操作并没有生效，导致最后的结果偏小。 所以，如果多个线程同时对某个数据进行读取或修改，就会出现不可预料的结果。为了避免这种情况，我们需要对多个线程进行同步，要实现同步，我们可以对需要操作的数据进行加锁保护，这里就需要用到threading.Lock了。 加锁保护是什么意思呢？就是说，某个线程在对数据进行操作前，需要先加锁，这样其他的线程发现被加锁了之后，就无法继续向下执行，会一直等待锁被释放，只有加锁的线程把锁释放了，其他的线程才能 继续加锁并对数据做修改，修改完了再释放锁。这样可以确保同一时间只有一个线程操作数据，多个线程不会再同时读取和修改同一个数据，这样最后的运行结果就是对的了。 将代码修改为如下内容： 运行结果如下： Python多线程的问题由于Python中GIL的限制，导致不论是在单核还是多核条件下，在同一时刻只能运行一个线程，导致Python多线程无法发挥多核并行的优势。 GIL全称为GlobalInterpreter Lock，中文翻译为全局解释器锁，其最初设计是出于数据安全而考虑的。 在Python多线程下，每个线程的执行方式如下： 获取GIL执行对应线程的代码 释放GIL可见，某个线程想要执行，必须先拿到 GIL，我们可以把 GIL看作是通行证，并且在一个 Python进程中，GIL只有一个。拿不到通行证的线程，就不允许执行。这样就会导致，即使是多核条件下，一个 Python 进程下的多个线程，同一时刻也只能执行一个线程。 不过对于爬虫这种 IO 密集型任务来说，这个问题影响并不大。而对于计算密集型任务来说，由于 GIL的存在，多线程总体的运行效率相比可能反而比单线程更低 "},{"title":"Session和Cookies","date":"2021-05-12T02:51:36.000Z","url":"/2021/05/12/Session%E5%92%8CCookies/","tags":[["http","/tags/http/"],["web","/tags/web/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"静态网页和动态网页示例代码： 这是最基本的HTML代码，保存为一个.html文件，然后把它放在某台具有固定公网IP的主机上，主机上装上Apache或Nginx等服务器，这样这台主机就可以作为服务器了，其他人便可以通过访问服务器看到这个页面，这就搭建了一个最简单的网站。 这种网页的内容是HTML代码编写的，文字、图片等内容均通过写好的HTML代码来指定，这种页面叫作静态网页。它加载速度快，编写简单，但是存在很大的缺陷，如可维护性差，不能根据URL灵活多变地显示内容等。例如，想要给这个网页的URL传入一个name参数，让其在网页中显示出来，是无法做到的。 因此，动态网页应运而生，它可以动态解析URL中参数的变化，关联数据库并动态呈现不同的页面内容，非常灵活多变。现在遇到的大多数网站都是动态网站，它们不再是一个简单的HTML，而是可能由JSP、PHP、Python等语言编写的，其功能比静态网页强大和丰富太多了。 此外，动态网站还可以实现用户登录和注册的功能。按照一般的逻辑来说，输入用户名和密码登录之后，肯定是拿到了一种类似凭证的东西，有了它，才能保持登录状态，才能访问登录之后才能看到的页面。 那么，这种神秘的凭证到底是什么呢？其实它就是 Session和Cookies 共同产生的结果。 无状态HTTP在了解Session和Cookies之前，还需要了解HTTP的一个特点，叫作无状态。 HTTP的无状态是指HTTP协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。 当向服务器发送请求后，服务器解析此请求，然后返回对应的响应，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录。 这意味着如果后续需要处理前面的信息，则必须重传，这也导致需要额外传递一些前面的重复请求，才能获取后续响应，这种效果显然不是想要的。为了保持前后状态，肯定不能将前面的请求全 部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。 这时两个用于保持HTTP连接状态的技术就出现了，它们分别是Session和Cookies。Session在服务端，也就是网站的服务器，用来保存用户的Session信息；Cookies在客户端，也可以理解为浏览器端，有了Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别Cookies并鉴定出是哪个用户，然后再判断用户是否是登录状态，进而返回对应的响应。 可以理解为Cookies里面保存了登录的凭证，有了它，只需要在下次请求携带Cookies发送请求而不必重新输入用户名、密码等信息重新登录了。 因此在爬虫中，有时候处理需要登录才能访问的页面时，一般会直接将登录成功后获取的Cookies放在请求头里面直接请求，而不必重新模拟登录。 CookiesCookies指某些网站为了辨别用户身份、进行Session跟踪而存储在用户本地终端上的数据。 SessionSession，中文称之为会话，其本身的含义是指有始有终的一系列动作/消息。比如，打电话时，从拿起电话拨号到挂断电话这中间的一系列过程可以称为一个 Session。 而在Web中，Session对象用来存储特定用户Session所需的属性及配置信息。这样，当用户在应用程序的Web页之间跳转时，存储在Session对象中的变量将不会丢失，而是在整个用户Session中一直存在下去。当用户请求来自应用程序的Web页时，如果该用户还没有Session，则Web服务器将自动创建一个 Session对象。当Session过期或被放弃后，服务器将终止该Session。 Session维持那么，怎样利用Cookies保持状态呢？当客户端第一次请求服务器时，服务器会返回一个响应头中带有Set-Cookie字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把Cookies保存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies放到请求头一起提交给服务器，Cookies携带了SessionID 信息，服务器检查该Cookies即可找到对应的Session是什么，然后再判断Session来以此来辨认用户状态。 在成功登录某个网站时，服务器会告诉客户端设置哪些Cookies信息，在后续访问页面时客户端会把Cookies发送给服务器，服务器再找到对应的Session加以判断。如果Session中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。 反之，如果传给服务器的Cookies是无效的，或者Session已经过期了，将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录。 所以，Cookies和Session需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录Session控制。 Cookies属性结构Cookie有如下几个属性。 Name，即该Cookie的名称。 Cookie一旦创建，名称便不可更改。 Value，即该Cookie的值。如果值为Unicode字符，需要为字符编码。如果值为二进制数据，则需要使用BASE64编码。 MaxAge，即该Cookie失效的时间，单位秒，也常和Expires一起使用，通过它可以计算出其有效时间。 MaxAge 如果为正数，则该Cookie在MaxAge秒之后失效。如果为负数，则关闭浏览器时Cookie即失效，浏览器也不会以任何形式保存该Cookie。 Path，即该Cookie的使用路径。如果设置为/path/，则只有路径为/path/的页面可以访问该Cookie。如果设置为/，则本域名下的所有页面都可以访问该Cookie。 Domain，即可以访问该Cookie的域名。例如如果设置为.zhihu.com，则所有以zhihu.com，结尾的域名都可以访问该Cookie。 Size字段，即此Cookie的大小。 Http字段，即Cookie的httponly属性。若此属性为true，则只有在HTTP Headers中会带有此Cookie的信息，而不能通过document.cookie来访问此Cookie。 Secure，即该Cookie是否仅被使用安全协议传输。安全协议。安全协议有HTTPS、SSL等，在网络上传输数据之前先将数据加密。默认为false。 会话会Cookie和持久和Cookie从表面意思来说，会话Cookie就是把Cookie放在浏览器内存里，浏览器在关闭之后该Cookie即失效；持久Cookie则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态。 严格来说，没有会话Cookie和持久Cookie之分，只是由Cookie的MaxAge或Expires字段决定了过期的时间。 因此，一些持久化登录的网站其实就是把Cookie的有效时间和Session有效期设置得比较长，下次再访问页面时仍然携带之前的Cookie，就可以直接保持登录状态。 常见误区在谈论Session机制的时候，常常听到这样一种误解 ——“只要关闭浏览器，Session就消失了”。可以想象一下会员卡的例子，除非顾客主动对店家提出销卡，否则店家绝对不会轻易删除顾客的资料。对Session来说，也是一样，除非程序通知服务器删除一个Session，否则服务器会一直保留。比如，程序一般都是在做注销操作时才去删除Session。 但是当关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭。之所以会有这种错觉，是因为大部分网站都使用会话Cookie来保存Session ID信息，而关闭浏览器后Cookies就消失了，再次连接服务器时，也就无法找到原来的Session了。如果服务器设置的Cookies保存到硬盘上，或者使用某种手段改写浏览器发出的HTTP请求头，把原来的Cookies发送给服务器，则再次打开浏览器，仍然能够找到原来的Session ID，依旧还是可以保持登录状态的。 而且恰恰是由于关闭浏览器不会导致Session被删除，这就需要服务器为Session设置一个失效时间，当距离客户端上一次使用Session的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把Session删除以节省存储空间。"},{"title":"爬虫的基本原理","date":"2021-05-11T16:12:50.000Z","url":"/2021/05/12/%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["python","/tags/python/"],["http","/tags/http/"],["爬虫","/tags/%E7%88%AC%E8%99%AB/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。如果把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。 爬虫概述简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，概要介绍一下。 获取网页获爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。 源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。 前面讲了请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个 请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样实现呢？ Python提供了许多库来帮助我们实现这个操作，如urllib、requests等。可以用这些库来帮助我们实现HTTP请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的Body部分，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。 提取信息提获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。 另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS选择器或XPath来提取网页信息的库，如Beautiful Soup、pyquery、lxml等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。 提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。 保存数据提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为TXT文本或JSON文本，也可以保存到数据库，如MySQL和MongoDB等，还可保存至远程服务器，如借助SFTP进行操作等。 自动化程序自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各 种异常处理、错误重试等操作，确保爬取持续高效地运行。 能抓怎样的数据能在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML代码，而最常抓取的便是HTML源代码。 另外，可能有些网页返回的不是HTML代码，而是一个JSON字符串（其中API接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。 此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。 另外，还可以看到各种扩展名的文件，如CSS、JavaScript和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。 上述内容其实都对应各自的URL，是基于HTTP或HTTPS协议的，只要是这种数据，爬虫都可以抓取。 JavaScript渲染页面有时候，在用urllib或requests抓取网页时，得到的源代码实际和浏览器中看到的不一样。 这是一个非常常见的问题。现在网页越来越多地采用Ajax、前端模块化工具来构建，整个网页可能都是由JavaScript渲染出来的，也就是说原始的HTML代码就是一个空壳，例如： body节点里面只有一个id为container的节点，但是需要注意在body节点后引入了app.js，它便负责整个网站的渲染。 在浏览器中打开这个页面时，首先会加载这个HTML内容，接着浏览器会发现其中引入了一个app.js文件，然后便会接着去请求这个文件，获取到该文件后，便会执行其中的JavaScript代码，而JavaScript则会改变HTML中的节点，向其添加内容，最后得到完整的页面。 但是在用urllib或requests等库请求当前页面时，我们得到的只是这个HTML代码，它不会帮助我们去继续加载这个JavaScript文件，这样也就看不到浏览器中的内容。 这也解释了为什么有时我们得到的源代码和浏览器中看到的不一样。 因此，使用基本HTTP请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，可以分析其后台Ajax接口，也可使用Selenium、Splash这样的库来实现模拟JavaScript渲染。"},{"title":"web网页基础","date":"2021-05-11T15:32:38.000Z","url":"/2021/05/11/web%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80/","tags":[["javascript","/tags/javascript/"],["html","/tags/html/"],["css","/tags/css/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"网页的组成首先，我们来了解网页的基本组成，网页可以分为三大部分：HTML、CSS和JavaScript。 如果把网页比作一个人的话，HTML相当于骨架，JavaScript相当于肌肉，CSS相当于皮肤，三者结合起来才能形成一个完整的网页。下面我们来分别介绍一下这三部分的功能。 HTMLHTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。 我们浏览的网页包括文字、按钮、图片和视频等各种复杂的元素，其基础架构就是HTML。不同类型的元素通过不同类型的标签来表示，如图片用img标签表示，视频用video标签表示，段落用p标签表示，它们之间的布局又常通过布局标签div嵌套组合而成，各种标签通过不同的排列和嵌套就可以形成网页的框架。 在Chrome浏览器中打开百度，右击并选择 “检查”项（或按F12键），打开开发者模式，这时在Elements选项卡中即可看到网页的源代码，如图所示。 这就是HTML，整个网页就是由各种标签嵌套组合而成的。这些标签定义的节点元素相互嵌套和组合形成了复杂的层次关系，就形成了网页的架构。 CSS虽然HTML定义了网页的结构，但是只有HTML页面的布局并不美观，可能只是简单的节点元素的排列，为了让网页看起来更好看一些，这里就需要借助CSS了。 CSS，全称叫作Cascading Style Sheets，即层叠样式表。“层叠”是指当在HTML中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式”指网页中文字大小、颜色、元素间距、排列等 格式。 CSS是目前唯一的网页页面排版样式标准，有了它的帮助，页面才会变得更为美观。例如： 这就是一个CSS样式。大括号前面是一个CSS选择器。此选择器的作用是首先选中id为head_wrapper且class 为s-ps-islite的节点，然后再选中其内部的class为s-p-top的节点。大括号内部写的就是一条条样式规则，例如position指定了这个元素的布局方式为绝对布局，bottom指定元素的下边距为40像素，width指定了宽度为100%占满父元素，height则指定了元素的高度。也就是说，我们将位置、宽度、高度等样式配置统一写成这样的形式，然后用大括号括起来，接着在开头再加上CSS选择器，这就代表这个样式对CSS选择器选中的元素生效，元素就会根据此样式来展示了。 在网页中，一般会统一定义整个网页的样式规则，并写入CSS文件中（其后缀为css）。在HTML中，只需要用link标签即可引入写好的CSS文件，这样整个页面就会变得美观、优雅。 JavaScriptJavaScript，简称JS，是一种脚本语言。HTML和CSS配合使用，提供给用户的只是一种静态信息，缺乏交互性。我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是JavaScript的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系，而是实现了一种实时、动态、交互的页面功能。 JavaScript通常也是以单独的文件形式加载的，后缀为js，在 HTML中通过script标签即可引入，例如: 综上所述，HTML定义了网页的内容和结构，CSS描述了网页的布局，JavaScript定义了网页的行为。"},{"title":"在Hexo博客中嵌入外链视频","date":"2021-05-11T01:12:11.000Z","url":"/2021/05/11/%E5%9C%A8Hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%B5%8C%E5%85%A5%E5%A4%96%E9%93%BE%E8%A7%86%E9%A2%91/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"嵌入的视频Hexo支持Youtube视频的嵌入，可以参考其实现方式。 首先，在node_modules/hexo/lib/plugins/tag/index.js中添加以下代码。 然后在node_modules/hexo/lib/plugins/tag/目录下新建bilibili.js文件，打开并添加如下代码： 重新启动下Hexo Server,在md页面中添加下列： 重新刷新页面，就可以看到视频正常加载了。 "},{"title":"腾讯云配置ssl证书","date":"2021-05-10T07:36:11.000Z","url":"/2021/05/10/%E8%85%BE%E8%AE%AF%E4%BA%91%E9%85%8D%E7%BD%AEssl%E8%AF%81%E4%B9%A6/","tags":[["nginx","/tags/nginx/"],["https","/tags/https/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"什么是SSL证书？SSL证书是用于在Web服务器与浏览器以及客户端之间建立加密链接的加密技术，通过配置和应用SSL证书来启用HTTPS协议，来保证互联网数据传输的安全，全球每天有数以亿计的网站都是通过HTTPS来确保数据安全，保护用户隐私。 申请腾讯云SSL证书百毒搜索腾讯SSL证书，找到免费使用SSL一年的产品，一系列骚操作后得到证书。 nginx配置修改下载SSL证书，上传到服务器/etc/pki/nginx/目录下。 修改/etc/配置文件，注意SSL证书的路径和实际上传的路径和名称一致。 "},{"title":"Hexo在腾讯云的部署","date":"2021-05-10T04:51:56.000Z","url":"/2021/05/10/Hexo%E5%9C%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E7%9A%84%E9%83%A8%E7%BD%B2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"简介Hexo在GitHub pages上的访问太慢了，迁移到腾讯云服务器上。 部署环境腾讯云服务器（Centos 64位）。 服务器配置安装git 创建git用户并修改权限 找到一下内容 在该语句下添加 退出（esc + :wq）并修改权限 本地使用gitbash创建密钥 在腾讯云中创建ssh，并将本地的id_rsa.pub中的文件内容全部复制到authorized_keys中。 修改权限 本地测试 云服务器中创建网站目录并设置权限 安装nginx 以上执行完之后，在浏览器中输入你的公网IP如果可以进入CentOs界面，说明Nginx安装成功。 配置nginx 重启服务 建立git仓库并修改权限 同步网站根目录 填入如下内容 修改权限 在本地Hexo目录下修改_config.yml文件中的deploy后的repo改为： 以上全部完成后，执行hexo的部署命令即可完成在腾讯云服务器上的博客部署。"},{"title":"HTTP基本原理","date":"2021-05-09T12:48:23.000Z","url":"/2021/05/09/HTTP%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["http","/tags/http/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"URI和URLURI （Uniform Resource Identifier） 即统一资源标志符。URL （Uniform Resource Locator 即统一资源定位符。 例如：既是一个URL，也是一个URI。用URL/URI来唯一指定了它的访问方式，这其中包括了访问协议Https，访问路径（即根目录）和资源名称favicon.ico。 URL是URI的一个子集，也就是每个URL都是URI，但不是每个URI都是URL。 URI还包括一个子类叫做URN（Universal Resource Name）即统一资源名称。但是在目前的互联网，URN的使用非常少，几乎所有的 URI都是URL，所以一般的网页链接我们可以称之为 URL，也可以称之为 URI。 超文本Hypertext，我们在浏览器里看到的网页就是超文本解析而成的，其网页源代码是一系列 HTML代码，里面包含了一系列标签，比如img显示图片，p指定显示段落等。浏览器解析这些标签后，便形成了我们平常看到的网页，而网页的源代码HTML就可以称作超文本。 HTTP和HTTPSHTTP的全称是Hyper Text Transfer Protocol，中文名叫作超文本传输协议，HTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。HTTP由万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet EngineeringTask Force）共同合作制定的规范，目前广泛使用的是HTTP 1.1版本。 HTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，简称为HTTPS。 HTTPS的安全基础是SSL，因此通过它传输的内容都是经过SSL加密的，它的主要作用可以分为两种： 建立一个信息安全通道，来保证数据传输的安全。 确认网站的真实性，凡是使用了HTTPS的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过CA机构颁发的安全签章来查询。 现在越来越多的网站和 App 都已经向 HTTPS 方向发展。例如： 苹果公司强制所有 iOS App 在 2017 年 1 月 1 日 前全部改为使用 HTTPS 加密，否则 App 就无法在应用商店上架。 谷歌从 2017 年 1 月推出的 Chrome 56 开始，对未进行 HTTPS 加密的网址链接亮出风险提示，即在地址栏的显著位置提醒用户 “此网页不安全”。 腾讯微信小程序的官方需求文档要求后台使用 HTTPS 请求进行网络通信，不满足条件的域名和协议无法请求。 因此，HTTPS 已经已经是大势所趋。 HTTP请求过程我们在浏览器中输入一个URL，回车之后便可以在浏览器中观察到页面内容。实际上，这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器。响应里包含了页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来，传输模型如图所示： 此处客户端即代表我们自己的 PC 或手机浏览器，服务器即要访问的网站所在的服务器。 为了更直观地说明这个过程，这里用浏览器的开发者模式下的Network监听组件来做演示，它可以显示访问当前请求网页时发生的所有网络请求和响应。打开浏览器（Chrome或Edge都可以），右击并选择 “检查”项，即可打开浏览器的开发者工具。这里访问百度，输入该 URL后回车，观察这个过程中发生了怎样的网络请求。可以看到，在Network页面下方 出现了一个个的条目，其中一个条目就代表一次发送请求和接收响应的过程，如图所示: 我们先观察第一个网络请求，即www.baidu.com，其中各列的含义如下。 第一列 Name：请求的名称，一般会将 URL的最后一部分内容当作名称。 第二列 Status：响应的状态码，这里显示为 200，代表响应是正常的。通过状态码，我们可以判断发送了请求之后是否得到了正常的响应。 第三列 Type：请求的文档类型。这里为document，代表我们这次请求的是一个 HTML文档，内容就是一些 HTML代码。 第四列 Initiator：请求源。用来标记请求是由哪个对象或进程发起的。 第五列 Size：从服务器下载的文件和请求的资源大小。如果是从缓存中取得的资源，则该列会显示 fromcache。 第六列 Time：发起请求到获取响应所用的总时间。 第七列 Waterfall：网络请求的可视化瀑布流。 我们点击这个条目即可看到其更详细的信息，如图所示。 首先是General部分，Request URL为请求的URL，Request Method为请求的方法，Status Code为响应状态码，Remote Address为远程服务器的地址和端口，Referrer Policy为 Referrer判别策略。 再继续往下，可以看到，有Response Headers和Request Headers，这分别代表响应头和请求头。请求头里带有许多请求信息，例如浏览器标识、Cookies、Host等信息，这是请求的一部分，服务器会根据请求头内的信息判断请求是否合法，进而作出对应的响应。图中看到的Response Headers就是响应的一部分，例如其中包含了服务器的类型、文档类型、日期等信息，浏览器接受到响应后，会解析响应内容，进而呈现网页内容。 请求请求，由客户端向服务端发出，可以分为4部分内容：请求方法（Request Method、请求的网址（Request URL）、请求头（Request Headers）、请求体（Request Body）。 请求方法常见的请求方法有两种：GET和POST。 在浏览器中直接输入URL并回车，这便发起了一个GET请求，请求的参数会直接包含到URL里。例如，在百度中搜索Python，这就是一个GET请求，链接为，其中URL中包 含了请求的参数信息，这里参数wd表示要搜寻的关键字。POST请求大多在表单提交时发起。比如，对于一个登录表单，输入用户名和密码后，点击“登录”按钮，这通常会发起一个POST请求，其数据通常 以表单的形式传输，而不会体现在URL中。 GET和POST请求方法有如下区别。 GET请求中的参数包含在URL里面，数据可以在URL中看到，而POST请求的URL不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。 GET请求提交的数据最多只有1024字节，而POST请求没有限制。 一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用GET方式请求的话，密码就会暴露在URL里面，造成密码泄露，所以这里最好以POST方式发送。上传文件时，由于文件内容比较大，也会选用POST方式。 我们平常遇到的绝大部分请求都是GET或POST请求，另外还有一些请求方法，如HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE等，我们简单将其总结为下表。 方法 描述 GET 请求页面，并返回页面内容 HEAD 类似于GET请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 大多用于表单提交或上传文件，数据包含在请求体中 PUT 从客户端向服务器传送的数据取代指定文档中的内容 DELETE 请求服务器删除指定的页面 CONNECT 把服务器当作跳板，让服务器代替客户端访问其他网页 OPTIONS 允许客户端查看服务器的性能 TRACE 回显服务器收到的请求，主要用于测试或诊断 table { margin: auto; font-size: 50%; } 请求的网址，即统一资源定位符 URL，它可以唯一确定我们想请求的资源。 请求头请求头，用来说明服务器要使用的附加信息，比较重要的信息有Cookie、Referer、User-Agent等。下面简要说明一些常用的头信息。 Accept：请求报头域，用于指定客户端可接受哪些类型的信息。 Accept-Language：指定客户端可接受的语言类型。 Accept-Encoding：指定客户端可接受的内容编码。 Host：用于指定请求资源的主机IP和端口号，其内容为请求URL的原始服务器或网关的位置。从 HTTP 1.1 版本开始，请求必须包含此内容。 Cookie：也常用复数形式Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会 话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是Cookies的功劳。Cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页 面时，都会在请求头中加上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。 Referer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、防盗链处理等。 User-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫。 Content-Type：也叫互联网媒体类型（Internet Media Type）或者MIME类型，在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html代表HTML格式，image/gif代表GIF图片，application/json代表JSON类型，更多对应关系可以查看此对照表：。 因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。 请求体请请求体一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。 登录之前，我们填写了用户名和密码信息，提交时这些内容就会以表单数据的形式提交给服务器，此时需要注意Request Headers中指定Content-Type为application/x-www-form-urlencoded。只有设置Content-Type为application/x-www-form-urlencoded，才会以表单数据的形式提交。另外，我们也可以将Content-Type设置为application/json来提交JSON数据，或者设置为multipart/form-data来上传文件。 表格中列出了Content-Type和POST提交数据方式的关系。 Content-Type 提交数据的方式 application/x-www-form-urlencodeed 表单数据 multipart/form-data 表单文件上传 application/json 序列化JSON数据 text/xml XML数据 在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type，不然可能会导致POST提交后无法正常响应。 响应响应，由服务端返回给客户端，可以分为三部分：响应状态码（Response Status Code）、响应头（Response Headers）和响应体（Response Body）。 响应状态码响应状态码表示服务器的响应状态，如200代表服务器正常响应，404代表页面未找到，500代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为200，则证明成功返 回数据，再进行进一步的处理，否则直接忽略。下表列出了常见的错误代码及错误原因。 状态码 说明 详情 100 继续 请求者应当继续提出请求，服务器已收到请求的一部分，正在等待其余部分 101 切换协议 请求者已要求服务器切换协议，服务器已确认并确认切换 200 成功 服务器已成功处理了请求 201 已创建 请求成功并且服务器创建了新的资源 202 已接受 服务器已接受请求，但尚未处理 203 非授权信息 服务器已经成功处理请求，但返回的信息可能来自另一个源 204 无内容 服务器成功处理了请求，但没有返回任何内容 205 重置内容 服务器成功处理了请求，但内容被重置 206 部分内容 服务器成功处理了部分请求 300 多种选择 针对请求，服务器可执行多种操作 301 永久移动 请求的网页已永久移动到新位置，即永久重定向 302 临时移动 请求的网页暂时跳转到其他页面，即暂时重定向 303 查看其他位置 如果原来的请求是POST，重定向目标文档应当通过GET方式访问资源 304 未修改 此次请求返回的网页未修改，继续使用上次的资源 305 使用代理 请求者应该使用代理访问该网页 307 临时重定向 请求的资源临时从其他位置响应 400 错误请求 服务器无法解析该请求 401 未授权 请求没有进行身份验证或验证未通过 403 禁止访问 服务器拒绝此请求 404 未找到 服务器找不到请求的网页 405 方法禁用 服务器禁用了请求中指定的方法 406 不接受 无法使用请求的内容响应请求的网页 407 需要代理授权 请求者需要使用代理授权 408 请求超时 服务器请求超时 409 冲突 服务器在完成请求时发生冲突 410 已删除 请求的资源已永久删除 411 需要有效长度 服务器不接受不含有效内容长度标头字段的请求 412 未满足前提条件 服务器未满足请求者在请求中设置的其中一个前提条件 413 请求实体过大 请求实体过大，超出服务器的处理能力 415 请求URL过长 请求的网址过长，服务器无法处理 416 请求范围不符 页面无法提供请求页面支持 417 未满足期望值 服务器为满足期望请求标头字段的要求 500 服务器内部错误 服务器遇到错误，无法完成请求 501 未实现 服务器不具备完成请求的功能 502 错误网关 服务器作为网关或代理，从上游服务器收到无效响应 503 服务不可用 服务器目前无法使用 504 网关超时 服务器作为网关或代理，但是没有及时从上游服务器收到响应 505 HTTP版本不支持 服务器不支持请求中所用的HTTP协议版本 响应头包含了服务器对请求的应答信息，如Content-Type、Server、Set-Cookie等。下面简要说明一些常用的响应头信息。 Date：标识响应产生的时间。 Last-Modified：指定资源的最后修改时间。 Content-Encoding：指定响应内容的编码。 Server：包含服务器的信息，比如名称、版本号等。 Content-Type：文档类型，指定返回的数据类型是什么，如text/html代表返回HTML文档，application/x-javascript则代表返回JavaScript文件，image/jpeg则代表返回图片。 Set-Cookie：设置Cookies。响应头中的Set-Cookie告诉浏览器需要将此内容放在Cookies中，下次请求携带Cookies请求。 Expires：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。 响应体最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体，如图所示。 在浏览器开发者工具中点击Response，就可以看到网页的源代码，也就是响应体的内容，它是解析的目标。在做爬虫时，我们主要通过响应体得到网页的源代码、JSON数据等，然后从中做相应内容的提取。"},{"title":"Git常用命令","date":"2021-05-08T10:03:19.000Z","url":"/2021/05/08/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":[["Git","/tags/Git/"]],"categories":[["Git","/categories/Git/"]],"content":"Git常用命令仓库在当前目录新建一个Git代码库 新建一个目录，将其初始化为Git代码库 下载一个项目和它的整个代码历史 配置显示当前的Git配置 编辑Git配置文件 设置提交代码时的用户信息 增加/删除文件添加指定文件到暂存区 添加指定目录到暂存区，包括子目录 添加当前目录的所有文件到暂存区 添加每个变化前，都会要求确认对于同一个文件的多处变化，可以实现分次提交 停止追踪指定文件，但该文件会保留在工作区 改名文件，并且将这个改名放入暂存区 代码提交提交暂存区到仓库区 提交暂存区的指定文件到仓库区 提交工作区自上次commit之后的变化，直接到仓库区 提交时显示所有diff信息 使用一次新的commit，替代上一次提交,如果代码没有任何新变化，则用来改写上一次commit的提交信息 重做上一次commit，并包括指定文件的新变化 分支列出所有本地分支 列出所有远程分支 列出所有本地分支和远程分支 新建一个分支，但依然停留在当前分支 新建一个分支，并切换到该分支 新建一个分支，指向指定commit 新建一个分支，与指定的远程分支建立追踪关系 切换到指定分支，并更新工作区 切换到上一个分支 建立追踪关系，在现有分支与指定的远程分支之间 合并指定分支到当前分支 选择一个commit，合并进当前分支 删除分支 删除远程分支 标签列出所有tag 新建一个tag在当前commit 新建一个tag在指定commit 删除本地tag 删除远程tag 查看tag信息 提交指定tag 提交所有tag 新建一个分支，指向某个tag 查看信息显示有变更的文件 显示当前分支的版本历史 显示commit历史，以及每次commit发生变更的文件 搜索提交历史，根据关键词 显示某个commit之后的所有变动，每个commit占据一行 显示某个commit之后的所有变动，其”提交说明”必须符合搜索条件 显示某个文件的版本历史，包括文件改名 显示指定文件相关的每一次diff 显示过去5次提交 显示所有提交过的用户，按提交次数排序 显示指定文件是什么人在什么时间修改过 显示暂存区和工作区的差异 显示暂存区和上一个commit的差异 显示工作区与当前分支最新commit之间的差异 显示两次提交之间的差异 显示今天你写了多少行代码 显示某次提交的元数据和内容变化 显示某次提交发生变化的文件 显示某次提交时，某个文件的内容 显示当前分支的最近几次提交 远程同步下载远程仓库的所有变动 显示所有远程仓库 显示某个远程仓库的信息 增加一个新的远程仓库，并命名 取回远程仓库的变化，并与本地分支合并 上传本地指定分支到远程仓库 强行推送当前分支到远程仓库，即使有冲突 推送所有分支到远程仓库 撤销恢复暂存区的指定文件到工作区 恢复某个commit的指定文件到暂存区和工作区 恢复暂存区的所有文件到工作区 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 重置暂存区与工作区，与上一次commit保持一致 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 重置当前HEAD为指定commit，但保持暂存区和工作区不变 新建一个commit，用来撤销指定commit，后者的所有变化都将被前者抵消，并且应用到当前分支 暂时将未提交的变化移除，稍后再移入 其他生成一个可供发布的压缩包 "},{"title":"Hexo搭建个人博客","date":"2021-05-08T07:39:22.000Z","url":"/2021/05/08/Hexo%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"HexoHexo是一个快速、简洁且高效的博客框架。 安装git安装 安装 Node.jsNode.js 为大多数平台提供了官方的 安装程序。对于中国大陆地区用户，可以前往 淘宝 Node.js 镜像 下载。 其它的安装方法： 安装 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 安装完成后，win+R输入cmd调出命令行，输入hexo提示如下，说明安装正确。 建站安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 新建完成后，指定文件夹的目录如下： _config.yml网站的 配置 信息，您可以在此配置大部分的参数。 package.json应用程序的信息。EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。 scaffolds模版文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。 Hexo的模板是指在新建的文章文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。 source资源文件夹是存放用户资源的地方。除 _posts文件夹之外，开头命名为 _ (下划线)的文件/文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public文件夹，而其他文件会被拷贝过去。 themes主题文件夹。Hexo 会根据主题来生成静态页面。 配置相关配置可直接访问官方文档查看，我们先从使用别人的主题开始，官方提供了335个主题下载使用，你也可以根据规范制定自己的主题。 主题创建Hexo主题非常容易，您只要在themes文件夹内，新增一个任意名称的文件夹，并修改_config.yml内的theme设定，即可切换主题。一个主题可能会有以下的结构： _config.yml主题的配置文件。和 Hexo 配置文件不同，主题配置文件修改时会自动更新，无需重启Hexo Server。 获取主题选择相应的主题，从github上获取到themes目录下。 修改主目录下_config.yml中的配置文件，将theme修改为获取主题的文件夹名。 运行在主目录下调用cmd命令hexo server运行服务，访问进入博客。 添加文章"}]