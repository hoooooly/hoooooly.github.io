[{"title":"requests库的基本使用","date":"2021-05-18T07:44:10.000Z","url":"/2021/05/18/requests%E5%BA%93%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":[["python","/tags/python/"],["requests","/tags/requests/"],["爬虫","/tags/%E7%88%AC%E8%99%AB/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"安装requests是一个第三方库，使用pip下载安装。 实例引入用Python写爬虫的第一步就是模拟发起一个请求，把网页的源代码获取下来。 在浏览器中输入一个URL并回车，实际上就是让浏览器帮我们发起一个GET类型的HTTP请求，浏览器得到源代码后，把它渲染出来就可以看到网页内容了。 那如果想用requests来获取源代码，应该怎么办呢？很简单，requests这个库提供了一个get方法，调用这个方法，并传入对应的URL就能得到网页的源代码。 比如这里有一个示例网站:，其内容如下： 这个网站展示了一些电影数据，如果想要把这个网页里面的数据爬下来，比如获取各个电影的名称、上映时间等信息，然后把它存下来的话，该怎么做呢？ 第一步当然就是获取它的网页源代码了。 可以用requests这个库轻松地完成这个过程，代码的写法是这样的： 输出结果如下： 由于网页内容比较多，这里省略了大部分内容。 不过看运行结果，我们已经成功获取网页的HTML源代码，里面包含了电影的标题、类型、上映时间，等等。 把网页源代码获取下来之后，下一步我们把想要的数据提取出来，数据的爬取就完成了。 请求HTTP中最常见的请求之一就是GET请求。 GET请求换一个示例网站，其URL为，如果客户端发起的是GET请求的话，该网站会判断并返回相应的请求信息，包括 Headers、IP等。 我们还是用相同的方法来发起一个GET请求，代码如下： 返回结果： 可以发现，成功发起了GET请求，也通过这个网站的返回结果得到了请求所携带的信息，包括Headers、URL、IP，等等。 对于GET请求，我们知道URL后面是可以跟上一些参数的，如果我们现在想添加两个参数，其中name是germey，age是25，URL就可以写成如下内容： 要构造这个请求链接，是不是要直接写成这样呢？ 这样也可以，但如果这些参数还需要手动拼接，未免有点不人性化。 一般情况下，这种信息我们利用params这个参数就可以直接传递了，示例如下： 返回结果： 把URL参数通过字典的形式传给get方法的params参数，通过返回信息可以判断，请求的链接自动被构造成了：。 网页的返回类型实际上是str类型，但是它很特殊，是JSON格式的。所以，如果想直接解析返回结果，得到一个JSON格式的数据的话，可以直接调用json方法。 示例如下： 结果如下： 调用json方法，就可以将返回结果是JSON格式的字符串转化为字典。 但需要注意的是，如果返回结果不是JSON格式，便会出现解析错误，抛出json.decoder.JSONDecodeError异常。 抓取网页上面的请求链接返回的是JSON形式的字符串，那么如果请求普通的网页，则肯定能获得相应的内容。下面以本课时最初的实例页面为例，我们再加上一点提取信息的逻辑，将代码完善成如下的样子： 运行结果： 抓取二进制数据抓取的是网站的一个页面，实际上它返回的是一个HTML文档。如果想抓取图片、音频、视频等文件，应该怎么办呢？ 图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以，想要抓取它们，就要拿到它们的二进制数据。 下面以 GitHub 的站点图标为例来看一下： 这里抓取的内容是站点图标，也就是在浏览器每一个标签上显示的小图标。 前者出现了乱码，后者结果前带有一个b，这代表是bytes类型的数据。 由于图片是二进制数据，所以前者在打印时转化为str类型，也就是图片直接转化为字符串，这当然会出现乱码。 上面返回的结果我们并不能看懂，它实际上是图片的二进制数据，没关系，将刚才提取到的信息保存下来就好了，代码如下： 这里用了open方法，它的第一个参数是文件名称，第二个参数代表以二进制的形式打开，可以向文件里写入二进制数据。 运行结束之后，可以发现在文件夹中出现了名为baidu.png的图标。 "},{"title":"Nginx配置详解","date":"2021-05-18T03:16:57.000Z","url":"/2021/05/18/Nginx%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/","tags":[["Nginx","/tags/Nginx/"],["负载均衡","/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"],["http","/tags/http/"]],"categories":[["Nginx","/categories/Nginx/"]],"content":"简介Nginx是lgor Sysoev为俄罗斯访问量第二的rambler.ru站点设计开发的。从2004年发布至今，凭借开源的力量，已经接近成熟与完善。 Nginx功能丰富，可作为HTTP服务器，也可作为反向代理服务器，邮件服务器。支持FastCGI、SSL、Virtual Host、URL Rewrite、Gzip等功能。并且支持很多第三方的模块扩展。 Nginx的稳定性、功能集、示例配置文件和低系统资源的消耗让他后来居上，在全球活跃的网站中有12.18%的使用比率，大约为2220万个网站。 Nginx常用功能1、Http代理，反向代理：作为web服务器最常用的功能之一，尤其是反向代理正向代理和反向代理： Nginx在做反向代理时，提供性能稳定，并且能够提供配置灵活的转发功能。Nginx可以根据不同的正则匹配，采取不同的转发策略，比如图片文件结尾的文件服务器，动态页面web服务器，只要你正则写的没问题，又有相对应的服务器解决方案，你就可以随心所欲的玩。并且Nginx对返回结果进行错误页跳转，异常判断等。如果被分发的服务器存在异常，他可以将请求重新转发给另外一台服务器，然后自动去除异常服务器。 2、负载均衡Nginx提供的负载均衡策略有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash。扩展策略，就天马行空，只有你想不到的没有他做不到的啦，你可以参照所有的负载均衡算法，给他一一找出来做下实现。 上3个图，理解这三种负载均衡算法的实现 Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 3、web缓存Nginx可以对不同的文件做不同的缓存处理，配置灵活，并且支持FastCGI_Cache，主要用于对FastCGI的动态程序进行缓存。配合着第三方的ngx_cache_purge，对制定的URL缓存内容可以的进行增删管理。 4、Nginx相关地址源码： 官网： Nginx配置文件结构如果你下载好啦，你的安装文件，不妨打开conf文件夹的nginx.conf文件，Nginx服务器的基础配置，默认的配置也存放在此。 在nginx.conf的注释符号为：# 默认的nginx配置文件nginx.conf内容如下： nginx 文件结构 1、全局块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。 2、events块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。 3、http块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。 4、server块：配置虚拟主机的相关参数，一个http中可以有多个server。 5、location块：配置请求的路由，以及各种页面的处理情况。 下面给大家上一个配置文件，作为理解。 上面是nginx的基本配置，需要注意的有以下几点： 1、几个常见配置项 2、惊群现象：一个网路连接到来，多个睡眠的进程被同时叫醒，但只有一个进程能获得链接，这样会影响系统性能。 3、每个指令必须有分号结束。 原文地址： "},{"title":"python多进程基本原理","date":"2021-05-17T12:04:49.000Z","url":"/2021/05/17/python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["python","/tags/python/"],["multiprocessing","/tags/multiprocessing/"]],"categories":[["python","/categories/python/"]],"content":"多进程的含义多进程（Process）是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是系统进行资源分配和调度的一个独立单位。 顾名思义，多进程就是启用多个进程同时运行。由于进程是线程的集合，而且进程是由一个或多个线程构成的，所以多进程的运 行意味着有大于或等于进程数量的线程在运行。 Python多进程的优势由于进程中GIL的存在，Python中的多线程并不能很好地发挥多核优势，一个进程中的多个线程，在同一时刻只能有一个线程运行。 对于多进程来说，每个进程都有属于自己的GIL，所以，在多核处理器下，多进程的运行是不会受GIL的影响的。因此，多进程能更好地发挥多核的优势。 当然，对于爬虫这种IO密集型任务来说，多线程和多进程影响差别并不大。对于计算密集型任务来说，Python的多进程相比多线程，其多核运行效率会有成倍的提升。 总的来说，Python的多进程整体来看是比多线程更有优势的。所以，在条件允许的情况下，能用多进程就尽量用多进程。 不过值得注意的是，由于进程是系统进行资源分配和调度的一个独立单位，所以各个进程之间的数据是无法共享的，如多个进程无法共享一个全局变量，进程之间的数据共享需要有单独的机制来实现到。 多进程的实现在Python中也有内置的库来实现多进程，它就是multiprocessing。 multiprocessing提供了一系列的组件，如Process（进程、Queue（队列）、Semaphore（信号量）、Pipe（管道）、Lock（锁）、Pool（进程池）等，接下来让我们来了解下它们的使用方法。 直接使用Process类在multiprocessing中，每一个进程都用一个Process类来表示。它的API调用如下： target表示调用对象，你可以传入方法的名字。 args表示被调用对象的位置参数元组，比如target是函数func，他有两个参数m，n，那么args就传入[m, n]即可。 kwargs表示调用对象的字典。 name是别名，相当于给这个进程取一个名字。 group分组。 先用一个实例来感受一下： 这是一个实现多进程最基础的方式：通过创建Process来新建一个子进程，其中target参数传入方法名，args是方法的参数，是以 元组的形式传入，其和被调用的方法process的参数是一一对应的。 注意：这里args 必须要是一个元组，如果只有一个参数，那也要在元组第一个元素后面加一个逗号，如果没有逗号则和单个元素本身没有区别，无法构成元组，导致参数传递出现问题。 创建完进程之后，我们通过调用start方法即可启动进程了。运行结果如下： 运行了5个子进程，每个进程都调用了process方法。process方法的index参数通过Process的args传入，分别是0~4这5个序号，最后打印出来，5个子进程运行结束。 由于进程是Python中最小的资源分配单元，因此这些进程和线程不同，各个进程之间的数据是不会共享的，每启动一个进程，都会独立分配资源。 在当前CPU核数足够的情况下，这些不同的进程会分配给不同的CPU核来运行，实现真正的并行执行。 运行结果如下： 通过cpu_count成功获取了CPU核心的数量：4个，不同的机器结果可能不同。通过 active_children获取到了当前正在活跃运行的进程列表。然后遍历每个进程，并将它们的名称和进程号打印出来了，这里进程号直接使用pid属性即可获取，进程名称直接通过name属性即可获取。 继承继Process类在上面的例子中，创建进程是直接使用Process这个类来创建的，这是一种创建进程的方式。不过，创建进程的方式不止这一 种，同样，也可以像线程Thread一样来通过继承的方式创建一个进程类，进程的基本操作我们在子类的run方法中实现即可。 通过一个实例来看一下： 声明了一个构造方法，这个方法接收一个loop参数，代表循环次数，并将其设置为全局变量。在run方法中，又使用这个loop变量循环了loop次并打印了当前的进程号和循环次数。 在调用时，用range方法得到了2、3、4三个数字，并把它们分别初始化了MyProcess进程，然后调用start方法将进程启动起来。 注意：这里进程的执行逻辑需要在run方法中实现，启动进程需要调用start方法，调用之后run方法便会执行。 运行结果如下： 三个进程分别打印出了2、3、4条结果，即进程13560打印了2次结果，进程9908 打印了3次结果，进程13728打印了4次结果。 注意，这里的进程pid代表进程号，不同机器、不同时刻运行结果可能不同。 通过上面的方式，非常方便地实现了一个进程的定义。为了复用方便，可以把一些方法写在每个进程类里封装好，在使用时直接初始化一个进程类运行即可。 守护进程在多进程中，同样存在守护进程的概念，如果一个进程被设置为守护进程，当父进程结束后，子进程会自动被终止，我们可以通过设置daemon属性来控制是否为守护进程。 还是原来的例子，增加了deamon属性的设置： 运行结果如下： 结果很简单，因为主进程没有做任何事情，直接输出一句话结束，所以在这时也直接终止了子进程的运行。 这样可以有效防止无控制地生成子进程。这样的写法可以让我们在主进程运行结束后无需额外担心子进程是否关闭，避免了独立子进程的运行。 进程等待上面的运行效果其实不太符合我们预期：主进程运行结束时，子进程（守护进程）也都退出了，子进程什么都没来得及执行。 能不能让所有子进程都执行完了然后再结束呢？当然是可以的，只需要加入join方法即可，可以将代码改写如下： 运行结果如下： 在调用start和join方法后，父进程就可以等待所有子进程都执行完毕后，再打印出结束的结果。 默认情况下，join是无限期的。也就是说，如果有子进程没有运行完毕，主进程会一直等待。这种情况下，如果子进程出现问题陷入了死循环，主进程也会无限等待下去。怎么解决这个问题呢？可以给join方法传递一个超时参数，代表最长等待秒数。如果子进程没有在这个指定秒数之内完成，会被强制返回，主进程不再会等待。也就是说这个参数设置了主进程等待该子进程的最长时间。 例如这里传入1，代表最长等待1秒，代码改写如下： 运行结果如下： 可以看到，有的子进程本来要运行3秒，结果运行1秒就被强制返回了，由于是守护进程，该子进程被终止了。 终止进程终止进程不止有守护进程这一种做法，我们也可以通过terminate方法来终止某个子进程，另外我们还可以通过is_alive方法判断进程是否还在运行。 下面看一个实例： 用Process创建了一个进程，接着调用start方法启动这个进程，然后调用terminate方法将进程终止，最后调用join方法。 另外，在进程运行不同的阶段，通过is_alive方法判断当前进程是否还在运行。 运行结果如下： 这里有一个值得注意的地方，在调用terminate方法之后，我们用is_alive方法获取进程的状态发现依然还是运行状态。在调用join方法之后，is_alive方法获取进程的运行状态才变为终止状态。 所以，在调用terminate方法之后，记得要调用一下join方法，这里调用join方法可以为进程提供时间来更新对象状态，用来反映出最终的进程终止效果。 进程互斥锁 在访问一些临界区资源时，使用Lock可以有效避免进程同时占用资源而导致的一些问题。 信号量｀multiprocessing库中的Semaphore`来实现信号量，实现多个进程共享资源，同时限制可访问的进程数量。 道管管道（Pipe）用来实现进程之间的通讯，管道可以是单向的，即half-duplex：一个进程负责发消息，另一个进程负责收消息；也可以是双向的duplex，即互相收发消息。 默认声明Pipe对象是双向管道，如果要创建单向管道，可以在初始化的时候传入deplex参数为False。 声明了一个默认为双向的管道，然后将管道的两端分别传给两个进程。两个进程互相收发。观察一下结果： 管道Pipe就像进程之间搭建的桥梁，利用它可以很方便地实现进程间通信。 进程池 假如现在我们遇到这么一个问题，我有10000个任务，每个任务需要启动一个进程来执行，并且一个进程运行完毕之后要紧接着 启动下一个进程，同时我还需要控制进程的并发数量，不能并发太高，不然CPU处理不过来（如果同时运行的进程能维持在一个 最高恒定值当然利用率是最高的）。 那么我们该如何来实现这个需求呢？ 用Process和Semaphore可以实现，但是实现起来比较烦琐。这种需求在平时又是非常常见的。此时，我们就可以派上进程池了，即multiprocessing中的Pool。 Pool可以提供指定数量的进程，供用户调用，当有新的请求提交到pool中时，如果池还没有满，就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行它。 用一个实例来实现一下，代码如下： 声明了一个大小为3的进程池，通过processes参数来指定，如果不指定，那么会自动根据处理器内核来分配进程数。接着我们使用apply_async方法将进程添加进去，args可以用来传递参数。 运行结果如下： 进程池大小为3，可以看到有3个进程同时执行，第4个进程在等待，在有进程运行完毕之后，第4个进程马上跟着运行，出现了如上的运行效果。 最后，我们要记得调用close方法来关闭进程池，使其不再接受新的任务，然后调用join方法让主进程等待子进程的退出，等子进程运行完毕之后，主进程接着运行并结束。 上面的写法多少有些烦琐，使用你进程池的map方法，可以将上述写法简化很多。 map方法是怎么用的呢？第一个参数就是要启动的进程对应的执行方法，第2个参数是一个可迭代对象，其中的每个元素会被传递给这个执行方法。 举个例子：现在有一个list，里面包含了很多URL，定义了一个方法用来抓取每个URL内容并解析，那么可以直接在map的第一个参数传入方法名，第2个参数传入URL数组。 用一个实例来感受一下： 运行结果： 这样，就可以实现3个进程并行运行。不同的进程相互独立地输出了对应的爬取结果。"},{"title":"python多线程基本原理","date":"2021-05-12T03:42:12.420Z","url":"/2021/05/12/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["python","/tags/python/"],["多线程","/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"],["threading","/tags/threading/"]],"categories":[["Python","/categories/Python/"]],"content":"多线程的含义说起多线程，就不得不先说什么是线程。然而想要弄明白什么是线程，又不得不先说什么是进程。 进程可以理解为是一个可以独立运行的程序单位，比如打开一个浏览器，这就开启了一个浏览器进程；打开一个文本编辑器，这就开启了一个文本编辑器进程。但一个进程中是可以同时处理很多事情的，比如在浏览器中，可以在多个选项卡中打开多个页面，有的页面在播放音乐，有的页面在播放视频，有的网页在播放动画，它们可以同时运行，互不干扰。为什么能同时做到同时运行这么多的任务呢？这里就需要引出线程的概念了，其实这一个个任务，实际上就对应着一个个线程的执行。 而进程呢？它就是线程的集合，进程就是由一个或多个线程构成的，线程是操作系统进行运算调度的最小单位，是进程中的一个最小运行单元。比如上面所说的浏览器进程，其中的播放音乐就是一个线程，播放视频也是一个线程，当然其中还有很多其他的线程在同时运行，这些线程的并发或并行执行最后使得整个浏览器可以同时运行这么多的任务。 了解了线程的概念，多线程就很容易理解了，多线程就是一个进程中同时执行多个线程，前面所说的浏览器的情景就是典型的多线程执行。 并发和并行并说到多进程和多线程，这里就需要再讲解两个概念，那就是并发和并行。知道，一个程序在计算机中运行，其底层是处理器通过运行一条条的指令来实现的。 并发，英文叫作concurrency。它是指同一时刻只能有一条指令执行，但是多个线程的对应的指令被快速轮换地执行。比如一个处理器，它先执行线程A的指令一段时间，再执行线程B的指令一段时间，再切回到线程A执行一段时间。 由于处理器执行指令的速度和切换的速度非常非常快，人完全感知不到计算机在这个过程中有多个线程切换上下文执行的操作，这就使得宏观上看起来多个线程在同时运行。但微观上只是这个处理器在连续不断地在多个线程之间切换和执行，每个线程的执行一定会占用这个处理器一个时间片段，同一时刻，其实只有一个线程在执行。 并行，英文叫作parallel。它是指同一时刻，有多条指令在多个处理器上同时执行，并行必须要依赖于多个处理器。不论是从宏观上还是微观上，多个线程都是在同一时刻一起执行的。 并行只能在多处理器系统中存在，如果的计算机处理器只有一个核，那就不可能实现并行。而并发在单处理器和多处理器系统中都是可以存在的，因为仅靠一个核，就可以实现并发。 举个例子，比如系统处理器需要同时运行多个线程。如果系统处理器只有一个核，那它只能通过并发的方式来运行这些线程。如果系统处理器有多个核，当一个核在执行一个线程时，另一个核可以执行另一个线程，这样这两个线程就实现了并行执行，当然其他的线程也可能和另外的线程处在同一个核上执行，它们之间就是并发执行。具体的执行方式，就取决于操作系统的调度了。 多线程适用场景多在一个程序进程中，有一些操作是比较耗时或者需要等待的，比如等待数据库的查询结果的返回，等待网页结果的响应。如果使用单线程，处理器必须要等到这些操作完成之后才能继续往下执行其他操作，而这个线程在等待的过程中，处理器明显是可以来执行其他的操作的。如果使用多线程，处理器就可以在某个线程等待的时候，去执行其他的线程，从而从整体上提高执行效率。 像上述场景，线程在执行过程中很多情况下是需要等待的。比如网络爬虫就是一个非常典型的例子，爬虫在向服务器发起请求之后，有一段时间必须要等待服务器的响应返回，这种任务就属于IO密集型任务。对于这种任务，如果启用多线程，处理器就可以在某个线程等待的过程中去处理其他的任务，从而提高整体的爬取效率。 但并不是所有的任务都是IO密集型任务，还有一种任务叫作计算密集型任务，也可以称之为CPU密集型任务。顾名思义，就是任务的运行一直需要处理器的参与。此时如果开启了多线程，一个处理器从一个计算密集型任务切换到切换到另一个计算密集型任务上去，处理器依然不会停下来，始终会忙于计算，这样并不会节省总体的时间，因为需要处理的任务的计算总量是不变的。如果线程数目过多，反而还会在线程切换的过程中多耗费一些时间，整体效率会变低。 所以，如果任务不全是计算密集型任务，可以使用多线程来提高程序整体的执行效率。尤其对于网络爬虫这种IO密集型任务来说，使用多线程会大大提高程序整体的爬取效率。 Python实现多线程实在Python中，实现多线程的模块叫作threading，是Python自带的模块。使用threading实现多线程的方法。 ###Thread直接创建子线程 首先，可以使用Thread类来创建一个线程，创建时需要指定target参数为运行的方法名称，如果被调用的方法需要传入额外的参数，则可以通过Thread的args参数来指定。示例如下： 运行结果如下： 在这里首先声明了一个方法，叫作target，它接收一个参数为second，通过方法的实现可以发现，这个方法其实就是执行了一个time.sleep休眠操作，second参数就是休眠秒数，其前后都print了一些内容，其中线程的名字通过threading.current_thread().name来获取出来，如果是主线程的话，其值就是MainThread，如果是子线程的话，其值就是Thread-*。 然后通过Thead类新建了两个线程，target参数就是刚才所定义的方法名，args以列表的形式传递。两次循环中，这里i分别就是1和5，这样两个线程就分别休眠1秒·和5秒，声明完成之后，调用start方法即可开始线程的运行。 观察结果可以发现，这里一共产生了三个线程，分别是主线程MainThread和两个子线程Thread-1、Thread-2。另外观察到，主线程首先运行结束，紧接着Thread-1、Thread-2才接连运行结束，分别间隔了1秒和4秒。这说明主线程并没有等待子线程运行完毕才结束运行，而是直接退出了，有点不符合常理。如果想要主线程等待子线程运行完毕之后才退出，可以让每个子线程对象都调用下join方法，实现如下： 运行结果如下： 主线程必须等待子线程都运行结束，主线程才继续运行并结束。 继承Thread类创建子线程另外，也可以通过继承Thread类的方式创建一个线程，该线程需要执行的方法写在类的run方法里面即可。上面的例子的等价改写为： 运行结果如下： 可以看到，两种实现方式，其运行效果是相同的。 守护进程在线程中有一个叫作守护线程的概念，如果一个线程被设置为守护线程，那么意味着这个线程是“不重要”的，这意味着，如果主线程结束了而该守护线程还没有运行完，那么它将会被强制结束。在Python中可以通过setDaemon方法来将某个线程设置为守护线程。 示例如下： 在这里通过 setDaemon方法将 t2 设置为了守护线程，这样主线程在运行完毕时，t2 线程会随着线程的结束而结束。 运行结果如下： 可以看到，没有Thread-2打印退出的消息，Thread-2随着主线程的退出而退出了。 这里并没有调用join方法，如果让t1和t2都调用join方法，主线程就会仍然等待各个子线程执行完毕再退出，不论其是否是守护线程。 互斥锁互在一个进程中的多个线程是共享资源的，比如在一个进程中，有一个全局变量count用来计数，声明多个线程，每个线程运行时都给count加1，代码实现如下： 在这里，声明了1000个线程，每个线程都是现取到当前的全局变量count值，然后休眠一小段时间，然后对count赋予新的值。 按照常理来说，最终的count值应该为1000。但其实不然。运行结果如下： 最后的结果居然只有15，而且多次运行或者换个环境运行结果是不同的.这是为什么呢？因为count这个值是共享的，每个线程都可以在执行temp = count这行代码时拿到当前count的值，但是这些线程中的一些线程可能是并发或者并行执行的，这就导致不同的线程拿到的可能是同一个count值，最后导致有些线程的count的加1操作并没有生效，导致最后的结果偏小。 所以，如果多个线程同时对某个数据进行读取或修改，就会出现不可预料的结果。为了避免这种情况，我们需要对多个线程进行同步，要实现同步，我们可以对需要操作的数据进行加锁保护，这里就需要用到threading.Lock了。 加锁保护是什么意思呢？就是说，某个线程在对数据进行操作前，需要先加锁，这样其他的线程发现被加锁了之后，就无法继续向下执行，会一直等待锁被释放，只有加锁的线程把锁释放了，其他的线程才能 继续加锁并对数据做修改，修改完了再释放锁。这样可以确保同一时间只有一个线程操作数据，多个线程不会再同时读取和修改同一个数据，这样最后的运行结果就是对的了。 将代码修改为如下内容： 运行结果如下： Python多线程的问题由于Python中GIL的限制，导致不论是在单核还是多核条件下，在同一时刻只能运行一个线程，导致Python多线程无法发挥多核并行的优势。 GIL全称为GlobalInterpreter Lock，中文翻译为全局解释器锁，其最初设计是出于数据安全而考虑的。 在Python多线程下，每个线程的执行方式如下： 获取GIL执行对应线程的代码 释放GIL可见，某个线程想要执行，必须先拿到 GIL，我们可以把 GIL看作是通行证，并且在一个 Python进程中，GIL只有一个。拿不到通行证的线程，就不允许执行。这样就会导致，即使是多核条件下，一个 Python 进程下的多个线程，同一时刻也只能执行一个线程。 不过对于爬虫这种 IO 密集型任务来说，这个问题影响并不大。而对于计算密集型任务来说，由于 GIL的存在，多线程总体的运行效率相比可能反而比单线程更低 "},{"title":"Session和Cookies","date":"2021-05-12T02:51:36.000Z","url":"/2021/05/12/Session%E5%92%8CCookies/","tags":[["http","/tags/http/"],["web","/tags/web/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"静态网页和动态网页示例代码： 这是最基本的HTML代码，保存为一个.html文件，然后把它放在某台具有固定公网IP的主机上，主机上装上Apache或Nginx等服务器，这样这台主机就可以作为服务器了，其他人便可以通过访问服务器看到这个页面，这就搭建了一个最简单的网站。 这种网页的内容是HTML代码编写的，文字、图片等内容均通过写好的HTML代码来指定，这种页面叫作静态网页。它加载速度快，编写简单，但是存在很大的缺陷，如可维护性差，不能根据URL灵活多变地显示内容等。例如，想要给这个网页的URL传入一个name参数，让其在网页中显示出来，是无法做到的。 因此，动态网页应运而生，它可以动态解析URL中参数的变化，关联数据库并动态呈现不同的页面内容，非常灵活多变。现在遇到的大多数网站都是动态网站，它们不再是一个简单的HTML，而是可能由JSP、PHP、Python等语言编写的，其功能比静态网页强大和丰富太多了。 此外，动态网站还可以实现用户登录和注册的功能。按照一般的逻辑来说，输入用户名和密码登录之后，肯定是拿到了一种类似凭证的东西，有了它，才能保持登录状态，才能访问登录之后才能看到的页面。 那么，这种神秘的凭证到底是什么呢？其实它就是 Session和Cookies 共同产生的结果。 无状态HTTP在了解Session和Cookies之前，还需要了解HTTP的一个特点，叫作无状态。 HTTP的无状态是指HTTP协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。 当向服务器发送请求后，服务器解析此请求，然后返回对应的响应，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录。 这意味着如果后续需要处理前面的信息，则必须重传，这也导致需要额外传递一些前面的重复请求，才能获取后续响应，这种效果显然不是想要的。为了保持前后状态，肯定不能将前面的请求全 部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。 这时两个用于保持HTTP连接状态的技术就出现了，它们分别是Session和Cookies。Session在服务端，也就是网站的服务器，用来保存用户的Session信息；Cookies在客户端，也可以理解为浏览器端，有了Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别Cookies并鉴定出是哪个用户，然后再判断用户是否是登录状态，进而返回对应的响应。 可以理解为Cookies里面保存了登录的凭证，有了它，只需要在下次请求携带Cookies发送请求而不必重新输入用户名、密码等信息重新登录了。 因此在爬虫中，有时候处理需要登录才能访问的页面时，一般会直接将登录成功后获取的Cookies放在请求头里面直接请求，而不必重新模拟登录。 CookiesCookies指某些网站为了辨别用户身份、进行Session跟踪而存储在用户本地终端上的数据。 SessionSession，中文称之为会话，其本身的含义是指有始有终的一系列动作/消息。比如，打电话时，从拿起电话拨号到挂断电话这中间的一系列过程可以称为一个 Session。 而在Web中，Session对象用来存储特定用户Session所需的属性及配置信息。这样，当用户在应用程序的Web页之间跳转时，存储在Session对象中的变量将不会丢失，而是在整个用户Session中一直存在下去。当用户请求来自应用程序的Web页时，如果该用户还没有Session，则Web服务器将自动创建一个 Session对象。当Session过期或被放弃后，服务器将终止该Session。 Session维持那么，怎样利用Cookies保持状态呢？当客户端第一次请求服务器时，服务器会返回一个响应头中带有Set-Cookie字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把Cookies保存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies放到请求头一起提交给服务器，Cookies携带了SessionID 信息，服务器检查该Cookies即可找到对应的Session是什么，然后再判断Session来以此来辨认用户状态。 在成功登录某个网站时，服务器会告诉客户端设置哪些Cookies信息，在后续访问页面时客户端会把Cookies发送给服务器，服务器再找到对应的Session加以判断。如果Session中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。 反之，如果传给服务器的Cookies是无效的，或者Session已经过期了，将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录。 所以，Cookies和Session需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录Session控制。 Cookies属性结构Cookie有如下几个属性。 Name，即该Cookie的名称。 Cookie一旦创建，名称便不可更改。 Value，即该Cookie的值。如果值为Unicode字符，需要为字符编码。如果值为二进制数据，则需要使用BASE64编码。 MaxAge，即该Cookie失效的时间，单位秒，也常和Expires一起使用，通过它可以计算出其有效时间。 MaxAge 如果为正数，则该Cookie在MaxAge秒之后失效。如果为负数，则关闭浏览器时Cookie即失效，浏览器也不会以任何形式保存该Cookie。 Path，即该Cookie的使用路径。如果设置为/path/，则只有路径为/path/的页面可以访问该Cookie。如果设置为/，则本域名下的所有页面都可以访问该Cookie。 Domain，即可以访问该Cookie的域名。例如如果设置为.zhihu.com，则所有以zhihu.com，结尾的域名都可以访问该Cookie。 Size字段，即此Cookie的大小。 Http字段，即Cookie的httponly属性。若此属性为true，则只有在HTTP Headers中会带有此Cookie的信息，而不能通过document.cookie来访问此Cookie。 Secure，即该Cookie是否仅被使用安全协议传输。安全协议。安全协议有HTTPS、SSL等，在网络上传输数据之前先将数据加密。默认为false。 会话会Cookie和持久和Cookie从表面意思来说，会话Cookie就是把Cookie放在浏览器内存里，浏览器在关闭之后该Cookie即失效；持久Cookie则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态。 严格来说，没有会话Cookie和持久Cookie之分，只是由Cookie的MaxAge或Expires字段决定了过期的时间。 因此，一些持久化登录的网站其实就是把Cookie的有效时间和Session有效期设置得比较长，下次再访问页面时仍然携带之前的Cookie，就可以直接保持登录状态。 常见误区在谈论Session机制的时候，常常听到这样一种误解 ——“只要关闭浏览器，Session就消失了”。可以想象一下会员卡的例子，除非顾客主动对店家提出销卡，否则店家绝对不会轻易删除顾客的资料。对Session来说，也是一样，除非程序通知服务器删除一个Session，否则服务器会一直保留。比如，程序一般都是在做注销操作时才去删除Session。 但是当关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭。之所以会有这种错觉，是因为大部分网站都使用会话Cookie来保存Session ID信息，而关闭浏览器后Cookies就消失了，再次连接服务器时，也就无法找到原来的Session了。如果服务器设置的Cookies保存到硬盘上，或者使用某种手段改写浏览器发出的HTTP请求头，把原来的Cookies发送给服务器，则再次打开浏览器，仍然能够找到原来的Session ID，依旧还是可以保持登录状态的。 而且恰恰是由于关闭浏览器不会导致Session被删除，这就需要服务器为Session设置一个失效时间，当距离客户端上一次使用Session的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把Session删除以节省存储空间。"},{"title":"爬虫的基本原理","date":"2021-05-11T16:12:50.000Z","url":"/2021/05/12/%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["http","/tags/http/"],["python","/tags/python/"],["爬虫","/tags/%E7%88%AC%E8%99%AB/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。如果把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。 爬虫概述简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，概要介绍一下。 获取网页获爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。 源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。 前面讲了请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个 请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样实现呢？ Python提供了许多库来帮助我们实现这个操作，如urllib、requests等。可以用这些库来帮助我们实现HTTP请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的Body部分，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。 提取信息提获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。 另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS选择器或XPath来提取网页信息的库，如Beautiful Soup、pyquery、lxml等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。 提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。 保存数据提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为TXT文本或JSON文本，也可以保存到数据库，如MySQL和MongoDB等，还可保存至远程服务器，如借助SFTP进行操作等。 自动化程序自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各 种异常处理、错误重试等操作，确保爬取持续高效地运行。 能抓怎样的数据能在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML代码，而最常抓取的便是HTML源代码。 另外，可能有些网页返回的不是HTML代码，而是一个JSON字符串（其中API接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。 此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。 另外，还可以看到各种扩展名的文件，如CSS、JavaScript和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。 上述内容其实都对应各自的URL，是基于HTTP或HTTPS协议的，只要是这种数据，爬虫都可以抓取。 JavaScript渲染页面有时候，在用urllib或requests抓取网页时，得到的源代码实际和浏览器中看到的不一样。 这是一个非常常见的问题。现在网页越来越多地采用Ajax、前端模块化工具来构建，整个网页可能都是由JavaScript渲染出来的，也就是说原始的HTML代码就是一个空壳，例如： body节点里面只有一个id为container的节点，但是需要注意在body节点后引入了app.js，它便负责整个网站的渲染。 在浏览器中打开这个页面时，首先会加载这个HTML内容，接着浏览器会发现其中引入了一个app.js文件，然后便会接着去请求这个文件，获取到该文件后，便会执行其中的JavaScript代码，而JavaScript则会改变HTML中的节点，向其添加内容，最后得到完整的页面。 但是在用urllib或requests等库请求当前页面时，我们得到的只是这个HTML代码，它不会帮助我们去继续加载这个JavaScript文件，这样也就看不到浏览器中的内容。 这也解释了为什么有时我们得到的源代码和浏览器中看到的不一样。 因此，使用基本HTTP请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，可以分析其后台Ajax接口，也可使用Selenium、Splash这样的库来实现模拟JavaScript渲染。"},{"title":"web网页基础","date":"2021-05-11T15:32:38.000Z","url":"/2021/05/11/web%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80/","tags":[["html","/tags/html/"],["css","/tags/css/"],["javascript","/tags/javascript/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"网页的组成首先，我们来了解网页的基本组成，网页可以分为三大部分：HTML、CSS和JavaScript。 如果把网页比作一个人的话，HTML相当于骨架，JavaScript相当于肌肉，CSS相当于皮肤，三者结合起来才能形成一个完整的网页。下面我们来分别介绍一下这三部分的功能。 HTMLHTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。 我们浏览的网页包括文字、按钮、图片和视频等各种复杂的元素，其基础架构就是HTML。不同类型的元素通过不同类型的标签来表示，如图片用img标签表示，视频用video标签表示，段落用p标签表示，它们之间的布局又常通过布局标签div嵌套组合而成，各种标签通过不同的排列和嵌套就可以形成网页的框架。 在Chrome浏览器中打开百度，右击并选择 “检查”项（或按F12键），打开开发者模式，这时在Elements选项卡中即可看到网页的源代码，如图所示。 这就是HTML，整个网页就是由各种标签嵌套组合而成的。这些标签定义的节点元素相互嵌套和组合形成了复杂的层次关系，就形成了网页的架构。 CSS虽然HTML定义了网页的结构，但是只有HTML页面的布局并不美观，可能只是简单的节点元素的排列，为了让网页看起来更好看一些，这里就需要借助CSS了。 CSS，全称叫作Cascading Style Sheets，即层叠样式表。“层叠”是指当在HTML中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式”指网页中文字大小、颜色、元素间距、排列等 格式。 CSS是目前唯一的网页页面排版样式标准，有了它的帮助，页面才会变得更为美观。例如： 这就是一个CSS样式。大括号前面是一个CSS选择器。此选择器的作用是首先选中id为head_wrapper且class 为s-ps-islite的节点，然后再选中其内部的class为s-p-top的节点。大括号内部写的就是一条条样式规则，例如position指定了这个元素的布局方式为绝对布局，bottom指定元素的下边距为40像素，width指定了宽度为100%占满父元素，height则指定了元素的高度。也就是说，我们将位置、宽度、高度等样式配置统一写成这样的形式，然后用大括号括起来，接着在开头再加上CSS选择器，这就代表这个样式对CSS选择器选中的元素生效，元素就会根据此样式来展示了。 在网页中，一般会统一定义整个网页的样式规则，并写入CSS文件中（其后缀为css）。在HTML中，只需要用link标签即可引入写好的CSS文件，这样整个页面就会变得美观、优雅。 JavaScriptJavaScript，简称JS，是一种脚本语言。HTML和CSS配合使用，提供给用户的只是一种静态信息，缺乏交互性。我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是JavaScript的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系，而是实现了一种实时、动态、交互的页面功能。 JavaScript通常也是以单独的文件形式加载的，后缀为js，在 HTML中通过script标签即可引入，例如: 综上所述，HTML定义了网页的内容和结构，CSS描述了网页的布局，JavaScript定义了网页的行为。"},{"title":"在Hexo博客中嵌入外链视频","date":"2021-05-11T01:12:11.000Z","url":"/2021/05/11/%E5%9C%A8Hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%B5%8C%E5%85%A5%E5%A4%96%E9%93%BE%E8%A7%86%E9%A2%91/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"嵌入的视频Hexo支持Youtube视频的嵌入，可以参考其实现方式。 首先，在node_modules/hexo/lib/plugins/tag/index.js中添加以下代码。 然后在node_modules/hexo/lib/plugins/tag/目录下新建bilibili.js文件，打开并添加如下代码： 重新启动下Hexo Server,在md页面中添加下列： 重新刷新页面，就可以看到视频正常加载了。 "},{"title":"腾讯云配置ssl证书","date":"2021-05-10T07:36:11.000Z","url":"/2021/05/10/%E8%85%BE%E8%AE%AF%E4%BA%91%E9%85%8D%E7%BD%AEssl%E8%AF%81%E4%B9%A6/","tags":[["nginx","/tags/nginx/"],["https","/tags/https/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"什么是SSL证书？SSL证书是用于在Web服务器与浏览器以及客户端之间建立加密链接的加密技术，通过配置和应用SSL证书来启用HTTPS协议，来保证互联网数据传输的安全，全球每天有数以亿计的网站都是通过HTTPS来确保数据安全，保护用户隐私。 申请腾讯云SSL证书百毒搜索腾讯SSL证书，找到免费使用SSL一年的产品，一系列骚操作后得到证书。 nginx配置修改下载SSL证书，上传到服务器/etc/pki/nginx/目录下。 修改/etc/配置文件，注意SSL证书的路径和实际上传的路径和名称一致。 "},{"title":"Hexo在腾讯云的部署","date":"2021-05-10T04:51:56.000Z","url":"/2021/05/10/Hexo%E5%9C%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E7%9A%84%E9%83%A8%E7%BD%B2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"简介Hexo在GitHub pages上的访问太慢了，迁移到腾讯云服务器上。 部署环境腾讯云服务器（Centos 64位）。 服务器配置安装git 创建git用户并修改权限 找到一下内容 在该语句下添加 退出（esc + :wq）并修改权限 本地使用gitbash创建密钥 在腾讯云中创建ssh，并将本地的id_rsa.pub中的文件内容全部复制到authorized_keys中。 修改权限 本地测试 云服务器中创建网站目录并设置权限 安装nginx 以上执行完之后，在浏览器中输入你的公网IP如果可以进入CentOs界面，说明Nginx安装成功。 配置nginx 重启服务 建立git仓库并修改权限 同步网站根目录 填入如下内容 修改权限 在本地Hexo目录下修改_config.yml文件中的deploy后的repo改为： 以上全部完成后，执行hexo的部署命令即可完成在腾讯云服务器上的博客部署。"},{"title":"HTTP基本原理","date":"2021-05-09T12:48:23.000Z","url":"/2021/05/09/HTTP%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["http","/tags/http/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"URI和URLURI （Uniform Resource Identifier） 即统一资源标志符。URL （Uniform Resource Locator 即统一资源定位符。 例如：既是一个URL，也是一个URI。用URL/URI来唯一指定了它的访问方式，这其中包括了访问协议Https，访问路径（即根目录）和资源名称favicon.ico。 URL是URI的一个子集，也就是每个URL都是URI，但不是每个URI都是URL。 URI还包括一个子类叫做URN（Universal Resource Name）即统一资源名称。但是在目前的互联网，URN的使用非常少，几乎所有的 URI都是URL，所以一般的网页链接我们可以称之为 URL，也可以称之为 URI。 超文本Hypertext，我们在浏览器里看到的网页就是超文本解析而成的，其网页源代码是一系列 HTML代码，里面包含了一系列标签，比如img显示图片，p指定显示段落等。浏览器解析这些标签后，便形成了我们平常看到的网页，而网页的源代码HTML就可以称作超文本。 HTTP和HTTPSHTTP的全称是Hyper Text Transfer Protocol，中文名叫作超文本传输协议，HTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。HTTP由万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet EngineeringTask Force）共同合作制定的规范，目前广泛使用的是HTTP 1.1版本。 HTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，简称为HTTPS。 HTTPS的安全基础是SSL，因此通过它传输的内容都是经过SSL加密的，它的主要作用可以分为两种： 建立一个信息安全通道，来保证数据传输的安全。 确认网站的真实性，凡是使用了HTTPS的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过CA机构颁发的安全签章来查询。 现在越来越多的网站和 App 都已经向 HTTPS 方向发展。例如： 苹果公司强制所有 iOS App 在 2017 年 1 月 1 日 前全部改为使用 HTTPS 加密，否则 App 就无法在应用商店上架。 谷歌从 2017 年 1 月推出的 Chrome 56 开始，对未进行 HTTPS 加密的网址链接亮出风险提示，即在地址栏的显著位置提醒用户 “此网页不安全”。 腾讯微信小程序的官方需求文档要求后台使用 HTTPS 请求进行网络通信，不满足条件的域名和协议无法请求。 因此，HTTPS 已经已经是大势所趋。 HTTP请求过程我们在浏览器中输入一个URL，回车之后便可以在浏览器中观察到页面内容。实际上，这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器。响应里包含了页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来，传输模型如图所示： 此处客户端即代表我们自己的 PC 或手机浏览器，服务器即要访问的网站所在的服务器。 为了更直观地说明这个过程，这里用浏览器的开发者模式下的Network监听组件来做演示，它可以显示访问当前请求网页时发生的所有网络请求和响应。打开浏览器（Chrome或Edge都可以），右击并选择 “检查”项，即可打开浏览器的开发者工具。这里访问百度，输入该 URL后回车，观察这个过程中发生了怎样的网络请求。可以看到，在Network页面下方 出现了一个个的条目，其中一个条目就代表一次发送请求和接收响应的过程，如图所示: 我们先观察第一个网络请求，即www.baidu.com，其中各列的含义如下。 第一列 Name：请求的名称，一般会将 URL的最后一部分内容当作名称。 第二列 Status：响应的状态码，这里显示为 200，代表响应是正常的。通过状态码，我们可以判断发送了请求之后是否得到了正常的响应。 第三列 Type：请求的文档类型。这里为document，代表我们这次请求的是一个 HTML文档，内容就是一些 HTML代码。 第四列 Initiator：请求源。用来标记请求是由哪个对象或进程发起的。 第五列 Size：从服务器下载的文件和请求的资源大小。如果是从缓存中取得的资源，则该列会显示 fromcache。 第六列 Time：发起请求到获取响应所用的总时间。 第七列 Waterfall：网络请求的可视化瀑布流。 我们点击这个条目即可看到其更详细的信息，如图所示。 首先是General部分，Request URL为请求的URL，Request Method为请求的方法，Status Code为响应状态码，Remote Address为远程服务器的地址和端口，Referrer Policy为 Referrer判别策略。 再继续往下，可以看到，有Response Headers和Request Headers，这分别代表响应头和请求头。请求头里带有许多请求信息，例如浏览器标识、Cookies、Host等信息，这是请求的一部分，服务器会根据请求头内的信息判断请求是否合法，进而作出对应的响应。图中看到的Response Headers就是响应的一部分，例如其中包含了服务器的类型、文档类型、日期等信息，浏览器接受到响应后，会解析响应内容，进而呈现网页内容。 请求请求，由客户端向服务端发出，可以分为4部分内容：请求方法（Request Method、请求的网址（Request URL）、请求头（Request Headers）、请求体（Request Body）。 请求方法常见的请求方法有两种：GET和POST。 在浏览器中直接输入URL并回车，这便发起了一个GET请求，请求的参数会直接包含到URL里。例如，在百度中搜索Python，这就是一个GET请求，链接为，其中URL中包 含了请求的参数信息，这里参数wd表示要搜寻的关键字。POST请求大多在表单提交时发起。比如，对于一个登录表单，输入用户名和密码后，点击“登录”按钮，这通常会发起一个POST请求，其数据通常 以表单的形式传输，而不会体现在URL中。 GET和POST请求方法有如下区别。 GET请求中的参数包含在URL里面，数据可以在URL中看到，而POST请求的URL不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。 GET请求提交的数据最多只有1024字节，而POST请求没有限制。 一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用GET方式请求的话，密码就会暴露在URL里面，造成密码泄露，所以这里最好以POST方式发送。上传文件时，由于文件内容比较大，也会选用POST方式。 我们平常遇到的绝大部分请求都是GET或POST请求，另外还有一些请求方法，如HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE等，我们简单将其总结为下表。 方法 描述 GET 请求页面，并返回页面内容 HEAD 类似于GET请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 大多用于表单提交或上传文件，数据包含在请求体中 PUT 从客户端向服务器传送的数据取代指定文档中的内容 DELETE 请求服务器删除指定的页面 CONNECT 把服务器当作跳板，让服务器代替客户端访问其他网页 OPTIONS 允许客户端查看服务器的性能 TRACE 回显服务器收到的请求，主要用于测试或诊断 table { margin: auto; font-size: 50%; } 请求的网址，即统一资源定位符 URL，它可以唯一确定我们想请求的资源。 请求头请求头，用来说明服务器要使用的附加信息，比较重要的信息有Cookie、Referer、User-Agent等。下面简要说明一些常用的头信息。 Accept：请求报头域，用于指定客户端可接受哪些类型的信息。 Accept-Language：指定客户端可接受的语言类型。 Accept-Encoding：指定客户端可接受的内容编码。 Host：用于指定请求资源的主机IP和端口号，其内容为请求URL的原始服务器或网关的位置。从 HTTP 1.1 版本开始，请求必须包含此内容。 Cookie：也常用复数形式Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会 话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是Cookies的功劳。Cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页 面时，都会在请求头中加上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。 Referer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、防盗链处理等。 User-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫。 Content-Type：也叫互联网媒体类型（Internet Media Type）或者MIME类型，在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html代表HTML格式，image/gif代表GIF图片，application/json代表JSON类型，更多对应关系可以查看此对照表：。 因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。 请求体请请求体一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。 登录之前，我们填写了用户名和密码信息，提交时这些内容就会以表单数据的形式提交给服务器，此时需要注意Request Headers中指定Content-Type为application/x-www-form-urlencoded。只有设置Content-Type为application/x-www-form-urlencoded，才会以表单数据的形式提交。另外，我们也可以将Content-Type设置为application/json来提交JSON数据，或者设置为multipart/form-data来上传文件。 表格中列出了Content-Type和POST提交数据方式的关系。 Content-Type 提交数据的方式 application/x-www-form-urlencodeed 表单数据 multipart/form-data 表单文件上传 application/json 序列化JSON数据 text/xml XML数据 在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type，不然可能会导致POST提交后无法正常响应。 响应响应，由服务端返回给客户端，可以分为三部分：响应状态码（Response Status Code）、响应头（Response Headers）和响应体（Response Body）。 响应状态码响应状态码表示服务器的响应状态，如200代表服务器正常响应，404代表页面未找到，500代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为200，则证明成功返 回数据，再进行进一步的处理，否则直接忽略。下表列出了常见的错误代码及错误原因。 状态码 说明 详情 100 继续 请求者应当继续提出请求，服务器已收到请求的一部分，正在等待其余部分 101 切换协议 请求者已要求服务器切换协议，服务器已确认并确认切换 200 成功 服务器已成功处理了请求 201 已创建 请求成功并且服务器创建了新的资源 202 已接受 服务器已接受请求，但尚未处理 203 非授权信息 服务器已经成功处理请求，但返回的信息可能来自另一个源 204 无内容 服务器成功处理了请求，但没有返回任何内容 205 重置内容 服务器成功处理了请求，但内容被重置 206 部分内容 服务器成功处理了部分请求 300 多种选择 针对请求，服务器可执行多种操作 301 永久移动 请求的网页已永久移动到新位置，即永久重定向 302 临时移动 请求的网页暂时跳转到其他页面，即暂时重定向 303 查看其他位置 如果原来的请求是POST，重定向目标文档应当通过GET方式访问资源 304 未修改 此次请求返回的网页未修改，继续使用上次的资源 305 使用代理 请求者应该使用代理访问该网页 307 临时重定向 请求的资源临时从其他位置响应 400 错误请求 服务器无法解析该请求 401 未授权 请求没有进行身份验证或验证未通过 403 禁止访问 服务器拒绝此请求 404 未找到 服务器找不到请求的网页 405 方法禁用 服务器禁用了请求中指定的方法 406 不接受 无法使用请求的内容响应请求的网页 407 需要代理授权 请求者需要使用代理授权 408 请求超时 服务器请求超时 409 冲突 服务器在完成请求时发生冲突 410 已删除 请求的资源已永久删除 411 需要有效长度 服务器不接受不含有效内容长度标头字段的请求 412 未满足前提条件 服务器未满足请求者在请求中设置的其中一个前提条件 413 请求实体过大 请求实体过大，超出服务器的处理能力 415 请求URL过长 请求的网址过长，服务器无法处理 416 请求范围不符 页面无法提供请求页面支持 417 未满足期望值 服务器为满足期望请求标头字段的要求 500 服务器内部错误 服务器遇到错误，无法完成请求 501 未实现 服务器不具备完成请求的功能 502 错误网关 服务器作为网关或代理，从上游服务器收到无效响应 503 服务不可用 服务器目前无法使用 504 网关超时 服务器作为网关或代理，但是没有及时从上游服务器收到响应 505 HTTP版本不支持 服务器不支持请求中所用的HTTP协议版本 响应头包含了服务器对请求的应答信息，如Content-Type、Server、Set-Cookie等。下面简要说明一些常用的响应头信息。 Date：标识响应产生的时间。 Last-Modified：指定资源的最后修改时间。 Content-Encoding：指定响应内容的编码。 Server：包含服务器的信息，比如名称、版本号等。 Content-Type：文档类型，指定返回的数据类型是什么，如text/html代表返回HTML文档，application/x-javascript则代表返回JavaScript文件，image/jpeg则代表返回图片。 Set-Cookie：设置Cookies。响应头中的Set-Cookie告诉浏览器需要将此内容放在Cookies中，下次请求携带Cookies请求。 Expires：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。 响应体最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体，如图所示。 在浏览器开发者工具中点击Response，就可以看到网页的源代码，也就是响应体的内容，它是解析的目标。在做爬虫时，我们主要通过响应体得到网页的源代码、JSON数据等，然后从中做相应内容的提取。"},{"title":"Git常用命令","date":"2021-05-08T10:03:19.000Z","url":"/2021/05/08/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":[["Git","/tags/Git/"]],"categories":[["Git","/categories/Git/"]],"content":"Git常用命令仓库在当前目录新建一个Git代码库 新建一个目录，将其初始化为Git代码库 下载一个项目和它的整个代码历史 配置显示当前的Git配置 编辑Git配置文件 设置提交代码时的用户信息 增加/删除文件添加指定文件到暂存区 添加指定目录到暂存区，包括子目录 添加当前目录的所有文件到暂存区 添加每个变化前，都会要求确认对于同一个文件的多处变化，可以实现分次提交 停止追踪指定文件，但该文件会保留在工作区 改名文件，并且将这个改名放入暂存区 代码提交提交暂存区到仓库区 提交暂存区的指定文件到仓库区 提交工作区自上次commit之后的变化，直接到仓库区 提交时显示所有diff信息 使用一次新的commit，替代上一次提交,如果代码没有任何新变化，则用来改写上一次commit的提交信息 重做上一次commit，并包括指定文件的新变化 分支列出所有本地分支 列出所有远程分支 列出所有本地分支和远程分支 新建一个分支，但依然停留在当前分支 新建一个分支，并切换到该分支 新建一个分支，指向指定commit 新建一个分支，与指定的远程分支建立追踪关系 切换到指定分支，并更新工作区 切换到上一个分支 建立追踪关系，在现有分支与指定的远程分支之间 合并指定分支到当前分支 选择一个commit，合并进当前分支 删除分支 删除远程分支 标签列出所有tag 新建一个tag在当前commit 新建一个tag在指定commit 删除本地tag 删除远程tag 查看tag信息 提交指定tag 提交所有tag 新建一个分支，指向某个tag 查看信息显示有变更的文件 显示当前分支的版本历史 显示commit历史，以及每次commit发生变更的文件 搜索提交历史，根据关键词 显示某个commit之后的所有变动，每个commit占据一行 显示某个commit之后的所有变动，其”提交说明”必须符合搜索条件 显示某个文件的版本历史，包括文件改名 显示指定文件相关的每一次diff 显示过去5次提交 显示所有提交过的用户，按提交次数排序 显示指定文件是什么人在什么时间修改过 显示暂存区和工作区的差异 显示暂存区和上一个commit的差异 显示工作区与当前分支最新commit之间的差异 显示两次提交之间的差异 显示今天你写了多少行代码 显示某次提交的元数据和内容变化 显示某次提交发生变化的文件 显示某次提交时，某个文件的内容 显示当前分支的最近几次提交 远程同步下载远程仓库的所有变动 显示所有远程仓库 显示某个远程仓库的信息 增加一个新的远程仓库，并命名 取回远程仓库的变化，并与本地分支合并 上传本地指定分支到远程仓库 强行推送当前分支到远程仓库，即使有冲突 推送所有分支到远程仓库 撤销恢复暂存区的指定文件到工作区 恢复某个commit的指定文件到暂存区和工作区 恢复暂存区的所有文件到工作区 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 重置暂存区与工作区，与上一次commit保持一致 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 重置当前HEAD为指定commit，但保持暂存区和工作区不变 新建一个commit，用来撤销指定commit，后者的所有变化都将被前者抵消，并且应用到当前分支 暂时将未提交的变化移除，稍后再移入 其他生成一个可供发布的压缩包 "},{"title":"Hexo搭建个人博客","date":"2021-05-08T07:39:22.000Z","url":"/2021/05/08/Hexo%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"HexoHexo是一个快速、简洁且高效的博客框架。 安装git安装 安装 Node.jsNode.js 为大多数平台提供了官方的 安装程序。对于中国大陆地区用户，可以前往 淘宝 Node.js 镜像 下载。 其它的安装方法： 安装 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 安装完成后，win+R输入cmd调出命令行，输入hexo提示如下，说明安装正确。 建站安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 新建完成后，指定文件夹的目录如下： _config.yml网站的 配置 信息，您可以在此配置大部分的参数。 package.json应用程序的信息。EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。 scaffolds模版文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。 Hexo的模板是指在新建的文章文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。 source资源文件夹是存放用户资源的地方。除 _posts文件夹之外，开头命名为 _ (下划线)的文件/文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public文件夹，而其他文件会被拷贝过去。 themes主题文件夹。Hexo 会根据主题来生成静态页面。 配置相关配置可直接访问官方文档查看，我们先从使用别人的主题开始，官方提供了335个主题下载使用，你也可以根据规范制定自己的主题。 主题创建Hexo主题非常容易，您只要在themes文件夹内，新增一个任意名称的文件夹，并修改_config.yml内的theme设定，即可切换主题。一个主题可能会有以下的结构： _config.yml主题的配置文件。和 Hexo 配置文件不同，主题配置文件修改时会自动更新，无需重启Hexo Server。 获取主题选择相应的主题，从github上获取到themes目录下。 修改主目录下_config.yml中的配置文件，将theme修改为获取主题的文件夹名。 运行在主目录下调用cmd命令hexo server运行服务，访问进入博客。 添加文章"}]