[{"title":"request、pyquest和pymongodb案例实战","date":"2021-05-23T13:12:36.000Z","url":"/2021/05/23/request%E3%80%81pyquest%E5%92%8Cpymongodb%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/","tags":[["request","/tags/request/"],["pyquest","/tags/pyquest/"],["pymongodb","/tags/pymongodb/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"准备工作在本节课开始之前，我们需要做好如下的准备工作： 安装好Python3（最低为 3.6 版本），并能成功运行Python3程序。 了解Python多进程的基本原理。 了解PythonHTTP请求库requests的基本用法。 了解正则表达式的用法和Python中正则表达式库re的基本用法。 了解PythonHTML解析库pyquery的基本用法。 了解MongoDB并安装和启动MongoDB服务。 了解Python的MongoDB操作库PyMongo的基本用法。 爬虫目标一个基本的静态网站作为案例进行爬取，需要爬取的链接为：，这个网站里面包含了一些电影信息。 要完成的目标是： 用requests爬取这个站点每一页的电影列表，顺着列表再爬取每个电影的详情页。 用pyquery和正则表达式提取每部电影的名称、封面、类别、上映时间、评分、剧情简介等内容。 把以上爬取的内容存入MongoDB数据库。 使用多进程实现爬取的加速。 爬取列表页爬取的第一步肯定要从列表页入手，首先观察一下列表页的结构和翻页规则。在浏览器中访问，然后打开浏览器开发者工具，观察每一个电影信息区块对应的HTML，以及进入到详情页的URL是怎样的，如图所示： 每部电影对应的区块都是一个div节点，它的class属性都有el-card这个值。每个列表页有10个这样的div节点，也就对应着10部电影的信息。 再分析下从列表页是怎么进入到详情页的，选中电影的名称，看下结果： 这个名称实际上是一个h2节点，其内部的文字就是电影的标题。h2节点的外面包含了一个a节点，这个a节点带有href属性，这就是一个超链接，其中href的值为/detail/1，这是一个相对网站的根URL路径，加上网站的根URL就构成了，也就是这部电影详情页的URL。这样只需要提取这个href属性就能构造出详情页的URL并接着爬取了。 接下来分析下翻页的逻辑，拉到页面的最下方，可以看到分页页码，如图所示： 页面显示一共有100条数据，10页的内容，因此页码最多是10。接着我们点击第2页，如图所示： 可以看到网页的URL变成了，相比根URL多了/page/2这部分内容。网页的结构还是和原来一模一样，所以我们可以和第1页一样处理。 接着查看第3页、第4页等内容，可以发现有这么一个规律，每一页的URL最后分别变成了/page/3、/page/4。所以，/page后面跟的就是列表页的页码，当然第1页也是一样，在根URL后面加上/page/1也是能访问的，只不过网站做了一下处理，默认的页码是1，所以显示第1页的内容。 分析到这里，逻辑基本就清晰了。 如果要完成列表页的爬取，可以这么实现： 遍历页码构造10页的索引页URL。 从每个索引页分析提取出每个电影的详情页URL。 先定义一些基础的变量，并引入一些必要的库，写法如下： 引入requests用来爬取页面，logging用来输出信息，re用来实现正则表达式解析，pyquery用来直接解析网页，pymongo用来实现MongoDB存储，urljoin用来做URL的拼接。 接着定义日志输出级别和输出格式，完成之后再定义BASE_URL为当前站点的根URL，TOTAL_PAGE为需要爬取的总页码数量。 定义好了之后，来实现一个页面爬取的方法，实现如下： 考虑到不仅要爬取列表页，还要爬取详情页，在这里定义一个较通用的爬取页面的方法，叫作scrape_page，它接收一个url参数，返回页面的html代码。 首先判断状态码是不是200，如果是，则直接返回页面的HTML代码，如果不是，则会输出错误日志信息。另外，这里实现了requests的异常处理，如果出现了爬取异常，则会输出对应的错误日志信息。这时将logging的error方法的exc_info参数设置为True则可以打印出Traceback错误堆栈信息。 有了scrape_page方法之后，给这个方法传入一个url，正常情况下它就可以返回页面的HTML代码。 在这个基础上，来定义列表页的爬取方法吧，实现如下： 方法名称叫作scrape_index，这个方法会接收一个page参数，即列表页的页码，在方法里面实现列表页的URL拼接，然后调用scrape_page方法爬取即可得到列表页的HTML代码了。 获取了HTML代码后，下一步就是解析列表页，并得到每部电影的详情页的URL了，实现如下： 这里我们定义了parse_index方法，它接收一个html参数，即列表页的HTML代码。接着用pyquery新建一个PyQuery对象，完成之后再用.el-card .name选择器选出来每个电影名称对应的超链接节点。遍历这些节点，通过调用attr方法并传入href获得详情页的URL路径，得到的href就是上文所说的类似/detail/1这样的结果。这并不是一个完整的URL，所以需要借助urljoin方法把BASE_URL和href拼接起来，获得详情页的完整URL，得到的结果就是类似这样完整的URL了，最后yield返回即可。 通过调用parse_index方法传入列表页的HTML代码就可以获得该列表页所有电影的详情页URL了，接下来把上面的方法串联调用一下，实现如下： 定义了main方法来完成上面所有方法的调用，首先使用range方法遍历一下页码，得到的page是1~10，接着把page变量传给scrape_index方法，得到列表页的HTML，赋值为index_html变量。接下来再将index_html变量传给parse_index方法，得到列表页所有电影的详情页URL，赋值为detail_urls，结果是一个生成器，调用list方法就可以将其输出出来。 由于输出内容比较多，这里只贴了一部分。可以看到，在这个过程中程序首先爬取了第1页列表页，然后得到了对应详情页的每个URL，接着再接着爬第2页、第3页，一直到第10页，依次输出了每一页的详情页URL。这样，就成功获取到所有电影详情页URL。 爬取详情页首先观察一下详情页的HTML代码，如图所示： 经过分析，要提取的内容和对应的节点信息如下： 封面：是一个img节点，其class属性为cover。 名称：是一个h2节点，其内容便是名称。 类别：是span节点，其内容便是类别 内容，其外侧是button节点，再外侧则是class为categories的div节点。 上映时间：是span节点，其内容包含了上映时间，其外侧是包含了class为info的div节点。但注意这个div前面还有一个class为info的div节点，可以使用其内容来区分，也可以使用nth-child或nth- of-type这样的选择器来区分。另外提取结果中还多了「上映」二字，可以用正则表达式把日期提取出来。 评分：是一个p节点，其内容便是评分，p节点的class属性为score。 剧情简介：是一个p节点，其内容便是 剧情简介，其外侧是class为drama的div节点。 刚才已经成功获取了详情页的URL，接下来要定义一个详情页的爬取方法，实现如下： 定义了一个scrape_detail方法，它接收一个url参数，并通过调用scrape_page方法获得网页源代码。由于刚才已经实现了scrape_page方法，所以在这里不用再写一遍页面爬取的逻辑，直接调用即可，这就做到了代码复用。 单独定义一个scrape_detail方法在逻辑上会显得更清晰，而且以后如果想要对scrape_detail方法进行改动，比如添加日志输出或是增加预处理，都可以在 scrape_detail里面实现，而不用改动scrape_page方法，灵活性会更好。 详情页的爬取方法已经实现了，接着就是详情页的解析了，实现如下： 定义了parse_detail方法用于解析详情页，它接收一个html参数，解析其中的内容，并以字典的形式返回结果。每个字段的解析情况如下所述： cover：封面，直接选取class为cover的img节点，并调用attr方法获取src属性的内容即可。 name：名称，直接选取a节点的直接子节点h2节点，并调用text方法提取其文本内容即可得到名称。 categories：类别，由于类别是多个，所以这里首先用.categories button span选取了class为categories的节点内部的span节点，其结果是多个，所以这里进行了遍历，取出了每个span节点的文本内容，得到的便是列表形式的类别。 published_at：上映时间，由于pyquery支持使用:contains直接指定包含的文本内容并进行提取，且每个上映时间信息都包含了「上映」二字，所以这里就直接使用:contains(上映)提取了class为info的div节点。提取之后，得到的结果类似「1993-07-26 上映」这样，并不想要「上映」这两个字，所以又调用了正则表达式把日期单独提取出来了。当然这里也可以直接使用strip或replace方法把多余的文字去掉。 drama：直接提取class为drama的节点内部的p节点的文本即可。 score：直接提取class为score的p节点的文本即可，由于提取结果是字符串，所以我们需要把它转成浮点数，即``float`类型。 上述字段提取完毕之后，构造一个字典返回。这样，成功完成了详情页的提取和分析了。 将main方法稍微改写一下，增加这两个方法的调用，改写如下： 首先遍历了detail_urls，获取了每个详情页的URL，然后依次调用了scrape_detail和parse_detail方法，最后得到了每个详情页的提取结果，赋值为data并输出。 运行结果如下： 可以看到，已经成功提取出每部电影的基本信息，包括封面、名称、类别，等等。 保存到MongoDB请确保现在有一个可以正常连接和使用的MongoDB数据库。 将数据导入MongoDB需要用到PyMongo这个库，这个在最开始已经引入过了。那么接下来我们定义一下 MongoDB的连接配置，实现如下： 在这里声明了几个变量，介绍如下： MONGO_CONNECTION_STRING：MongoDB的连接字符串，里面定义了MongoDB的基本连接信息，如host、port，还可以定义用户名密码等内容。 MONGO_DB_NAME：MongoDB数据库的名称。 MONGO_COLLECTION_NAME：MongoDB的集合名称。 这里用MongoClient声明了一个连接对象，然后依次声明了存储的数据库和集合。接下来，再实现一个将数据保存到MongoDB的方法，实现如下： 声明了一个save_data方法，它接收一个data参数，也就是我们刚才提取的电影详情信息。在方法里面，调用了update_one方法，第1个参数是查询条件，即根据name进行查询；第2个参数是data对象本身，也就是所有的数据，这里用$set操作符表示更新操作；第3个参数很关键，这里实际上是upsert参数，如果把这个设置为 True，则可以做到存在即更新，不存在即插入的功能，更新会根据第一个参数设置的name字段，所以这样可以防止数据库中出现同名的电影数据。 注：实际上电影可能有同名，但该场景下的爬取数据没有同名情况，当然这里更重要的是实现MongoDB的去重操作。 接下来将main方法稍微改写一下就好了，改写如下： 重新运行，输出结果： 运行完毕之后我们可以使用MongoDB客户端工具可视化查看已经爬取到的数据，结果如下： 多进程加速由于整个的爬取是单进程的，而且只能逐条爬取，速度稍微有点慢，有没有方法来对整个爬取过程进行加速呢？ 在前面学习了多进程的基本原理和使用方法，下面就来实践一下多进程的爬取。 由于一共有10页详情页，并且这10页内容是互不干扰的，所以可以一页开一个进程来爬取。由于这10个列表页页码正好可以提前构造成一个列表，所以可以选用多进程里面的进程池Pool来实现这个过程。 这里需要改写下main方法的调用，实现如下： 这里首先给main方法添加一个参数page，用以表示列表页的页码。接着声明了一个进程池，并声明pages为所有需要遍历的页码，即1~10。最后调用map方法，第1个参数就是需要被调用的方法，第2个参数就是pages，即需要遍历的页码。 这样pages就会被依次遍历。把1~10这10个页码分别传递给main方法，并把每次的调用变成一个进程，加入到进程池中执行，进程池会根据当前运行环境来决定运行多少进程。 运行输出结果和之前类似，但是可以明显看到加了多进程执行之后，爬取速度快了非常多。可以清空一下之前的MongoDB数据，可以发现数据依然可以被正常保存到MongoDB数据库中。"},{"title":"MongoDB数据库的使用","date":"2021-05-23T06:46:16.000Z","url":"/2021/05/23/MongoDB%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":[["python","/tags/python/"],["mongodb","/tags/mongodb/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"准备工作开始之前，请确保你已经安装好了MongoDB并启动了其服务，同时安装好了Python的PyMongo库。 安装好之后，我们需要把MongoDB服务启动起来。启动完成之后，它会默认在本地localhost的27017端口上运行。 接下来我们需要安装PyMongo这个库，它是Python用来操作MongoDB的第三方库，直接用pip3安装即可： 连接MongoDB连接MongoDB时，需要使用PyMongo库里面的MongoClient。一般来说，向其传入MongoDB的IP及端口即可，其中第一个参数为地址host，第二个参数为端口 port（如果不给它传递参数，则默认是27017）： 另外，MongoClient的第一个参数host还可以直接传入MongoDB的连接字符串，它以mongodb开头，例如： 这样也可以达到同样的连接效果。 指定数据库MongoDB中可以建立多个数据库，接下来指定操作其中一个数据库。这里以test数据库作为下一步需要在程序中指定使用的例子： 这里调用client的test属性即可返回test数据库。当然，也可以这样指定： 这两种方式是等价的。 指定集合MongoDB的每个数据库又包含许多集合（collection），它们类似于关系型数据库中的表。 下一步需要指定要操作的集合，这里指定一个名称为students的集合。与指定数据库类似，指定集合也有两种方式： 或是 这样便声明了一个Collection对象。 插入数据接下来，便可以插入数据了。对students这个集合新建一条学生数据，这条数据以字典形式表示： 新建的这条数据里指定了学生的学号、姓名、年龄和性别。直接调用collection的insert方法即可插入数据，代码如下： 在MongoDB中，每条数据其实都有一个_id属性来唯一标识。如果没有显式指明该属性，MongoDB会自动产生一个ObjectId类型的_id属性。insert()方法会在执行后返回_id值。 运行结果如下： 当然，我们也可以同时插入多条数据，只需要以列表形式传递即可，示例如下： 返回结果是对应的_id的集合： 在PyMongo中，官方已经不推荐使用insert方法了。但是如果你要继续使用也没有什么问题。目前，官方推荐使用insert_one和insert_many方法来分别插入单条记录和多条记录，示例如下： 运行结果如下： 与insert方法不同，返回的是InsertOneResult对象，可以调用其inserted_id属性获取_id。 对于insert_many方法，可以将数据以列表形式传递，示例如下： 运行结果如下： 该方法返回的类型是InsertManyResult，调用inserted_ids属性可以获取插入数据的_id列表。 查询数据插入数据后，可以利用find_one或find方法进行查询，其中find_one查询得到的是单个结果，find则返回一个生成器对象。示例如下： 这里我们查询name为Holy的数据，它的返回结果是字典类型，运行结果如下： 查询结果： 可以发现，它多了_id属性，这就是MongoDB在插入过程中自动添加的。此外，也可以根据ObjectId来查询，此时需要调用bson库里面的objectid： 查询结果： 对于多条数据的查询，可以使用find方法。例如，这里查找性别为male的数据，示例如下： 运行结果如下： 返回结果是Cursor类型，它相当于一个生成器，需要遍历获取的所有结果，其中每个结果都是字典类型。 如果要查询年龄大于20的数据，则写法如下： 查询的条件键值已经不是单纯的数字了，而是一个字典，其键名为比较符号$gt，意思是大于，键值为20。 比较符号归纳如下： 符号 含义 示例 $lt 小于 {‘age’:{‘$lt’:20}} $gt 大于 {‘age’:{‘$gt’:20}} $lte 小于或等于 {‘age’:{‘$lte’:20}} $gte 大于或等于 {‘age’:{‘$gte’:20}} $ne 不等于 {‘age’:{‘$ne’:20}} $in 在范围内 {‘age’:{‘$in’:[20,23]}} $nin 不在范围内 {‘age’:{‘$nin’:[20, 23]}} table { margin: auto; font-size: 80%; } 另外，还可以进行正则匹配查询。例如，查询名字以H开头的学生数据，示例如下： 这里使用$regex来指定正则匹配，^M.*代表以M开头的正则表达式。 一些功能符号归类为下表： 符号 含义 示例 $regrx 匹配正则表达式 {‘name’:{‘$regex’:’^M.*’}} $exists 属性是否存在 {‘name’:{‘$exists’:True}} $type 类型判断 {‘age’:{‘$type’:’int’}} $mod 数字模操作 {‘age’:{‘$mod’:[5,0]}} $text 文本查询 {‘$text’:{‘$search’:’Holy’}} $where 高级条件查询 {‘$where’:’obj.fans_count==obj.follows_count’} 计数要统计查询结果有多少条数据，可以调用count方法。以统计所有数据条数为例： 我们还可以统计符合某个条件的数据： 运行结果是一个数值，即符合条件的数据条数。 排序排序时，可以直接调用sort方法，并在其中传入排序的字段及升降序标志。示例如下： 运行结果如下： 调用pymongo.ASCENDING指定升序。如果要降序排列，可以传入pymongo.DESCENDING。 偏移只需要取某几个元素，可以利用skip方法偏移几个位置，比如偏移2，就代表忽略前两个元素，得到第3个及以后的元素： 运行结果如下： 还可以用limit方法指定要取的结果个数，示例如下： 运行结果如下： 值得注意的是，在数据量非常庞大的时候，比如在查询千万、亿级别的数据库时，最好不要使用大的偏移量，因为这样很可能导致内存溢出。 更新数据更新，可以使用update方法，指定更新的条件和更新后的数据即可。例如： 更新name为Holy的数据的年龄：首先指定查询条件，然后将数据查询出来，修改年龄后调用update方法将原条件和修改后的数据传入。 运行结果如下： 返回结果是字典形式，ok代表执行成功，nModified代表影响的数据条数。 也可以使用$set操作符对数据进行更新，代码如下： 这样可以只更新student字典内存在的字段。如果原先还有其他字段，则不会更新，也不会删除。而如果不用$set的话，则会把之前的数据全部用student字典替换；如果原本存在其他字段，则会被删除。 update方法其实也是官方不推荐使用的方法。这里也分为update_one方法和update_many方法，用法更加严格，它们的第2个参数需要使用$类型操作符作为字典的键名，示例如下: 上面的例子中调用了update_one方法，使得第2个参数不能再直接传入修改后的字典，而是需要使用&#123;&#39;$set&#39;:student&#125;这样的形式，其返回结果是UpdateResult类型。然后分别调用matched_count和modified_count属性，可以获得匹配的数据条数和影响的数据条数。 运行结果如下： 删除删除操作比较简单，直接调用remove方法指定删除的条件即可，此时符合条件的所有数据均会被删除。 示例如下： 运行结果如下： 另外，这里依然存在两个新的推荐方法 ——delete_one和delete_many，示例如下： 运行结果如下： delete_one删除第一条符合条件的数据，delete_many即删除所有符合条件的数据。它们的返回结果都是DeleteResult类型，可以调用deleted_count属性获取删除的数据条数。"},{"title":"Pyquery的使用","date":"2021-05-22T16:49:30.000Z","url":"/2021/05/23/Pyquery%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":[["python","/tags/python/"],["pyquery","/tags/pyquery/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"准备工作pyquery是Python的第三方库，安装命令如下： 初始化解析HTML文本的时候，首先需要将其初始化为一个pyquery对象。它的初始化方式有多种，比如直接传入字符串、传入URL、传入文件名，等等。 字符串初始化可以直接把HTML的内容当作参数来初始化pyquery对象。 运行结果如下： 引入pyquery这个对象，取别名为pq，然后声明了一个长HTML字符串，并将其当作参数传递给pyquery类，这样就成功完成了初始化。 接下来，将初始化的对象传入CSS选择器。在这个实例中，传入li节点，这样就可以选择所有的li节点。 URL初始化初始化的参数不仅可以以字符串的形式传递，还可以传入网页的URL，只需要指定参数为url即可： pyquery对象会首先请求这个URL，然后用得到的HTML内容完成初始化。这就相当于将网页的源代码以字符串的形式传递给pyquery类来初始化。 它与下面的功能是相同的： 文件初始化除了传递一个URL，还可以传递本地的文件名，参数指定为filename即可： 这里需要有一个本地HTML文件demo.html，其内容是待解析的HTML字符串。这样它会先读取本地的文件内容，然后将文件内容以字符串的形式传递给 pyquery类来初始化。 以上3种方式均可初始化，当然最常用的初始化方式还是以字符串形式传递。 基本CSS选择器 运行结果： 初始化pyquery对象之后，传入CSS选择器#container .list li，它的意思是先选取id为container的节点，然后再选取其内部class为list的所有li节点，最后打印输出。成功获取到了符合条件的节点。将它的类型打印输出后发现，它的类型依然是pyquery类型。 直接遍历这些节点，然后调用text方法，就可以获取节点的文本内容，代码示例如下： 结果如下： 查找节点子节点查找子节点需要用到find方法，传入的参数是CSS选择器： 运行结果： 通过.list参数选取class为list的节点，然后调用find方法，传入CSS选择器，选取其内部的li节点，最后打印输出。可以发现，find方法会将符合条件的所有节点选择出来，结果的类型是pyquery类型。 find的查找范围是节点的所有子孙节点，而如果我们只想查找子节点，那可以用children方法： 运行结果： 如果要筛选所有子节点中符合条件的节点，比如想筛选出子节点中class为active的节点，可以向children方法传入CSS选择器.active，代码如下： 结果如下： 输出的结果做了筛选，留下了class为active的节点。 父节点可以用parent方法获取某个节点的父节点： 运行结果如下： 用.list选取class为list的节点，然后调用parent方法得到其父节点，其类型依然是pyquery类型。这里的父节点是该节点的直接父节点，也就是说，它不会再去查找父节点的父节点，即祖先节点。 如果你想获取某个祖先节点，该怎么办呢？可以用parents方法： 运行结果如下： 输出结果有两个：一个是class为wrap的节点，一个是id为container的节点。也就是说，使用parents方法会返回所有的祖先节点。 要筛选某个祖先节点的话，可以向parents方法传入CSS选择器，这样就会返回祖先节点中符合CSS选择器的节点： 结果如下： 输出结果少了一个节点，只保留了class为wrap的节点。 兄弟节点要获取兄弟节点，可以使用siblings方法。这里还是以上面的HTML代码为例： 结果如下： 先选择class为list的节点，内部class为item-0和active的节点，也就是第3个li节点。很明显，它的兄弟节点有4个，那就是第1、2、4、5 个li节点。结果显示的正是4个兄弟节点。 筛选某个兄弟节点，可以用siblings方法传入CSS选择器，这样就会从所有兄弟节点中挑选出符合条件的节点了： 筛选class为active的节点，从刚才的结果中可以观察到，class为active兄弟节点的是第4个li节点，所以结果应该是1个。运行结果： 遍历pyquery的选择结果既可能是多个节点，也可能是单个节点，类型都是pyquery类型，并没有返回列表。对于单个节点来说，可以直接打印输出，也可以直接转成字符串： 运行结果如下： 对于有多个节点的结果，我们就需要用遍历来获取了。例如，如果要把每一个li节点进行遍历，需要调用items方法： 运行结果如下： 调用items方法后，会得到一个生成器，遍历一下，就可以逐个得到li节点对象了，它的类型也是pyquery类型。每个li节点还可以调用前面所说的方法进行选择，比如继续查询子节点，寻找某个祖先节点等。 获取信息提取到节点之后，最终目的是提取节点包含的信息。比较重要的信息有两类，一是获取属性，二是获取文本。 获取属性提取到某个pyquery类型的节点后，就可以调用attr方法来获取属性： 运行结果如下： 首先选中class为item-0和active的li节点内的a节点，它的类型是pyquery类型。然后调用attr方法。在这个方法中传入属性的名称，就可以得到属性值了。此外，也可以通过调用attr属性来获取属性值，用法如下： 结果：link3.html，这两种方法的结果完全一样。 获取文本获取节点之后的另一个主要操作就是获取其内部文本了，此时可以调用text方法来实现： 运行结果如下： 首先选中一个a节点，然后调用text方法，就可以获取其内部的文本信息了。text会忽略节点内部包含的所有HTML，只返回纯文字内容。 如果想要获取这个节点内部的HTML文本，就要用html方法： 如果我们选中的结果是多个节点，text或html方法会返回什么内容？我们用实例来看一下： 运行结果如下： html方法返回的是第1个li节点的内部HTML文本，而text则返回了所有的li节点内部的纯文本，中间用一个空格分割开，即返回结果是一个字符串。 节点操作pyquery提供了一系列方法来对节点进行动态修改，比如为某个节点添加一个class，移除某个节点等，这些操作有时会为提取信息带来极大的便利。由于节点操作的方法太多，下面举几个典型的例子来说明它的用法。 addClass和removeClass 首先选中第3个li节点，然后调用removeClass方法，将li节点的active这class移除，第2步调用addClass方法，将class添加回来。每执行一次操作，就打印输出当前li节点的内容。 运行结果如下： 一共输出了3次。第2次输出时，li节点的active这个class被移除了，第3次class又添加回来了。addClass和removeClass方法可以动态改变节点的class属性。 attr、text、html除了操作class这个属性外，也可以用attr方法对属性进行操作。此外，还可以用text和html方法来改变节点内部的内容。示例如下： 首先选中li节点，然后调用attr方法来修改属性。该方法的第1个参数为属性名，第2个参数为属性值。最后调用text和html方法来改变节点内部的内容。3次操作后，分别打印输出当前的li节点。 运行结果如下： 调用attr方法后，li节点多了一个原本不存在的属性name，其值为link。接着调用text方法传入文本，li节点内部的文本全被改为传入的字符串文本。最后，调用html方法传入HTML文本，li节点内部又变为传入的HTML文本了。 使用attr方法时如果只传入第1个参数的属性名，则是获取这个属性值；如果传入第2个参数，可以用来修改属性值。使用text和html方法时如果不传参数，则是获取节点内纯文本和HTML文本，如果传入参数，则进行赋值。 removeremove方法就是移除，它有时会为信息的提取带来非常大的便利。下面有一段 HTML文本： 想提取“Hello, World”这个字符串，该怎样操作呢？ 这里先直接尝试提取class为wrap的节点的内容，看看是不是我们想要的。 运行结果如下： 这个结果还包含了内部的p节点的内容，也就是说text把所有的纯文本全提取出来了。 如果想去掉p节点内部的文本，可以选择再把p节点内的文本提取一遍，然后从整个结果中移除这个子串，但这个做法明显比较烦琐。 这时remove方法就可以派上用场了，可以接着这么做： 首先选中p节点，然后调用remove方法将其移除，这时wrap内部就只剩下“Hello, World”这句话了，最后利用text方法提取即可。 其实还有很多其他节点操作的方法，比如append、empty和prepend等方法，详细的用法可以参考官方文档：。 伪类选择器CSS选择器之所以强大，还有一个很重要的原因，那就是它支持多种多样的伪类选择器，例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等。示例如下： 在这个例子中使用了CSS3的伪类选择器，依次选择了第1个li节点、最后一个li节点、第2个li节点、第3个li之后的li节点、偶数位置的li节点、包含second文本的li节点。"},{"title":"正则表达式","date":"2021-05-21T13:08:59.000Z","url":"/2021/05/21/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","tags":[["python","/tags/python/"],["re","/tags/re/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"正则表达式正则表达式是处理字符串的强大工具，它有自己特定的语法结构。能实现字符串的检索、替换、匹配验证，对于爬虫来说，从HTML里提取想要的信息就非常方便。 实例引入下面用几个实例来看一下正则表达式的用法。 打开开源中国提供的正则表达式测试工具，输入待匹配的文本，然后选择常用的正则表达式，就可以得出相应的匹配结果了。 例如，输入下面这段待匹配的文本： 这段字符串中包含了一个电话号码和一个电子邮件，接下来就尝试用正则表达式提取出来，如图所示。 在网页右侧选择“匹配 Email地址”，就可以看到下方出现了文本中的E-mail。如果选择“匹配网址URL”，就可以看到下方出现了文本中的URL。 这里使用了正则表达式的匹配功能，也就是用一定规则将特定的文本提取出来。 比方说，电子邮件是有其特定的组成格式的：一段字符串 + @ 符号 + 某个域名。而URL的组成格式则是协议类型``+``冒号``+``双斜线``+``域名和路径。 可以用下面的正则表达式匹配URL： 用这个正则表达式去匹配一个字符串，如果这个字符串中包含类似URL的文本，那就会被提取出来。 下表中列出了常用的匹配规则： 模式 描述 \\w 匹配字母、数字及下划线 \\W 匹配不是字母、数字及下划线的字符 \\s 匹配任意空白字符，等价于 [\\t\\n\\r\\f] \\S 匹配任意非空字符 \\d 匹配任意数字，等价于 [0~9] \\D 匹配任意非数字的字符 \\A 匹配字符串开头 \\Z 匹配字符串结尾，如果存在换行，只匹配到换行前的结束字符串 \\z 匹配字符串结尾，如果存在换行，同时还会匹配换行符 \\G 匹配最后匹配完成的位置 \\n 匹配一个换行符 \\t 匹配一个制表符 ^ 匹配一行字符串的开头 $ 匹配一行字符串的结尾 . 匹配任意字符，除了换行符，当 re.DOTALL 标记被指定时，则可以匹配包括换行符的任意字符 […] 用来表示一组字符，单独列出，比如 [amk] 匹配 a、m 或 k [^…] 不在 [] 中的字符，比如 匹配除了 a、b、c 之外的字符 * 匹配 0 个或多个表达式 + 匹配 1 个或多个表达式 ? 匹配 0 个或 1 个前面的正则表达式定义的片段，非贪婪方式 {n} 精确匹配 n 个前面的表达式 {n, m} 匹配 n 到 m 次由前面正则表达式定义的片段，贪婪方式 a|b 匹配 a 或 b () 匹配括号内的表达式，也表示一个组 table { margin: auto; font-size: 80%; } Python的re库提供了整个正则表达式的实现，利用这个库，可以在Python中使用正则表达式。 match首先介绍一个常用的匹配方法——match，向它传入要匹配的字符串，以及正则表达式，就可以检测这个正则表达式是否匹配字符串。match方法会尝试从字符串的起始位置匹配正则表达式，如果匹配，就返回匹配成功的结果；如果不匹配，就返回None。 运行结果如下： 这里首先声明了一个字符串，其中包含英文字母、空白字符、数字等。接下来，我们写一个正则表达式： 用它来匹配这个长字符串。开头的^·匹配字符串的开头，也就是以Hello开头；\\s匹配空白字符，用来匹配目标字符串的空格；\\d匹配数字，3个\\d匹配123；再写1个\\s匹配空格；后面的4567，依然能用4个\\d来匹配，但是这么写比较烦琐，所以后面可以跟&#123;4&#125;代表匹配前面的规则4次，也就是匹配4个数字；后面再紧接1个空白字符，最后\\w&#123;10&#125;匹配10个字母及下划线。 在match方法中，第一个参数传入正则表达式，第二个参数传入要匹配的字符串。打印输出结果，可以看到结果是SRE_Match对象，这证明成功匹配。该对象有两个方法：group方法可以输出匹配的内容，结果是Hello 123 4567 World_This，这恰好是正则表达式规则所匹配的内容；span方法可以输出匹配的范围，结果是(0, 25)，这就是匹配到的结果字符串在原字符串中的位置范围。 在match方法中，第一个参数传入正则表达式，第二个参数传入要匹配的字符串。打印输出结果，可以看到结果是SRE_Match对象，这证明成功匹配。该对象有两个方法：group方法可以输出匹配的内容，结果是Hello 123 4567 World_This，这恰好是正则表达式规则所匹配的内容；span方法可以输出匹配的范围，结果是(0, 25)，这就是匹配到的结果字符串在原字符串中的位置范围。 匹配目标用match方法得到了匹配到的字符串内容，想从字符串中提取一部分内容，该怎么办呢？ 要从一段文本中提取出邮件或电话号码等内容。可以使用()括号将想提取的子字符串括起来。()实际上标记了一个子表达式的开始和结束位置，被标记的每个子表达式会依次对应每一个分组，调用group方法传入分组的索引即可获取提取的结果。 示例如下： 这里把字符串中的1234567提取出来，此时可以将数字部分的正则表达式用()括起来，然后调用了group(1)获取匹配结果。 运行结果如下： 可以看到，成功得到了1234567。这里用的是group(1)，它与group()有所不同，后者会输出完整的匹配结果，而前者会输出第一个被()包围的匹配结果。假如正则表达式后面还有()包括的内容，那么可以依次用group(2)、group(3)等来获取。 通用匹配刚才写的正则表达比较复杂，出现空白字符写\\s匹配，出现数字用\\d匹配，这样的工作量非常大。 可以用一个万能匹配来减少这些工作，那就是.*。其中.可以匹配任意字符（除换行符），*代表匹配前面的字符无限次，组合在一起可以匹配任意字符。 改写一下正则表达式： 运行结果： 贪婪与非贪婪使用通用匹配.*时，有时候匹配到的并不是我们想要的结果。 想获取中间的数字，所以中间依然写的是(\\d+)。由于数字两侧的内容比较杂乱，所以略写成.*。最后，组成^He.*(\\d+).*Demo$，看样子并没有什么问题。 运行结果： 奇怪的事情发生了，只得到了7这个数字，这是怎么回事呢？ 这里就涉及一个贪婪匹配与非贪婪匹配的问题了。在贪婪匹配下，.*会匹配尽可能多的字符。正则表达式中.*后面是\\d+，也就是至少一个数字，并没有指定具体多少个数字，因此，.*就尽可能匹配多的字符，这里就把123456匹配了，给\\d+留下一个可满足条件的数字7，最后得到的内容就只有数字7了。 这里需要使用非贪婪匹配，非贪婪匹配的写法是.*?。 将第一个.*改成了.*?，转变为非贪婪匹配。运行结果如下： 成功获取1234567，贪婪匹配是尽可能匹配多的字符，非贪婪匹配就是尽可能匹配少的字符。当.*?匹配到Hello后面的空白字符时，再往后的字符就是数字了，而\\d+恰好可以匹配，.*?就不再进行匹配，交给\\d+去匹配后面的数字。这样.*?匹配了尽可能少的字符，\\d+的结果就是1234567。 在做匹配的时候，字符串中间尽量使用非贪婪匹配，也就是用.*?来代替.*，以免出现匹配结果缺失的情况。 需要注意的是，如果匹配的结果在字符串结尾，.*?就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符。例如： 运行结果如下： 修饰符正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。 用实例来看一下： 字符串中加了换行符，正则表达式还是一样的，用来匹配其中的数字。看一下运行结果： 运行直接报错，也就是说正则表达式没有匹配到这个字符串，返回结果为None，又调用了group方法导致AttributeError。为什么加了一个换行符，就匹配不到了呢？这是因为我们匹配的是除换行符之外的任意字符，当遇到换行符时，.*?就不能匹配了，导致匹配失败。 这里只需加一个修饰符re.S，即可修正这个错误： 运行结果如下： 这个re.S在网页匹配中经常用到。因为HTML节点经常会有换行，加上它，就可以匹配节点与节点之间的换行了。 还有一些修饰符，在必要的情况下也可以使用，如表所示： 修饰符 描述 re.I 使匹配对大小写不敏感 re.L 做本地化识别（locale-aware）匹配 re.M 多行匹配，影响 ^ 和 $ re.S 使匹配包括换行在内的所有字符 re.U 根据 Unicode 字符集解析字符。这个标志影响 \\w、\\W、\\b 和 \\B re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解 "},{"title":"python图片转webp格式小工具","date":"2021-05-21T11:02:55.000Z","url":"/2021/05/21/python%E5%9B%BE%E7%89%87%E8%BD%ACwebp%E6%A0%BC%E5%BC%8F%E5%B0%8F%E5%B7%A5%E5%85%B7/","tags":[["python","/tags/python/"]],"categories":[["Python","/categories/Python/"]]},{"title":"docker的安装","date":"2021-05-20T08:45:41.000Z","url":"/2021/05/20/docker%E7%9A%84%E5%AE%89%E8%A3%85/","tags":[["linux","/tags/linux/"],["docker","/tags/docker/"]],"categories":[["Linux","/categories/Linux/"]],"content":"安装环境安装环境是Centos环境，要求版本是 CentOS 7 or 8以上。 卸载旧版本的docker 使用yum仓库安装安装方式有很多种，可以参考官方文档。 设置仓库源下载yum-utils工具包和稳固的安装源。 安装docker引擎 1.安装最新版本 2.安装特定版本 列出可安装的版本。 选择安装版本。 启动docker运行下列命令启动docker。 测试docker安装正确。 卸载docker卸载docker和下载的容器及包： 镜像等配置文件不会自动移除，需要手动删除。 "},{"title":"基于NextCloud搭建个人网盘","date":"2021-05-20T08:16:41.000Z","url":"/2021/05/20/%E5%9F%BA%E4%BA%8Ecentos%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%9B%98/","tags":[["docker","/tags/docker/"],["nextCloud","/tags/nextCloud/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"NextcloudNextcloud是一款开源免费的私有云存储网盘项目，可以让你快速便捷地搭建一套属于自己或团队的云同步网盘，从而实现跨平台跨设备文件同步、共享、版本控制、团队协作等功能。它的客户端覆盖了Windows、Mac、Android、iOS、Linux等各种平台，也提供了网页端以及WebDAV接口，所以你几乎可以在各种设备上方便地访问你的云盘。 安装 Nextcloud安装Mysql 运行mysql并且设置访问端口：3306，容器名称：mysql ,管理员密码：******** 进入容器bash。 登陆mysql 接着输入管理员密码：admin,回车。创建一个数据库 创建一个用户 创建一个用户名称为：nextcloud；‘%’：代表不限ip登陆，远程登陆; 密码为：admin。 授权。 给这个用户nextcloud授予 这个数据库nextcloud.*所有的权限，远程登陆，密码为admin； 基于Docker部署Nextcloud服务端选择以Docker的方式来部署nextcloud是因为Docker可以跨平台上运行，可以确保执行环境的一致性，有利于应用的迁移和管理。 服务端部署的基本流程是：安装Docker并启动 –&gt; 运行Nextcloud容器 –&gt; 访问Web端初始化。 安装的话不做过多说明，参考官方文档或本博客其他博文。 下载Nextcloud镜像 运行Nextcloud 参数说明： -d #容器后台运行 –name nextcloud #容器名 -v /data/nextcloud:/var/www/html #将宿主机的目录/data/nextcloud挂载到容器的/var/www/html -p 8000:80 #将宿主机的端口（此处以8000为例）映射到容器的80端口” 查看运行的容器。 停止运行的容器。 "},{"title":"requests库的基本使用","date":"2021-05-18T07:44:10.000Z","url":"/2021/05/18/requests%E5%BA%93%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":[["python","/tags/python/"],["爬虫","/tags/%E7%88%AC%E8%99%AB/"],["requests","/tags/requests/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"安装requests是一个第三方库，使用pip下载安装。 实例引入用Python写爬虫的第一步就是模拟发起一个请求，把网页的源代码获取下来。 在浏览器中输入一个URL并回车，实际上就是让浏览器帮我们发起一个GET类型的HTTP请求，浏览器得到源代码后，把它渲染出来就可以看到网页内容了。 那如果想用requests来获取源代码，应该怎么办呢？很简单，requests这个库提供了一个get方法，调用这个方法，并传入对应的URL就能得到网页的源代码。 比如这里有一个示例网站:，其内容如下： 这个网站展示了一些电影数据，如果想要把这个网页里面的数据爬下来，比如获取各个电影的名称、上映时间等信息，然后把它存下来的话，该怎么做呢？ 第一步当然就是获取它的网页源代码了。 可以用requests这个库轻松地完成这个过程，代码的写法是这样的： 输出结果如下： 由于网页内容比较多，这里省略了大部分内容。 不过看运行结果，我们已经成功获取网页的HTML源代码，里面包含了电影的标题、类型、上映时间，等等。 把网页源代码获取下来之后，下一步我们把想要的数据提取出来，数据的爬取就完成了。 请求HTTP中最常见的请求之一就是GET请求。 GET请求换一个示例网站，其URL为，如果客户端发起的是GET请求的话，该网站会判断并返回相应的请求信息，包括 Headers、IP等。 我们还是用相同的方法来发起一个GET请求，代码如下： 返回结果： 可以发现，成功发起了GET请求，也通过这个网站的返回结果得到了请求所携带的信息，包括Headers、URL、IP，等等。 对于GET请求，我们知道URL后面是可以跟上一些参数的，如果我们现在想添加两个参数，其中name是germey，age是25，URL就可以写成如下内容： 要构造这个请求链接，是不是要直接写成这样呢？ 这样也可以，但如果这些参数还需要手动拼接，未免有点不人性化。 一般情况下，这种信息我们利用params这个参数就可以直接传递了，示例如下： 返回结果： 把URL参数通过字典的形式传给get方法的params参数，通过返回信息可以判断，请求的链接自动被构造成了：。 网页的返回类型实际上是str类型，但是它很特殊，是JSON格式的。所以，如果想直接解析返回结果，得到一个JSON格式的数据的话，可以直接调用json方法。 示例如下： 结果如下： 调用json方法，就可以将返回结果是JSON格式的字符串转化为字典。 但需要注意的是，如果返回结果不是JSON格式，便会出现解析错误，抛出json.decoder.JSONDecodeError异常。 抓取网页上面的请求链接返回的是JSON形式的字符串，那么如果请求普通的网页，则肯定能获得相应的内容。下面以本课时最初的实例页面为例，我们再加上一点提取信息的逻辑，将代码完善成如下的样子： 运行结果： 抓取二进制数据抓取的是网站的一个页面，实际上它返回的是一个HTML文档。如果想抓取图片、音频、视频等文件，应该怎么办呢？ 图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以，想要抓取它们，就要拿到它们的二进制数据。 下面以 GitHub 的站点图标为例来看一下： 这里抓取的内容是站点图标，也就是在浏览器每一个标签上显示的小图标。 前者出现了乱码，后者结果前带有一个b，这代表是bytes类型的数据。 由于图片是二进制数据，所以前者在打印时转化为str类型，也就是图片直接转化为字符串，这当然会出现乱码。 上面返回的结果我们并不能看懂，它实际上是图片的二进制数据，没关系，将刚才提取到的信息保存下来就好了，代码如下： 这里用了open方法，它的第一个参数是文件名称，第二个参数代表以二进制的形式打开，可以向文件里写入二进制数据。 运行结束之后，可以发现在文件夹中出现了名为baidu.png的图标。 添加添headers在发起一个HTTP请求的时候，会有一个请求头Request Headers，那么这个怎么来设置呢？ 很简单，使用headers参数就可以完成了。 在刚才的实例中，是没有设置Request Headers信息的，如果不设置，某些网站会发现这不是一个正常的浏览器发起的请求，网站可能会返回异常的结果，导致网页抓取失败。 要添加Headers信息，比如添加一个User-Agent字段，可以这么写： 当然，我们可以在headers这个参数中任意添加其他的字段信息。 POST请求使用requests实现post请求，示例如下： 这里还是请求，该网站可以判断如果请求是POST方式，就把相关请求信息返回。 运行结果如下： 响应发送请求后，得到的就是响应，即Response。 在上面的实例中，使用text和content获取了响应的内容。此外，还有很多属性和方法可以用来获取其他信息，比如状态码、响应头、Cookies等。示例如下: 这里分别打印输出status_code属性得到状态码，输出headers属性得到响应头，输出cookies属性得到Cookies，输出url属性得到URL，输出history属性得到请求历史。 运行结果如下： headers和cookies这两个属性得到的结果分别是CaseInsensitiveDict和RequestsCookieJar类型。 状态码是用来表示响应状态的，比如返回200代表我们得到的响应是没问题的，上面的例子正好输出的结果也是200，所以可以通过判断Response的状态码来确认是否爬取成功。 requests还提供了一个内置的状态码查询对象requests.codes，用法示例如下： 这里通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用requests.codes.ok得到的是成功的状态码200。 这样的话，我们就不用再在程序里面写状态码对应的数字了，用字符串表示状态码会显得更加直观。 下面列出了返回码和相应的查询条件： 比如，如果想判断结果是不是404状态，可以用requests.codes.not_found来比对。 高级用法刚才，了解requests的基本用法，如基本的GET、POST请求以及Response对象。当然requests能做到的不仅这些，它几乎可以完成HTTP的所有操作。 下面来了解下requests的一些高级用法，如文件上传、Cookies设置、代理设置等。 文件上传requests可以模拟提交一些数据。假如有的网站需要上传文件，也可以用它来实现，示例如下： 要注意的是，baidu.png需要和当前脚本在同一目录下。如果有其他文件，当然也可以使用其他文件来上传，更改下代码即可。运行结果如下： 以上省略部分内容，这个网站会返回响应，里面包含files这个字段，而form字段是空的，这证明文件上传部分会单独有一个files字段来标识。 Cookies获取Cookies。 运行结果如下： 调用cookies属性即可成功得到Cookies，可以发现它是RequestCookieJar类型。然后用items 方法将其转化为元组组成的列表，遍历输出每一个Cookie的名称和值，实现Cookie的遍历解析。 可以直接用Cookie来维持登录状态，下面我们以GitHub为例来说明一下，首先我们登录GitHub，然后将Headers中的Cookie内容复制下来，如图所示： 可以替换成你自己的Cookie，将其设置到Headers里面，然后发送请求。 Session维持在requests中，如果直接利用get或post等方法的确可以做到模拟网页的请求，但是这实际上是相当于不同的Session，相当于你用两个浏览器打开了不同的页面。 设想这样一个场景，第一个请求利用post方法登录了某个网站，第二次想获取成功登录后的自己的个人信息，又用了一次get方法去请求个人信息页面。实际上，这相当于打开了两个浏览器，是两个完全不相关的Session，能成功获取个人信息吗？当然不能。 解决这个问题的主要方法就是维持同一个Session，相当于打开一个新的浏览器选项卡而不是新开一个浏览器。但我又不想每次设置Cookies，那该怎么办呢？这时候就有了新的利器——**Session对象**。 利用它，可以方便地维护一个Session，而且不用担心Cookies的问题，它会帮我们自动处理好。示例如下： 运行结果如下： 这并不行。再用 Session试试看： 运行结果如下： 利用Session，可以做到模拟同一个Session而不用担心Cookies的问题。它通常用于模拟登录成功之后再进行下一步的操作。 SSL证书验证现在很多网站都要求使用HTTPS协议，但是有些网站可能并没有设置好HTTPS证书，或者网站的HTTPS证书不被CA机构认可，这时候，这些网站可能就会出现SSL证书错误的提示。 比如示例网站：。 用浏览器打开这个URL，则会提示「您的连接不是私密连接」这样的错误，如图所示： 那如果我们一定要爬取这个网站怎么办呢？我们可以使用verify参数控制是否验证证书，如果将其设置为False，在请求时就不会再验证证书是否有效。如果不加verify参数的话，默认值是True，会自动验证。 改写代码如下： 这样就会打印出请求成功的状态码： 不过发现报了一个警告，它建议我们给它指定证书。可以通过设置忽略警告的方式来屏蔽这个警告： 或者通过捕获警告到日志的方式忽略警告： 当然，也可以指定一个本地证书用作客户端证书，这可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组： 上面的代码是演示实例，要有crt和key文件，并且指定它们的路径。另外注意，本地私有证书的key必须是解密状态，加密状态的key是不支持的。 超时时间在本机网络状况不好或者服务器网络响应延迟甚至无响应时，可能会等待很久才能收到响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错。这需要用到timeout参数。这个时间的计算是发出请求到服务器返回响应的时间。示例如下： 通过这样的方式将超时时间设置为1秒，如果1秒内没有响应，那就抛出异常。 实际上，请求分为两个阶段，即连接（connect）和读取（read）。 上面设置的timeout将用作连接和读取这二者的timeout总和。 如果要分别指定，就可以传入一个元组： 如果想永久等待，可以直接将timeout设置为None，或者不设置直接留空，因为默认是None。这样的话，如果服务器还在运行，但是响应特别慢，那就慢慢等吧，它永远不会返回超时错误的。其用法如下： 或直接不加参数： 身份认证在访问某些设置了身份认证的网站时，例如：，我们可能会遇到这样的认证窗口，如图所示： 如果遇到了这种情况，那就是这个网站启用了基本身份认证，英文叫作HTTP Basic Access Authentication，它是一种用来允许网页浏览器或其他客户端程序在请求时提供用户名和口令形式的身份凭证的一种登录验证方式。 如果遇到了这种情况，怎么用reqeusts来爬取呢，当然也有办法。 可以使用requests自带的身份认证功能，通过auth参数即可设置，示例如下： 成功的话，返回状态码200。 如果参数都传一个HTTPBasicAuth类，就显得有点烦琐了，所以requests提供了一个更简单的写法，可以直接传一个元组，它会默认使用HTTPBasicAuth这个类来认证。 上面的代码可以直接简写如下： 此外，requests还提供了其他认证方式，如OAuth认证，不过此时需要安装oauth包，安装命令如下： 使用OAuth1认证的方法如下： 更多详细的功能就可以参考requests_oauthlib的官方文档：，不再赘述。 代理设置某些网站在测试的时候请求几次，能正常获取内容。但是对于大规模且频繁的请求，网站可能会弹出验证码，或者跳转到登录认证页面，更甚者可能会直接封禁客户端的IP，导致一定时间段内无法访问。 为了防止这种情况发生，我们需要设置代理来解决这个问题，这就需要用到proxies参数。可以用这样的方式设置： 当然，直接运行这个实例或许行不通，因为这个代理可能是无效的，可以直接搜索寻找有效的代理并替换试验一下。 若代理需要使用上文所述的身份认证，可以使用类似ttp://user:password@host:port这样的语法来设置代理，示例如下： 除了基本的HTTP代理外，requests还支持SOCKS协议的代理。 首先，需要安装socks这个库： 然后就可以使用SOCKS协议代理了，示例如下： "},{"title":"Nginx配置详解","date":"2021-05-18T03:16:57.000Z","url":"/2021/05/18/Nginx%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3/","tags":[["Nginx","/tags/Nginx/"],["负载均衡","/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"],["http","/tags/http/"]],"categories":[["Nginx","/categories/Nginx/"]],"content":"简介Nginx是lgor Sysoev为俄罗斯访问量第二的rambler.ru站点设计开发的。从2004年发布至今，凭借开源的力量，已经接近成熟与完善。 Nginx功能丰富，可作为HTTP服务器，也可作为反向代理服务器，邮件服务器。支持FastCGI、SSL、Virtual Host、URL Rewrite、Gzip等功能。并且支持很多第三方的模块扩展。 Nginx的稳定性、功能集、示例配置文件和低系统资源的消耗让他后来居上，在全球活跃的网站中有12.18%的使用比率，大约为2220万个网站。 Nginx常用功能1、Http代理，反向代理：作为web服务器最常用的功能之一，尤其是反向代理正向代理和反向代理： Nginx在做反向代理时，提供性能稳定，并且能够提供配置灵活的转发功能。Nginx可以根据不同的正则匹配，采取不同的转发策略，比如图片文件结尾的文件服务器，动态页面web服务器，只要你正则写的没问题，又有相对应的服务器解决方案，你就可以随心所欲的玩。并且Nginx对返回结果进行错误页跳转，异常判断等。如果被分发的服务器存在异常，他可以将请求重新转发给另外一台服务器，然后自动去除异常服务器。 2、负载均衡Nginx提供的负载均衡策略有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash。扩展策略，就天马行空，只有你想不到的没有他做不到的啦，你可以参照所有的负载均衡算法，给他一一找出来做下实现。 上3个图，理解这三种负载均衡算法的实现 Ip hash算法，对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。 3、web缓存Nginx可以对不同的文件做不同的缓存处理，配置灵活，并且支持FastCGI_Cache，主要用于对FastCGI的动态程序进行缓存。配合着第三方的ngx_cache_purge，对制定的URL缓存内容可以的进行增删管理。 4、Nginx相关地址源码： 官网： Nginx配置文件结构如果你下载好啦，你的安装文件，不妨打开conf文件夹的nginx.conf文件，Nginx服务器的基础配置，默认的配置也存放在此。 在nginx.conf的注释符号为：# 默认的nginx配置文件nginx.conf内容如下： nginx 文件结构 1、全局块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。 2、events块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。 3、http块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。 4、server块：配置虚拟主机的相关参数，一个http中可以有多个server。 5、location块：配置请求的路由，以及各种页面的处理情况。 下面给大家上一个配置文件，作为理解。 上面是nginx的基本配置，需要注意的有以下几点： 1、几个常见配置项 2、惊群现象：一个网路连接到来，多个睡眠的进程被同时叫醒，但只有一个进程能获得链接，这样会影响系统性能。 3、每个指令必须有分号结束。 原文地址： "},{"title":"python多进程基本原理","date":"2021-05-17T12:04:49.000Z","url":"/2021/05/17/python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["python","/tags/python/"],["multiprocessing","/tags/multiprocessing/"]],"categories":[["Python","/categories/Python/"]],"content":"多进程的含义多进程（Process）是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是系统进行资源分配和调度的一个独立单位。 顾名思义，多进程就是启用多个进程同时运行。由于进程是线程的集合，而且进程是由一个或多个线程构成的，所以多进程的运 行意味着有大于或等于进程数量的线程在运行。 Python多进程的优势由于进程中GIL的存在，Python中的多线程并不能很好地发挥多核优势，一个进程中的多个线程，在同一时刻只能有一个线程运行。 对于多进程来说，每个进程都有属于自己的GIL，所以，在多核处理器下，多进程的运行是不会受GIL的影响的。因此，多进程能更好地发挥多核的优势。 当然，对于爬虫这种IO密集型任务来说，多线程和多进程影响差别并不大。对于计算密集型任务来说，Python的多进程相比多线程，其多核运行效率会有成倍的提升。 总的来说，Python的多进程整体来看是比多线程更有优势的。所以，在条件允许的情况下，能用多进程就尽量用多进程。 不过值得注意的是，由于进程是系统进行资源分配和调度的一个独立单位，所以各个进程之间的数据是无法共享的，如多个进程无法共享一个全局变量，进程之间的数据共享需要有单独的机制来实现到。 多进程的实现在Python中也有内置的库来实现多进程，它就是multiprocessing。 multiprocessing提供了一系列的组件，如Process（进程、Queue（队列）、Semaphore（信号量）、Pipe（管道）、Lock（锁）、Pool（进程池）等，接下来让我们来了解下它们的使用方法。 直接使用Process类在multiprocessing中，每一个进程都用一个Process类来表示。它的API调用如下： target表示调用对象，你可以传入方法的名字。 args表示被调用对象的位置参数元组，比如target是函数func，他有两个参数m，n，那么args就传入[m, n]即可。 kwargs表示调用对象的字典。 name是别名，相当于给这个进程取一个名字。 group分组。 先用一个实例来感受一下： 这是一个实现多进程最基础的方式：通过创建Process来新建一个子进程，其中target参数传入方法名，args是方法的参数，是以 元组的形式传入，其和被调用的方法process的参数是一一对应的。 注意：这里args 必须要是一个元组，如果只有一个参数，那也要在元组第一个元素后面加一个逗号，如果没有逗号则和单个元素本身没有区别，无法构成元组，导致参数传递出现问题。 创建完进程之后，我们通过调用start方法即可启动进程了。运行结果如下： 运行了5个子进程，每个进程都调用了process方法。process方法的index参数通过Process的args传入，分别是0~4这5个序号，最后打印出来，5个子进程运行结束。 由于进程是Python中最小的资源分配单元，因此这些进程和线程不同，各个进程之间的数据是不会共享的，每启动一个进程，都会独立分配资源。 在当前CPU核数足够的情况下，这些不同的进程会分配给不同的CPU核来运行，实现真正的并行执行。 运行结果如下： 通过cpu_count成功获取了CPU核心的数量：4个，不同的机器结果可能不同。通过 active_children获取到了当前正在活跃运行的进程列表。然后遍历每个进程，并将它们的名称和进程号打印出来了，这里进程号直接使用pid属性即可获取，进程名称直接通过name属性即可获取。 继承继Process类在上面的例子中，创建进程是直接使用Process这个类来创建的，这是一种创建进程的方式。不过，创建进程的方式不止这一 种，同样，也可以像线程Thread一样来通过继承的方式创建一个进程类，进程的基本操作我们在子类的run方法中实现即可。 通过一个实例来看一下： 声明了一个构造方法，这个方法接收一个loop参数，代表循环次数，并将其设置为全局变量。在run方法中，又使用这个loop变量循环了loop次并打印了当前的进程号和循环次数。 在调用时，用range方法得到了2、3、4三个数字，并把它们分别初始化了MyProcess进程，然后调用start方法将进程启动起来。 注意：这里进程的执行逻辑需要在run方法中实现，启动进程需要调用start方法，调用之后run方法便会执行。 运行结果如下： 三个进程分别打印出了2、3、4条结果，即进程13560打印了2次结果，进程9908 打印了3次结果，进程13728打印了4次结果。 注意，这里的进程pid代表进程号，不同机器、不同时刻运行结果可能不同。 通过上面的方式，非常方便地实现了一个进程的定义。为了复用方便，可以把一些方法写在每个进程类里封装好，在使用时直接初始化一个进程类运行即可。 守护进程在多进程中，同样存在守护进程的概念，如果一个进程被设置为守护进程，当父进程结束后，子进程会自动被终止，我们可以通过设置daemon属性来控制是否为守护进程。 还是原来的例子，增加了deamon属性的设置： 运行结果如下： 结果很简单，因为主进程没有做任何事情，直接输出一句话结束，所以在这时也直接终止了子进程的运行。 这样可以有效防止无控制地生成子进程。这样的写法可以让我们在主进程运行结束后无需额外担心子进程是否关闭，避免了独立子进程的运行。 进程等待上面的运行效果其实不太符合我们预期：主进程运行结束时，子进程（守护进程）也都退出了，子进程什么都没来得及执行。 能不能让所有子进程都执行完了然后再结束呢？当然是可以的，只需要加入join方法即可，可以将代码改写如下： 运行结果如下： 在调用start和join方法后，父进程就可以等待所有子进程都执行完毕后，再打印出结束的结果。 默认情况下，join是无限期的。也就是说，如果有子进程没有运行完毕，主进程会一直等待。这种情况下，如果子进程出现问题陷入了死循环，主进程也会无限等待下去。怎么解决这个问题呢？可以给join方法传递一个超时参数，代表最长等待秒数。如果子进程没有在这个指定秒数之内完成，会被强制返回，主进程不再会等待。也就是说这个参数设置了主进程等待该子进程的最长时间。 例如这里传入1，代表最长等待1秒，代码改写如下： 运行结果如下： 可以看到，有的子进程本来要运行3秒，结果运行1秒就被强制返回了，由于是守护进程，该子进程被终止了。 终止进程终止进程不止有守护进程这一种做法，我们也可以通过terminate方法来终止某个子进程，另外我们还可以通过is_alive方法判断进程是否还在运行。 下面看一个实例： 用Process创建了一个进程，接着调用start方法启动这个进程，然后调用terminate方法将进程终止，最后调用join方法。 另外，在进程运行不同的阶段，通过is_alive方法判断当前进程是否还在运行。 运行结果如下： 这里有一个值得注意的地方，在调用terminate方法之后，我们用is_alive方法获取进程的状态发现依然还是运行状态。在调用join方法之后，is_alive方法获取进程的运行状态才变为终止状态。 所以，在调用terminate方法之后，记得要调用一下join方法，这里调用join方法可以为进程提供时间来更新对象状态，用来反映出最终的进程终止效果。 进程互斥锁 在访问一些临界区资源时，使用Lock可以有效避免进程同时占用资源而导致的一些问题。 信号量｀multiprocessing库中的Semaphore`来实现信号量，实现多个进程共享资源，同时限制可访问的进程数量。 道管管道（Pipe）用来实现进程之间的通讯，管道可以是单向的，即half-duplex：一个进程负责发消息，另一个进程负责收消息；也可以是双向的duplex，即互相收发消息。 默认声明Pipe对象是双向管道，如果要创建单向管道，可以在初始化的时候传入deplex参数为False。 声明了一个默认为双向的管道，然后将管道的两端分别传给两个进程。两个进程互相收发。观察一下结果： 管道Pipe就像进程之间搭建的桥梁，利用它可以很方便地实现进程间通信。 进程池 假如现在我们遇到这么一个问题，我有10000个任务，每个任务需要启动一个进程来执行，并且一个进程运行完毕之后要紧接着 启动下一个进程，同时我还需要控制进程的并发数量，不能并发太高，不然CPU处理不过来（如果同时运行的进程能维持在一个 最高恒定值当然利用率是最高的）。 那么我们该如何来实现这个需求呢？ 用Process和Semaphore可以实现，但是实现起来比较烦琐。这种需求在平时又是非常常见的。此时，我们就可以派上进程池了，即multiprocessing中的Pool。 Pool可以提供指定数量的进程，供用户调用，当有新的请求提交到pool中时，如果池还没有满，就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来执行它。 用一个实例来实现一下，代码如下： 声明了一个大小为3的进程池，通过processes参数来指定，如果不指定，那么会自动根据处理器内核来分配进程数。接着我们使用apply_async方法将进程添加进去，args可以用来传递参数。 运行结果如下： 进程池大小为3，可以看到有3个进程同时执行，第4个进程在等待，在有进程运行完毕之后，第4个进程马上跟着运行，出现了如上的运行效果。 最后，我们要记得调用close方法来关闭进程池，使其不再接受新的任务，然后调用join方法让主进程等待子进程的退出，等子进程运行完毕之后，主进程接着运行并结束。 上面的写法多少有些烦琐，使用你进程池的map方法，可以将上述写法简化很多。 map方法是怎么用的呢？第一个参数就是要启动的进程对应的执行方法，第2个参数是一个可迭代对象，其中的每个元素会被传递给这个执行方法。 举个例子：现在有一个list，里面包含了很多URL，定义了一个方法用来抓取每个URL内容并解析，那么可以直接在map的第一个参数传入方法名，第2个参数传入URL数组。 用一个实例来感受一下： 运行结果： 这样，就可以实现3个进程并行运行。不同的进程相互独立地输出了对应的爬取结果。"},{"title":"python多线程基本原理","date":"2021-05-12T03:42:12.420Z","url":"/2021/05/12/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["python","/tags/python/"],["多线程","/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"],["threading","/tags/threading/"]],"categories":[["Python","/categories/Python/"]],"content":"多线程的含义说起多线程，就不得不先说什么是线程。然而想要弄明白什么是线程，又不得不先说什么是进程。 进程可以理解为是一个可以独立运行的程序单位，比如打开一个浏览器，这就开启了一个浏览器进程；打开一个文本编辑器，这就开启了一个文本编辑器进程。但一个进程中是可以同时处理很多事情的，比如在浏览器中，可以在多个选项卡中打开多个页面，有的页面在播放音乐，有的页面在播放视频，有的网页在播放动画，它们可以同时运行，互不干扰。为什么能同时做到同时运行这么多的任务呢？这里就需要引出线程的概念了，其实这一个个任务，实际上就对应着一个个线程的执行。 而进程呢？它就是线程的集合，进程就是由一个或多个线程构成的，线程是操作系统进行运算调度的最小单位，是进程中的一个最小运行单元。比如上面所说的浏览器进程，其中的播放音乐就是一个线程，播放视频也是一个线程，当然其中还有很多其他的线程在同时运行，这些线程的并发或并行执行最后使得整个浏览器可以同时运行这么多的任务。 了解了线程的概念，多线程就很容易理解了，多线程就是一个进程中同时执行多个线程，前面所说的浏览器的情景就是典型的多线程执行。 并发和并行并说到多进程和多线程，这里就需要再讲解两个概念，那就是并发和并行。知道，一个程序在计算机中运行，其底层是处理器通过运行一条条的指令来实现的。 并发，英文叫作concurrency。它是指同一时刻只能有一条指令执行，但是多个线程的对应的指令被快速轮换地执行。比如一个处理器，它先执行线程A的指令一段时间，再执行线程B的指令一段时间，再切回到线程A执行一段时间。 由于处理器执行指令的速度和切换的速度非常非常快，人完全感知不到计算机在这个过程中有多个线程切换上下文执行的操作，这就使得宏观上看起来多个线程在同时运行。但微观上只是这个处理器在连续不断地在多个线程之间切换和执行，每个线程的执行一定会占用这个处理器一个时间片段，同一时刻，其实只有一个线程在执行。 并行，英文叫作parallel。它是指同一时刻，有多条指令在多个处理器上同时执行，并行必须要依赖于多个处理器。不论是从宏观上还是微观上，多个线程都是在同一时刻一起执行的。 并行只能在多处理器系统中存在，如果的计算机处理器只有一个核，那就不可能实现并行。而并发在单处理器和多处理器系统中都是可以存在的，因为仅靠一个核，就可以实现并发。 举个例子，比如系统处理器需要同时运行多个线程。如果系统处理器只有一个核，那它只能通过并发的方式来运行这些线程。如果系统处理器有多个核，当一个核在执行一个线程时，另一个核可以执行另一个线程，这样这两个线程就实现了并行执行，当然其他的线程也可能和另外的线程处在同一个核上执行，它们之间就是并发执行。具体的执行方式，就取决于操作系统的调度了。 多线程适用场景多在一个程序进程中，有一些操作是比较耗时或者需要等待的，比如等待数据库的查询结果的返回，等待网页结果的响应。如果使用单线程，处理器必须要等到这些操作完成之后才能继续往下执行其他操作，而这个线程在等待的过程中，处理器明显是可以来执行其他的操作的。如果使用多线程，处理器就可以在某个线程等待的时候，去执行其他的线程，从而从整体上提高执行效率。 像上述场景，线程在执行过程中很多情况下是需要等待的。比如网络爬虫就是一个非常典型的例子，爬虫在向服务器发起请求之后，有一段时间必须要等待服务器的响应返回，这种任务就属于IO密集型任务。对于这种任务，如果启用多线程，处理器就可以在某个线程等待的过程中去处理其他的任务，从而提高整体的爬取效率。 但并不是所有的任务都是IO密集型任务，还有一种任务叫作计算密集型任务，也可以称之为CPU密集型任务。顾名思义，就是任务的运行一直需要处理器的参与。此时如果开启了多线程，一个处理器从一个计算密集型任务切换到切换到另一个计算密集型任务上去，处理器依然不会停下来，始终会忙于计算，这样并不会节省总体的时间，因为需要处理的任务的计算总量是不变的。如果线程数目过多，反而还会在线程切换的过程中多耗费一些时间，整体效率会变低。 所以，如果任务不全是计算密集型任务，可以使用多线程来提高程序整体的执行效率。尤其对于网络爬虫这种IO密集型任务来说，使用多线程会大大提高程序整体的爬取效率。 Python实现多线程实在Python中，实现多线程的模块叫作threading，是Python自带的模块。使用threading实现多线程的方法。 ###Thread直接创建子线程 首先，可以使用Thread类来创建一个线程，创建时需要指定target参数为运行的方法名称，如果被调用的方法需要传入额外的参数，则可以通过Thread的args参数来指定。示例如下： 运行结果如下： 在这里首先声明了一个方法，叫作target，它接收一个参数为second，通过方法的实现可以发现，这个方法其实就是执行了一个time.sleep休眠操作，second参数就是休眠秒数，其前后都print了一些内容，其中线程的名字通过threading.current_thread().name来获取出来，如果是主线程的话，其值就是MainThread，如果是子线程的话，其值就是Thread-*。 然后通过Thead类新建了两个线程，target参数就是刚才所定义的方法名，args以列表的形式传递。两次循环中，这里i分别就是1和5，这样两个线程就分别休眠1秒·和5秒，声明完成之后，调用start方法即可开始线程的运行。 观察结果可以发现，这里一共产生了三个线程，分别是主线程MainThread和两个子线程Thread-1、Thread-2。另外观察到，主线程首先运行结束，紧接着Thread-1、Thread-2才接连运行结束，分别间隔了1秒和4秒。这说明主线程并没有等待子线程运行完毕才结束运行，而是直接退出了，有点不符合常理。如果想要主线程等待子线程运行完毕之后才退出，可以让每个子线程对象都调用下join方法，实现如下： 运行结果如下： 主线程必须等待子线程都运行结束，主线程才继续运行并结束。 继承Thread类创建子线程另外，也可以通过继承Thread类的方式创建一个线程，该线程需要执行的方法写在类的run方法里面即可。上面的例子的等价改写为： 运行结果如下： 可以看到，两种实现方式，其运行效果是相同的。 守护进程在线程中有一个叫作守护线程的概念，如果一个线程被设置为守护线程，那么意味着这个线程是“不重要”的，这意味着，如果主线程结束了而该守护线程还没有运行完，那么它将会被强制结束。在Python中可以通过setDaemon方法来将某个线程设置为守护线程。 示例如下： 在这里通过 setDaemon方法将 t2 设置为了守护线程，这样主线程在运行完毕时，t2 线程会随着线程的结束而结束。 运行结果如下： 可以看到，没有Thread-2打印退出的消息，Thread-2随着主线程的退出而退出了。 这里并没有调用join方法，如果让t1和t2都调用join方法，主线程就会仍然等待各个子线程执行完毕再退出，不论其是否是守护线程。 互斥锁互在一个进程中的多个线程是共享资源的，比如在一个进程中，有一个全局变量count用来计数，声明多个线程，每个线程运行时都给count加1，代码实现如下： 在这里，声明了1000个线程，每个线程都是现取到当前的全局变量count值，然后休眠一小段时间，然后对count赋予新的值。 按照常理来说，最终的count值应该为1000。但其实不然。运行结果如下： 最后的结果居然只有15，而且多次运行或者换个环境运行结果是不同的.这是为什么呢？因为count这个值是共享的，每个线程都可以在执行temp = count这行代码时拿到当前count的值，但是这些线程中的一些线程可能是并发或者并行执行的，这就导致不同的线程拿到的可能是同一个count值，最后导致有些线程的count的加1操作并没有生效，导致最后的结果偏小。 所以，如果多个线程同时对某个数据进行读取或修改，就会出现不可预料的结果。为了避免这种情况，我们需要对多个线程进行同步，要实现同步，我们可以对需要操作的数据进行加锁保护，这里就需要用到threading.Lock了。 加锁保护是什么意思呢？就是说，某个线程在对数据进行操作前，需要先加锁，这样其他的线程发现被加锁了之后，就无法继续向下执行，会一直等待锁被释放，只有加锁的线程把锁释放了，其他的线程才能 继续加锁并对数据做修改，修改完了再释放锁。这样可以确保同一时间只有一个线程操作数据，多个线程不会再同时读取和修改同一个数据，这样最后的运行结果就是对的了。 将代码修改为如下内容： 运行结果如下： Python多线程的问题由于Python中GIL的限制，导致不论是在单核还是多核条件下，在同一时刻只能运行一个线程，导致Python多线程无法发挥多核并行的优势。 GIL全称为GlobalInterpreter Lock，中文翻译为全局解释器锁，其最初设计是出于数据安全而考虑的。 在Python多线程下，每个线程的执行方式如下： 获取GIL执行对应线程的代码 释放GIL可见，某个线程想要执行，必须先拿到 GIL，我们可以把 GIL看作是通行证，并且在一个 Python进程中，GIL只有一个。拿不到通行证的线程，就不允许执行。这样就会导致，即使是多核条件下，一个 Python 进程下的多个线程，同一时刻也只能执行一个线程。 不过对于爬虫这种 IO 密集型任务来说，这个问题影响并不大。而对于计算密集型任务来说，由于 GIL的存在，多线程总体的运行效率相比可能反而比单线程更低 "},{"title":"Session和Cookies","date":"2021-05-12T02:51:36.000Z","url":"/2021/05/12/Session%E5%92%8CCookies/","tags":[["http","/tags/http/"],["web","/tags/web/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"静态网页和动态网页示例代码： 这是最基本的HTML代码，保存为一个.html文件，然后把它放在某台具有固定公网IP的主机上，主机上装上Apache或Nginx等服务器，这样这台主机就可以作为服务器了，其他人便可以通过访问服务器看到这个页面，这就搭建了一个最简单的网站。 这种网页的内容是HTML代码编写的，文字、图片等内容均通过写好的HTML代码来指定，这种页面叫作静态网页。它加载速度快，编写简单，但是存在很大的缺陷，如可维护性差，不能根据URL灵活多变地显示内容等。例如，想要给这个网页的URL传入一个name参数，让其在网页中显示出来，是无法做到的。 因此，动态网页应运而生，它可以动态解析URL中参数的变化，关联数据库并动态呈现不同的页面内容，非常灵活多变。现在遇到的大多数网站都是动态网站，它们不再是一个简单的HTML，而是可能由JSP、PHP、Python等语言编写的，其功能比静态网页强大和丰富太多了。 此外，动态网站还可以实现用户登录和注册的功能。按照一般的逻辑来说，输入用户名和密码登录之后，肯定是拿到了一种类似凭证的东西，有了它，才能保持登录状态，才能访问登录之后才能看到的页面。 那么，这种神秘的凭证到底是什么呢？其实它就是 Session和Cookies 共同产生的结果。 无状态HTTP在了解Session和Cookies之前，还需要了解HTTP的一个特点，叫作无状态。 HTTP的无状态是指HTTP协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。 当向服务器发送请求后，服务器解析此请求，然后返回对应的响应，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录。 这意味着如果后续需要处理前面的信息，则必须重传，这也导致需要额外传递一些前面的重复请求，才能获取后续响应，这种效果显然不是想要的。为了保持前后状态，肯定不能将前面的请求全 部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。 这时两个用于保持HTTP连接状态的技术就出现了，它们分别是Session和Cookies。Session在服务端，也就是网站的服务器，用来保存用户的Session信息；Cookies在客户端，也可以理解为浏览器端，有了Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别Cookies并鉴定出是哪个用户，然后再判断用户是否是登录状态，进而返回对应的响应。 可以理解为Cookies里面保存了登录的凭证，有了它，只需要在下次请求携带Cookies发送请求而不必重新输入用户名、密码等信息重新登录了。 因此在爬虫中，有时候处理需要登录才能访问的页面时，一般会直接将登录成功后获取的Cookies放在请求头里面直接请求，而不必重新模拟登录。 CookiesCookies指某些网站为了辨别用户身份、进行Session跟踪而存储在用户本地终端上的数据。 SessionSession，中文称之为会话，其本身的含义是指有始有终的一系列动作/消息。比如，打电话时，从拿起电话拨号到挂断电话这中间的一系列过程可以称为一个 Session。 而在Web中，Session对象用来存储特定用户Session所需的属性及配置信息。这样，当用户在应用程序的Web页之间跳转时，存储在Session对象中的变量将不会丢失，而是在整个用户Session中一直存在下去。当用户请求来自应用程序的Web页时，如果该用户还没有Session，则Web服务器将自动创建一个 Session对象。当Session过期或被放弃后，服务器将终止该Session。 Session维持那么，怎样利用Cookies保持状态呢？当客户端第一次请求服务器时，服务器会返回一个响应头中带有Set-Cookie字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把Cookies保存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies放到请求头一起提交给服务器，Cookies携带了SessionID 信息，服务器检查该Cookies即可找到对应的Session是什么，然后再判断Session来以此来辨认用户状态。 在成功登录某个网站时，服务器会告诉客户端设置哪些Cookies信息，在后续访问页面时客户端会把Cookies发送给服务器，服务器再找到对应的Session加以判断。如果Session中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。 反之，如果传给服务器的Cookies是无效的，或者Session已经过期了，将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录。 所以，Cookies和Session需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录Session控制。 Cookies属性结构Cookie有如下几个属性。 Name，即该Cookie的名称。 Cookie一旦创建，名称便不可更改。 Value，即该Cookie的值。如果值为Unicode字符，需要为字符编码。如果值为二进制数据，则需要使用BASE64编码。 MaxAge，即该Cookie失效的时间，单位秒，也常和Expires一起使用，通过它可以计算出其有效时间。 MaxAge 如果为正数，则该Cookie在MaxAge秒之后失效。如果为负数，则关闭浏览器时Cookie即失效，浏览器也不会以任何形式保存该Cookie。 Path，即该Cookie的使用路径。如果设置为/path/，则只有路径为/path/的页面可以访问该Cookie。如果设置为/，则本域名下的所有页面都可以访问该Cookie。 Domain，即可以访问该Cookie的域名。例如如果设置为.zhihu.com，则所有以zhihu.com，结尾的域名都可以访问该Cookie。 Size字段，即此Cookie的大小。 Http字段，即Cookie的httponly属性。若此属性为true，则只有在HTTP Headers中会带有此Cookie的信息，而不能通过document.cookie来访问此Cookie。 Secure，即该Cookie是否仅被使用安全协议传输。安全协议。安全协议有HTTPS、SSL等，在网络上传输数据之前先将数据加密。默认为false。 会话会Cookie和持久和Cookie从表面意思来说，会话Cookie就是把Cookie放在浏览器内存里，浏览器在关闭之后该Cookie即失效；持久Cookie则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态。 严格来说，没有会话Cookie和持久Cookie之分，只是由Cookie的MaxAge或Expires字段决定了过期的时间。 因此，一些持久化登录的网站其实就是把Cookie的有效时间和Session有效期设置得比较长，下次再访问页面时仍然携带之前的Cookie，就可以直接保持登录状态。 常见误区在谈论Session机制的时候，常常听到这样一种误解 ——“只要关闭浏览器，Session就消失了”。可以想象一下会员卡的例子，除非顾客主动对店家提出销卡，否则店家绝对不会轻易删除顾客的资料。对Session来说，也是一样，除非程序通知服务器删除一个Session，否则服务器会一直保留。比如，程序一般都是在做注销操作时才去删除Session。 但是当关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭。之所以会有这种错觉，是因为大部分网站都使用会话Cookie来保存Session ID信息，而关闭浏览器后Cookies就消失了，再次连接服务器时，也就无法找到原来的Session了。如果服务器设置的Cookies保存到硬盘上，或者使用某种手段改写浏览器发出的HTTP请求头，把原来的Cookies发送给服务器，则再次打开浏览器，仍然能够找到原来的Session ID，依旧还是可以保持登录状态的。 而且恰恰是由于关闭浏览器不会导致Session被删除，这就需要服务器为Session设置一个失效时间，当距离客户端上一次使用Session的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把Session删除以节省存储空间。"},{"title":"爬虫的基本原理","date":"2021-05-11T16:12:50.000Z","url":"/2021/05/12/%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["python","/tags/python/"],["http","/tags/http/"],["爬虫","/tags/%E7%88%AC%E8%99%AB/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。如果把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。 爬虫概述简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，概要介绍一下。 获取网页获爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。 源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。 前面讲了请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个 请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样实现呢？ Python提供了许多库来帮助我们实现这个操作，如urllib、requests等。可以用这些库来帮助我们实现HTTP请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的Body部分，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。 提取信息提获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。 另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS选择器或XPath来提取网页信息的库，如Beautiful Soup、pyquery、lxml等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。 提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。 保存数据提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为TXT文本或JSON文本，也可以保存到数据库，如MySQL和MongoDB等，还可保存至远程服务器，如借助SFTP进行操作等。 自动化程序自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各 种异常处理、错误重试等操作，确保爬取持续高效地运行。 能抓怎样的数据能在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML代码，而最常抓取的便是HTML源代码。 另外，可能有些网页返回的不是HTML代码，而是一个JSON字符串（其中API接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。 此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。 另外，还可以看到各种扩展名的文件，如CSS、JavaScript和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。 上述内容其实都对应各自的URL，是基于HTTP或HTTPS协议的，只要是这种数据，爬虫都可以抓取。 JavaScript渲染页面有时候，在用urllib或requests抓取网页时，得到的源代码实际和浏览器中看到的不一样。 这是一个非常常见的问题。现在网页越来越多地采用Ajax、前端模块化工具来构建，整个网页可能都是由JavaScript渲染出来的，也就是说原始的HTML代码就是一个空壳，例如： body节点里面只有一个id为container的节点，但是需要注意在body节点后引入了app.js，它便负责整个网站的渲染。 在浏览器中打开这个页面时，首先会加载这个HTML内容，接着浏览器会发现其中引入了一个app.js文件，然后便会接着去请求这个文件，获取到该文件后，便会执行其中的JavaScript代码，而JavaScript则会改变HTML中的节点，向其添加内容，最后得到完整的页面。 但是在用urllib或requests等库请求当前页面时，我们得到的只是这个HTML代码，它不会帮助我们去继续加载这个JavaScript文件，这样也就看不到浏览器中的内容。 这也解释了为什么有时我们得到的源代码和浏览器中看到的不一样。 因此，使用基本HTTP请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，可以分析其后台Ajax接口，也可使用Selenium、Splash这样的库来实现模拟JavaScript渲染。"},{"title":"web网页基础","date":"2021-05-11T15:32:38.000Z","url":"/2021/05/11/web%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80/","tags":[["html","/tags/html/"],["css","/tags/css/"],["javascript","/tags/javascript/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"网页的组成首先，我们来了解网页的基本组成，网页可以分为三大部分：HTML、CSS和JavaScript。 如果把网页比作一个人的话，HTML相当于骨架，JavaScript相当于肌肉，CSS相当于皮肤，三者结合起来才能形成一个完整的网页。下面我们来分别介绍一下这三部分的功能。 HTMLHTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。 我们浏览的网页包括文字、按钮、图片和视频等各种复杂的元素，其基础架构就是HTML。不同类型的元素通过不同类型的标签来表示，如图片用img标签表示，视频用video标签表示，段落用p标签表示，它们之间的布局又常通过布局标签div嵌套组合而成，各种标签通过不同的排列和嵌套就可以形成网页的框架。 在Chrome浏览器中打开百度，右击并选择 “检查”项（或按F12键），打开开发者模式，这时在Elements选项卡中即可看到网页的源代码，如图所示。 这就是HTML，整个网页就是由各种标签嵌套组合而成的。这些标签定义的节点元素相互嵌套和组合形成了复杂的层次关系，就形成了网页的架构。 CSS虽然HTML定义了网页的结构，但是只有HTML页面的布局并不美观，可能只是简单的节点元素的排列，为了让网页看起来更好看一些，这里就需要借助CSS了。 CSS，全称叫作Cascading Style Sheets，即层叠样式表。“层叠”是指当在HTML中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式”指网页中文字大小、颜色、元素间距、排列等 格式。 CSS是目前唯一的网页页面排版样式标准，有了它的帮助，页面才会变得更为美观。例如： 这就是一个CSS样式。大括号前面是一个CSS选择器。此选择器的作用是首先选中id为head_wrapper且class 为s-ps-islite的节点，然后再选中其内部的class为s-p-top的节点。大括号内部写的就是一条条样式规则，例如position指定了这个元素的布局方式为绝对布局，bottom指定元素的下边距为40像素，width指定了宽度为100%占满父元素，height则指定了元素的高度。也就是说，我们将位置、宽度、高度等样式配置统一写成这样的形式，然后用大括号括起来，接着在开头再加上CSS选择器，这就代表这个样式对CSS选择器选中的元素生效，元素就会根据此样式来展示了。 在网页中，一般会统一定义整个网页的样式规则，并写入CSS文件中（其后缀为css）。在HTML中，只需要用link标签即可引入写好的CSS文件，这样整个页面就会变得美观、优雅。 JavaScriptJavaScript，简称JS，是一种脚本语言。HTML和CSS配合使用，提供给用户的只是一种静态信息，缺乏交互性。我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是JavaScript的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系，而是实现了一种实时、动态、交互的页面功能。 JavaScript通常也是以单独的文件形式加载的，后缀为js，在 HTML中通过script标签即可引入，例如: 综上所述，HTML定义了网页的内容和结构，CSS描述了网页的布局，JavaScript定义了网页的行为。"},{"title":"在Hexo博客中嵌入外链视频","date":"2021-05-11T01:12:11.000Z","url":"/2021/05/11/%E5%9C%A8Hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%B5%8C%E5%85%A5%E5%A4%96%E9%93%BE%E8%A7%86%E9%A2%91/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"嵌入的视频Hexo支持Youtube视频的嵌入，可以参考其实现方式。 首先，在node_modules/hexo/lib/plugins/tag/index.js中添加以下代码。 然后在node_modules/hexo/lib/plugins/tag/目录下新建bilibili.js文件，打开并添加如下代码： 重新启动下Hexo Server,在md页面中添加下列： 重新刷新页面，就可以看到视频正常加载了。 "},{"title":"腾讯云配置ssl证书","date":"2021-05-10T07:36:11.000Z","url":"/2021/05/10/%E8%85%BE%E8%AE%AF%E4%BA%91%E9%85%8D%E7%BD%AEssl%E8%AF%81%E4%B9%A6/","tags":[["nginx","/tags/nginx/"],["https","/tags/https/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"什么是SSL证书？SSL证书是用于在Web服务器与浏览器以及客户端之间建立加密链接的加密技术，通过配置和应用SSL证书来启用HTTPS协议，来保证互联网数据传输的安全，全球每天有数以亿计的网站都是通过HTTPS来确保数据安全，保护用户隐私。 申请腾讯云SSL证书百毒搜索腾讯SSL证书，找到免费使用SSL一年的产品，一系列骚操作后得到证书。 nginx配置修改下载SSL证书，上传到服务器/etc/pki/nginx/目录下。 修改/etc/配置文件，注意SSL证书的路径和实际上传的路径和名称一致。 "},{"title":"Hexo在腾讯云的部署","date":"2021-05-10T04:51:56.000Z","url":"/2021/05/10/Hexo%E5%9C%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E7%9A%84%E9%83%A8%E7%BD%B2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"简介Hexo在GitHub pages上的访问太慢了，迁移到腾讯云服务器上。 部署环境腾讯云服务器（Centos 64位）。 服务器配置安装git 创建git用户并修改权限 找到一下内容 在该语句下添加 退出（esc + :wq）并修改权限 本地使用gitbash创建密钥 在腾讯云中创建ssh，并将本地的id_rsa.pub中的文件内容全部复制到authorized_keys中。 修改权限 本地测试 云服务器中创建网站目录并设置权限 安装nginx 以上执行完之后，在浏览器中输入你的公网IP如果可以进入CentOs界面，说明Nginx安装成功。 配置nginx 重启服务 建立git仓库并修改权限 同步网站根目录 填入如下内容 修改权限 在本地Hexo目录下修改_config.yml文件中的deploy后的repo改为： 以上全部完成后，执行hexo的部署命令即可完成在腾讯云服务器上的博客部署。"},{"title":"HTTP基本原理","date":"2021-05-09T12:48:23.000Z","url":"/2021/05/09/HTTP%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["http","/tags/http/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"URI和URLURI （Uniform Resource Identifier） 即统一资源标志符。URL （Uniform Resource Locator 即统一资源定位符。 例如：既是一个URL，也是一个URI。用URL/URI来唯一指定了它的访问方式，这其中包括了访问协议Https，访问路径（即根目录）和资源名称favicon.ico。 URL是URI的一个子集，也就是每个URL都是URI，但不是每个URI都是URL。 URI还包括一个子类叫做URN（Universal Resource Name）即统一资源名称。但是在目前的互联网，URN的使用非常少，几乎所有的 URI都是URL，所以一般的网页链接我们可以称之为 URL，也可以称之为 URI。 超文本Hypertext，我们在浏览器里看到的网页就是超文本解析而成的，其网页源代码是一系列 HTML代码，里面包含了一系列标签，比如img显示图片，p指定显示段落等。浏览器解析这些标签后，便形成了我们平常看到的网页，而网页的源代码HTML就可以称作超文本。 HTTP和HTTPSHTTP的全称是Hyper Text Transfer Protocol，中文名叫作超文本传输协议，HTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。HTTP由万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet EngineeringTask Force）共同合作制定的规范，目前广泛使用的是HTTP 1.1版本。 HTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，简称为HTTPS。 HTTPS的安全基础是SSL，因此通过它传输的内容都是经过SSL加密的，它的主要作用可以分为两种： 建立一个信息安全通道，来保证数据传输的安全。 确认网站的真实性，凡是使用了HTTPS的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过CA机构颁发的安全签章来查询。 现在越来越多的网站和 App 都已经向 HTTPS 方向发展。例如： 苹果公司强制所有 iOS App 在 2017 年 1 月 1 日 前全部改为使用 HTTPS 加密，否则 App 就无法在应用商店上架。 谷歌从 2017 年 1 月推出的 Chrome 56 开始，对未进行 HTTPS 加密的网址链接亮出风险提示，即在地址栏的显著位置提醒用户 “此网页不安全”。 腾讯微信小程序的官方需求文档要求后台使用 HTTPS 请求进行网络通信，不满足条件的域名和协议无法请求。 因此，HTTPS 已经已经是大势所趋。 HTTP请求过程我们在浏览器中输入一个URL，回车之后便可以在浏览器中观察到页面内容。实际上，这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器。响应里包含了页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来，传输模型如图所示： 此处客户端即代表我们自己的 PC 或手机浏览器，服务器即要访问的网站所在的服务器。 为了更直观地说明这个过程，这里用浏览器的开发者模式下的Network监听组件来做演示，它可以显示访问当前请求网页时发生的所有网络请求和响应。打开浏览器（Chrome或Edge都可以），右击并选择 “检查”项，即可打开浏览器的开发者工具。这里访问百度，输入该 URL后回车，观察这个过程中发生了怎样的网络请求。可以看到，在Network页面下方 出现了一个个的条目，其中一个条目就代表一次发送请求和接收响应的过程，如图所示: 我们先观察第一个网络请求，即www.baidu.com，其中各列的含义如下。 第一列 Name：请求的名称，一般会将 URL的最后一部分内容当作名称。 第二列 Status：响应的状态码，这里显示为 200，代表响应是正常的。通过状态码，我们可以判断发送了请求之后是否得到了正常的响应。 第三列 Type：请求的文档类型。这里为document，代表我们这次请求的是一个 HTML文档，内容就是一些 HTML代码。 第四列 Initiator：请求源。用来标记请求是由哪个对象或进程发起的。 第五列 Size：从服务器下载的文件和请求的资源大小。如果是从缓存中取得的资源，则该列会显示 fromcache。 第六列 Time：发起请求到获取响应所用的总时间。 第七列 Waterfall：网络请求的可视化瀑布流。 我们点击这个条目即可看到其更详细的信息，如图所示。 首先是General部分，Request URL为请求的URL，Request Method为请求的方法，Status Code为响应状态码，Remote Address为远程服务器的地址和端口，Referrer Policy为 Referrer判别策略。 再继续往下，可以看到，有Response Headers和Request Headers，这分别代表响应头和请求头。请求头里带有许多请求信息，例如浏览器标识、Cookies、Host等信息，这是请求的一部分，服务器会根据请求头内的信息判断请求是否合法，进而作出对应的响应。图中看到的Response Headers就是响应的一部分，例如其中包含了服务器的类型、文档类型、日期等信息，浏览器接受到响应后，会解析响应内容，进而呈现网页内容。 请求请求，由客户端向服务端发出，可以分为4部分内容：请求方法（Request Method、请求的网址（Request URL）、请求头（Request Headers）、请求体（Request Body）。 请求方法常见的请求方法有两种：GET和POST。 在浏览器中直接输入URL并回车，这便发起了一个GET请求，请求的参数会直接包含到URL里。例如，在百度中搜索Python，这就是一个GET请求，链接为，其中URL中包 含了请求的参数信息，这里参数wd表示要搜寻的关键字。POST请求大多在表单提交时发起。比如，对于一个登录表单，输入用户名和密码后，点击“登录”按钮，这通常会发起一个POST请求，其数据通常 以表单的形式传输，而不会体现在URL中。 GET和POST请求方法有如下区别。 GET请求中的参数包含在URL里面，数据可以在URL中看到，而POST请求的URL不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。 GET请求提交的数据最多只有1024字节，而POST请求没有限制。 一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用GET方式请求的话，密码就会暴露在URL里面，造成密码泄露，所以这里最好以POST方式发送。上传文件时，由于文件内容比较大，也会选用POST方式。 我们平常遇到的绝大部分请求都是GET或POST请求，另外还有一些请求方法，如HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE等，我们简单将其总结为下表。 方法 描述 GET 请求页面，并返回页面内容 HEAD 类似于GET请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 大多用于表单提交或上传文件，数据包含在请求体中 PUT 从客户端向服务器传送的数据取代指定文档中的内容 DELETE 请求服务器删除指定的页面 CONNECT 把服务器当作跳板，让服务器代替客户端访问其他网页 OPTIONS 允许客户端查看服务器的性能 TRACE 回显服务器收到的请求，主要用于测试或诊断 table { margin: auto; font-size: 50%; } 请求的网址，即统一资源定位符 URL，它可以唯一确定我们想请求的资源。 请求头请求头，用来说明服务器要使用的附加信息，比较重要的信息有Cookie、Referer、User-Agent等。下面简要说明一些常用的头信息。 Accept：请求报头域，用于指定客户端可接受哪些类型的信息。 Accept-Language：指定客户端可接受的语言类型。 Accept-Encoding：指定客户端可接受的内容编码。 Host：用于指定请求资源的主机IP和端口号，其内容为请求URL的原始服务器或网关的位置。从 HTTP 1.1 版本开始，请求必须包含此内容。 Cookie：也常用复数形式Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会 话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是Cookies的功劳。Cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页 面时，都会在请求头中加上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。 Referer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、防盗链处理等。 User-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫。 Content-Type：也叫互联网媒体类型（Internet Media Type）或者MIME类型，在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html代表HTML格式，image/gif代表GIF图片，application/json代表JSON类型，更多对应关系可以查看此对照表：。 因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。 请求体请请求体一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。 登录之前，我们填写了用户名和密码信息，提交时这些内容就会以表单数据的形式提交给服务器，此时需要注意Request Headers中指定Content-Type为application/x-www-form-urlencoded。只有设置Content-Type为application/x-www-form-urlencoded，才会以表单数据的形式提交。另外，我们也可以将Content-Type设置为application/json来提交JSON数据，或者设置为multipart/form-data来上传文件。 表格中列出了Content-Type和POST提交数据方式的关系。 Content-Type 提交数据的方式 application/x-www-form-urlencodeed 表单数据 multipart/form-data 表单文件上传 application/json 序列化JSON数据 text/xml XML数据 在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type，不然可能会导致POST提交后无法正常响应。 响应响应，由服务端返回给客户端，可以分为三部分：响应状态码（Response Status Code）、响应头（Response Headers）和响应体（Response Body）。 响应状态码响应状态码表示服务器的响应状态，如200代表服务器正常响应，404代表页面未找到，500代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为200，则证明成功返 回数据，再进行进一步的处理，否则直接忽略。下表列出了常见的错误代码及错误原因。 状态码 说明 详情 100 继续 请求者应当继续提出请求，服务器已收到请求的一部分，正在等待其余部分 101 切换协议 请求者已要求服务器切换协议，服务器已确认并确认切换 200 成功 服务器已成功处理了请求 201 已创建 请求成功并且服务器创建了新的资源 202 已接受 服务器已接受请求，但尚未处理 203 非授权信息 服务器已经成功处理请求，但返回的信息可能来自另一个源 204 无内容 服务器成功处理了请求，但没有返回任何内容 205 重置内容 服务器成功处理了请求，但内容被重置 206 部分内容 服务器成功处理了部分请求 300 多种选择 针对请求，服务器可执行多种操作 301 永久移动 请求的网页已永久移动到新位置，即永久重定向 302 临时移动 请求的网页暂时跳转到其他页面，即暂时重定向 303 查看其他位置 如果原来的请求是POST，重定向目标文档应当通过GET方式访问资源 304 未修改 此次请求返回的网页未修改，继续使用上次的资源 305 使用代理 请求者应该使用代理访问该网页 307 临时重定向 请求的资源临时从其他位置响应 400 错误请求 服务器无法解析该请求 401 未授权 请求没有进行身份验证或验证未通过 403 禁止访问 服务器拒绝此请求 404 未找到 服务器找不到请求的网页 405 方法禁用 服务器禁用了请求中指定的方法 406 不接受 无法使用请求的内容响应请求的网页 407 需要代理授权 请求者需要使用代理授权 408 请求超时 服务器请求超时 409 冲突 服务器在完成请求时发生冲突 410 已删除 请求的资源已永久删除 411 需要有效长度 服务器不接受不含有效内容长度标头字段的请求 412 未满足前提条件 服务器未满足请求者在请求中设置的其中一个前提条件 413 请求实体过大 请求实体过大，超出服务器的处理能力 415 请求URL过长 请求的网址过长，服务器无法处理 416 请求范围不符 页面无法提供请求页面支持 417 未满足期望值 服务器为满足期望请求标头字段的要求 500 服务器内部错误 服务器遇到错误，无法完成请求 501 未实现 服务器不具备完成请求的功能 502 错误网关 服务器作为网关或代理，从上游服务器收到无效响应 503 服务不可用 服务器目前无法使用 504 网关超时 服务器作为网关或代理，但是没有及时从上游服务器收到响应 505 HTTP版本不支持 服务器不支持请求中所用的HTTP协议版本 响应头包含了服务器对请求的应答信息，如Content-Type、Server、Set-Cookie等。下面简要说明一些常用的响应头信息。 Date：标识响应产生的时间。 Last-Modified：指定资源的最后修改时间。 Content-Encoding：指定响应内容的编码。 Server：包含服务器的信息，比如名称、版本号等。 Content-Type：文档类型，指定返回的数据类型是什么，如text/html代表返回HTML文档，application/x-javascript则代表返回JavaScript文件，image/jpeg则代表返回图片。 Set-Cookie：设置Cookies。响应头中的Set-Cookie告诉浏览器需要将此内容放在Cookies中，下次请求携带Cookies请求。 Expires：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。 响应体最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体，如图所示。 在浏览器开发者工具中点击Response，就可以看到网页的源代码，也就是响应体的内容，它是解析的目标。在做爬虫时，我们主要通过响应体得到网页的源代码、JSON数据等，然后从中做相应内容的提取。"},{"title":"Git常用命令","date":"2021-05-08T10:03:19.000Z","url":"/2021/05/08/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":[["Git","/tags/Git/"]],"categories":[["Git","/categories/Git/"]],"content":"Git常用命令仓库在当前目录新建一个Git代码库 新建一个目录，将其初始化为Git代码库 下载一个项目和它的整个代码历史 配置显示当前的Git配置 编辑Git配置文件 设置提交代码时的用户信息 增加/删除文件添加指定文件到暂存区 添加指定目录到暂存区，包括子目录 添加当前目录的所有文件到暂存区 添加每个变化前，都会要求确认对于同一个文件的多处变化，可以实现分次提交 停止追踪指定文件，但该文件会保留在工作区 改名文件，并且将这个改名放入暂存区 代码提交提交暂存区到仓库区 提交暂存区的指定文件到仓库区 提交工作区自上次commit之后的变化，直接到仓库区 提交时显示所有diff信息 使用一次新的commit，替代上一次提交,如果代码没有任何新变化，则用来改写上一次commit的提交信息 重做上一次commit，并包括指定文件的新变化 分支列出所有本地分支 列出所有远程分支 列出所有本地分支和远程分支 新建一个分支，但依然停留在当前分支 新建一个分支，并切换到该分支 新建一个分支，指向指定commit 新建一个分支，与指定的远程分支建立追踪关系 切换到指定分支，并更新工作区 切换到上一个分支 建立追踪关系，在现有分支与指定的远程分支之间 合并指定分支到当前分支 选择一个commit，合并进当前分支 删除分支 删除远程分支 标签列出所有tag 新建一个tag在当前commit 新建一个tag在指定commit 删除本地tag 删除远程tag 查看tag信息 提交指定tag 提交所有tag 新建一个分支，指向某个tag 查看信息显示有变更的文件 显示当前分支的版本历史 显示commit历史，以及每次commit发生变更的文件 搜索提交历史，根据关键词 显示某个commit之后的所有变动，每个commit占据一行 显示某个commit之后的所有变动，其”提交说明”必须符合搜索条件 显示某个文件的版本历史，包括文件改名 显示指定文件相关的每一次diff 显示过去5次提交 显示所有提交过的用户，按提交次数排序 显示指定文件是什么人在什么时间修改过 显示暂存区和工作区的差异 显示暂存区和上一个commit的差异 显示工作区与当前分支最新commit之间的差异 显示两次提交之间的差异 显示今天你写了多少行代码 显示某次提交的元数据和内容变化 显示某次提交发生变化的文件 显示某次提交时，某个文件的内容 显示当前分支的最近几次提交 远程同步下载远程仓库的所有变动 显示所有远程仓库 显示某个远程仓库的信息 增加一个新的远程仓库，并命名 取回远程仓库的变化，并与本地分支合并 上传本地指定分支到远程仓库 强行推送当前分支到远程仓库，即使有冲突 推送所有分支到远程仓库 撤销恢复暂存区的指定文件到工作区 恢复某个commit的指定文件到暂存区和工作区 恢复暂存区的所有文件到工作区 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 重置暂存区与工作区，与上一次commit保持一致 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 重置当前HEAD为指定commit，但保持暂存区和工作区不变 新建一个commit，用来撤销指定commit，后者的所有变化都将被前者抵消，并且应用到当前分支 暂时将未提交的变化移除，稍后再移入 其他生成一个可供发布的压缩包 "},{"title":"Hexo搭建个人博客","date":"2021-05-08T07:39:22.000Z","url":"/2021/05/08/Hexo%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"HexoHexo是一个快速、简洁且高效的博客框架。 安装git安装 安装 Node.jsNode.js 为大多数平台提供了官方的 安装程序。对于中国大陆地区用户，可以前往 淘宝 Node.js 镜像 下载。 其它的安装方法： 安装 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 安装完成后，win+R输入cmd调出命令行，输入hexo提示如下，说明安装正确。 建站安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 新建完成后，指定文件夹的目录如下： _config.yml网站的 配置 信息，您可以在此配置大部分的参数。 package.json应用程序的信息。EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。 scaffolds模版文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。 Hexo的模板是指在新建的文章文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。 source资源文件夹是存放用户资源的地方。除 _posts文件夹之外，开头命名为 _ (下划线)的文件/文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public文件夹，而其他文件会被拷贝过去。 themes主题文件夹。Hexo 会根据主题来生成静态页面。 配置相关配置可直接访问官方文档查看，我们先从使用别人的主题开始，官方提供了335个主题下载使用，你也可以根据规范制定自己的主题。 主题创建Hexo主题非常容易，您只要在themes文件夹内，新增一个任意名称的文件夹，并修改_config.yml内的theme设定，即可切换主题。一个主题可能会有以下的结构： _config.yml主题的配置文件。和 Hexo 配置文件不同，主题配置文件修改时会自动更新，无需重启Hexo Server。 获取主题选择相应的主题，从github上获取到themes目录下。 修改主目录下_config.yml中的配置文件，将theme修改为获取主题的文件夹名。 运行在主目录下调用cmd命令hexo server运行服务，访问进入博客。 添加文章"}]