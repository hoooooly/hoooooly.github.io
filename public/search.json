[{"title":"python多线程基本原理","date":"2021-05-12T19:42:12.000Z","url":"/2021/05/13/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["python","/tags/python/"],["多线程","/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"],["threading","/tags/threading/"]],"categories":[["Python","/categories/Python/"]],"content":"多线程的含义说起多线程，就不得不先说什么是线程。然而想要弄明白什么是线程，又不得不先说什么是进程。 进程我们可以理解为是一个可以独立运行的程序单位，比如打开一个浏览器，这就开启了一个浏览器进程；打开一个文本编辑器，这就开启了一个文本编辑器进程。但一个进程中是可以同时处理很多事情的，比如在浏览器中，我们可以在多个选项卡中打开多个页面，有的页面在播放音乐，有的页面在播放视频，有的网页在播放动画，它们可以同时运行，互不干扰。为什么能同时做到同时运行这么多的任务呢？这里就需要引出线程的概念了，其实这一个个任务，实际上就对应着一个个线程的执行。 而进程呢？它就是线程的集合，进程就是由一个或多个线程构成的，线程是操作系统进行运算调度的最小单位，是进程中的一个最小运行单元。比如上面所说的浏览器进程，其中的播放音乐就是一个线程，播 放视频也是一个线程，当然其中还有很多其他的线程在同时运行，这些线程的并发或并行执行最后使得整个浏览器可以同时运行这么多的任务。 了解了线程的概念，多线程就很容易理解了，多线程就是一个进程中同时执行多个线程，前面所说的浏览器的情景就是典型的多线程执行。 并发和并行 并说到多进程和多线程，这里就需要再讲解两个概念，那就是并发和并行。我们知道，一个程序在计算机中运行，其底层是处理器通过运行一条条的指令来实现的。 并发，英文叫作concurrency。它是指同一时刻只能有一条指令执行，但是多个线程的对应的指令被快速轮换地执行。比如一个处理器，它先执行线程A的指令一段时间，再执行线程B的指令一段时间，再切回到线程A执行一段时间。 由于处理器执行指令的速度和切换的速度非常非常快，人完全感知不到计算机在这个过程中有多个线程切换上下文执行的操作，这就使得宏观上看起来多个线程在同时运行。但微观上只是这个处理器在连续不断地在多个线程之间切换和执行，每个线程的执行一定会占用这个处理器一个时间片段，同一时刻，其实只有一个线程在执行。 并行，英文叫作parallel。它是指同一时刻，有多条指令在多个处理器上同时执行，并行必须要依赖于多个处理器。不论是从宏观上还是微观上，多个线程都是在同一时刻一起执行的。 并行只能在多处理器系统中存在，如果我们的计算机处理器只有一个核，那就不可能实现并行。而并发在单处理器和多处理器系统中都是可以存在的，因为仅靠一个核，就可以实现并发。 举个例子，比如系统处理器需要同时运行多个线程。如果系统处理器只有一个核，那它只能通过并发的方式来运行这些线程。如果系统处理器有多个核，当一个核在执行一个线程时，另一个核可以执行另一个线程，这样这两个线程就实现了并行执行，当然其他的线程也可能和另外的线程处在同一个核上执行，它们之间就是并发执行。具体的执行方式，就取决于操作系统的调度了。 多线程适用场景多在一个程序进程中，有一些操作是比较耗时或者需要等待的，比如等待数据库的查询结果的返回，等待网页结果的响应。如果使用单线程，处理器必须要等到这些操作完成之后才能继续往下执行其他操作，而这个线程在等待的过程中，处理器明显是可以来执行其他的操作的。如果使用多线程，处理器就可以在某个线程等待的时候，去执行其他的线程，从而从整体上提高执行效率。 像上述场景，线程在执行过程中很多情况下是需要等待的。比如网络爬虫就是一个非常典型的例子，爬虫在向服务器发起请求之后，有一段时间必须要等待服务器的响应返回，这种任务就属于IO密集型任务。对于这种任务，如果我们启用多线程，处理器就可以在某个线程等待的过程中去处理其他的任务，从而提高整体的爬取效率。 但并不是所有的任务都是IO密集型任务，还有一种任务叫作计算密集型任务，也可以称之为CPU密集型任务。顾名思义，就是任务的运行一直需要处理器的参与。此时如果我们开启了多线程，一个处理器从一个计算密集型任务切换到切换到另一个计算密集型任务上去，处理器依然不会停下来，始终会忙于计算，这样并不会节省总体的时间，因为需要处理的任务的计算总量是不变的。如果线程数目过多，反而还会在线程切换的过程中多耗费一些时间，整体效率会变低。 所以，如果任务不全是计算密集型任务，我们可以使用多线程来提高程序整体的执行效率。尤其对于网络爬虫这种 IO 密集型任务来说，使用多线程会大大提高程序整体的爬取效率。 Python实现多线程实在Python中，实现多线程的模块叫作threading，是Python自带的模块。使用threading实现多线程的方法。 Thread直接创建子线程首先，我们可以使用Thread类来创建一个线程，创建时需要指定target参数为运行的方法名称，如果被调用的方法需要传入额外的参数，则可以通过Thread的args参数来指定。示例如下："},{"title":"Session和Cookies","date":"2021-05-12T02:51:36.000Z","url":"/2021/05/12/Session%E5%92%8CCookies/","tags":[["http","/tags/http/"],["web","/tags/web/"]],"categories":[["http","/categories/http/"]],"content":"静态网页和动态网页示例代码： 这是最基本的HTML代码，保存为一个.html文件，然后把它放在某台具有固定公网IP的主机上，主机上装上Apache或Nginx等服务器，这样这台主机就可以作为服务器了，其他人便可以通过访问服务器看到这个页面，这就搭建了一个最简单的网站。 这种网页的内容是HTML代码编写的，文字、图片等内容均通过写好的HTML代码来指定，这种页面叫作静态网页。它加载速度快，编写简单，但是存在很大的缺陷，如可维护性差，不能根据URL灵活多变地显示内容等。例如，想要给这个网页的URL传入一个name参数，让其在网页中显示出来，是无法做到的。 因此，动态网页应运而生，它可以动态解析URL中参数的变化，关联数据库并动态呈现不同的页面内容，非常灵活多变。现在遇到的大多数网站都是动态网站，它们不再是一个简单的HTML，而是可能由JSP、PHP、Python等语言编写的，其功能比静态网页强大和丰富太多了。 此外，动态网站还可以实现用户登录和注册的功能。按照一般的逻辑来说，输入用户名和密码登录之后，肯定是拿到了一种类似凭证的东西，有了它，才能保持登录状态，才能访问登录之后才能看到的页面。 那么，这种神秘的凭证到底是什么呢？其实它就是 Session和Cookies 共同产生的结果。 无状态HTTP在了解Session和Cookies之前，还需要了解HTTP的一个特点，叫作无状态。 HTTP的无状态是指HTTP协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。 当向服务器发送请求后，服务器解析此请求，然后返回对应的响应，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录。 这意味着如果后续需要处理前面的信息，则必须重传，这也导致需要额外传递一些前面的重复请求，才能获取后续响应，这种效果显然不是想要的。为了保持前后状态，肯定不能将前面的请求全 部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。 这时两个用于保持HTTP连接状态的技术就出现了，它们分别是Session和Cookies。Session在服务端，也就是网站的服务器，用来保存用户的Session信息；Cookies在客户端，也可以理解为浏览器端，有了Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别Cookies并鉴定出是哪个用户，然后再判断用户是否是登录状态，进而返回对应的响应。 可以理解为Cookies里面保存了登录的凭证，有了它，只需要在下次请求携带Cookies发送请求而不必重新输入用户名、密码等信息重新登录了。 因此在爬虫中，有时候处理需要登录才能访问的页面时，一般会直接将登录成功后获取的Cookies放在请求头里面直接请求，而不必重新模拟登录。 CookiesCookies指某些网站为了辨别用户身份、进行Session跟踪而存储在用户本地终端上的数据。 SessionSession，中文称之为会话，其本身的含义是指有始有终的一系列动作/消息。比如，打电话时，从拿起电话拨号到挂断电话这中间的一系列过程可以称为一个 Session。 而在Web中，Session对象用来存储特定用户Session所需的属性及配置信息。这样，当用户在应用程序的Web页之间跳转时，存储在Session对象中的变量将不会丢失，而是在整个用户Session中一直存在下去。当用户请求来自应用程序的Web页时，如果该用户还没有Session，则Web服务器将自动创建一个 Session对象。当Session过期或被放弃后，服务器将终止该Session。 Session维持那么，怎样利用Cookies保持状态呢？当客户端第一次请求服务器时，服务器会返回一个响应头中带有Set-Cookie字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把Cookies保存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies放到请求头一起提交给服务器，Cookies携带了SessionID 信息，服务器检查该Cookies即可找到对应的Session是什么，然后再判断Session来以此来辨认用户状态。 在成功登录某个网站时，服务器会告诉客户端设置哪些Cookies信息，在后续访问页面时客户端会把Cookies发送给服务器，服务器再找到对应的Session加以判断。如果Session中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。 反之，如果传给服务器的Cookies是无效的，或者Session已经过期了，将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录。 所以，Cookies和Session需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录Session控制。 Cookies属性结构Cookie有如下几个属性。 Name，即该Cookie的名称。 Cookie一旦创建，名称便不可更改。 Value，即该Cookie的值。如果值为Unicode字符，需要为字符编码。如果值为二进制数据，则需要使用BASE64编码。 MaxAge，即该Cookie失效的时间，单位秒，也常和Expires一起使用，通过它可以计算出其有效时间。 MaxAge 如果为正数，则该Cookie在MaxAge秒之后失效。如果为负数，则关闭浏览器时Cookie即失效，浏览器也不会以任何形式保存该Cookie。 Path，即该Cookie的使用路径。如果设置为/path/，则只有路径为/path/的页面可以访问该Cookie。如果设置为/，则本域名下的所有页面都可以访问该Cookie。 Domain，即可以访问该Cookie的域名。例如如果设置为.zhihu.com，则所有以zhihu.com，结尾的域名都可以访问该Cookie。 Size字段，即此Cookie的大小。 Http字段，即Cookie的httponly属性。若此属性为true，则只有在HTTP Headers中会带有此Cookie的信息，而不能通过document.cookie来访问此Cookie。 Secure，即该Cookie是否仅被使用安全协议传输。安全协议。安全协议有HTTPS、SSL等，在网络上传输数据之前先将数据加密。默认为false。 会话会Cookie和持久和Cookie从表面意思来说，会话Cookie就是把Cookie放在浏览器内存里，浏览器在关闭之后该Cookie即失效；持久Cookie则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态。 严格来说，没有会话Cookie和持久Cookie之分，只是由Cookie的MaxAge或Expires字段决定了过期的时间。 因此，一些持久化登录的网站其实就是把Cookie的有效时间和Session有效期设置得比较长，下次再访问页面时仍然携带之前的Cookie，就可以直接保持登录状态。 常见误区在谈论Session机制的时候，常常听到这样一种误解 ——“只要关闭浏览器，Session就消失了”。可以想象一下会员卡的例子，除非顾客主动对店家提出销卡，否则店家绝对不会轻易删除顾客的资料。对Session来说，也是一样，除非程序通知服务器删除一个Session，否则服务器会一直保留。比如，程序一般都是在做注销操作时才去删除Session。 但是当关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭。之所以会有这种错觉，是因为大部分网站都使用会话Cookie来保存Session ID信息，而关闭浏览器后Cookies就消失了，再次连接服务器时，也就无法找到原来的Session了。如果服务器设置的Cookies保存到硬盘上，或者使用某种手段改写浏览器发出的HTTP请求头，把原来的Cookies发送给服务器，则再次打开浏览器，仍然能够找到原来的Session ID，依旧还是可以保持登录状态的。 而且恰恰是由于关闭浏览器不会导致Session被删除，这就需要服务器为Session设置一个失效时间，当距离客户端上一次使用Session的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把Session删除以节省存储空间。"},{"title":"爬虫的基本原理","date":"2021-05-11T16:12:50.000Z","url":"/2021/05/12/%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["http","/tags/http/"],["python","/tags/python/"],["爬虫","/tags/%E7%88%AC%E8%99%AB/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。如果把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。 爬虫概述简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，概要介绍一下。 获取网页获爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。 源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。 前面讲了请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个 请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样实现呢？ Python提供了许多库来帮助我们实现这个操作，如urllib、requests等。可以用这些库来帮助我们实现HTTP请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的Body部分，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。 提取信息提获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。 另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS选择器或XPath来提取网页信息的库，如Beautiful Soup、pyquery、lxml等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。 提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。 保存数据提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为TXT文本或JSON文本，也可以保存到数据库，如MySQL和MongoDB等，还可保存至远程服务器，如借助SFTP进行操作等。 自动化程序自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各 种异常处理、错误重试等操作，确保爬取持续高效地运行。 能抓怎样的数据能在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML代码，而最常抓取的便是HTML源代码。 另外，可能有些网页返回的不是HTML代码，而是一个JSON字符串（其中API接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。 此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。 另外，还可以看到各种扩展名的文件，如CSS、JavaScript和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。 上述内容其实都对应各自的URL，是基于HTTP或HTTPS协议的，只要是这种数据，爬虫都可以抓取。 JavaScript渲染页面有时候，在用urllib或requests抓取网页时，得到的源代码实际和浏览器中看到的不一样。 这是一个非常常见的问题。现在网页越来越多地采用Ajax、前端模块化工具来构建，整个网页可能都是由JavaScript渲染出来的，也就是说原始的HTML代码就是一个空壳，例如： body节点里面只有一个id为container的节点，但是需要注意在body节点后引入了app.js，它便负责整个网站的渲染。 在浏览器中打开这个页面时，首先会加载这个HTML内容，接着浏览器会发现其中引入了一个app.js文件，然后便会接着去请求这个文件，获取到该文件后，便会执行其中的JavaScript代码，而JavaScript则会改变HTML中的节点，向其添加内容，最后得到完整的页面。 但是在用urllib或requests等库请求当前页面时，我们得到的只是这个HTML代码，它不会帮助我们去继续加载这个JavaScript文件，这样也就看不到浏览器中的内容。 这也解释了为什么有时我们得到的源代码和浏览器中看到的不一样。 因此，使用基本HTTP请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，可以分析其后台Ajax接口，也可使用Selenium、Splash这样的库来实现模拟JavaScript渲染。"},{"title":"web网页基础","date":"2021-05-11T15:32:38.000Z","url":"/2021/05/11/web%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80/","tags":[["html","/tags/html/"],["css","/tags/css/"],["javascript","/tags/javascript/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"网页的组成首先，我们来了解网页的基本组成，网页可以分为三大部分：HTML、CSS和JavaScript。 如果把网页比作一个人的话，HTML相当于骨架，JavaScript相当于肌肉，CSS相当于皮肤，三者结合起来才能形成一个完整的网页。下面我们来分别介绍一下这三部分的功能。 HTMLHTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。 我们浏览的网页包括文字、按钮、图片和视频等各种复杂的元素，其基础架构就是HTML。不同类型的元素通过不同类型的标签来表示，如图片用img标签表示，视频用video标签表示，段落用p标签表示，它们之间的布局又常通过布局标签div嵌套组合而成，各种标签通过不同的排列和嵌套就可以形成网页的框架。 在Chrome浏览器中打开百度，右击并选择 “检查”项（或按F12键），打开开发者模式，这时在Elements选项卡中即可看到网页的源代码，如图所示。 这就是HTML，整个网页就是由各种标签嵌套组合而成的。这些标签定义的节点元素相互嵌套和组合形成了复杂的层次关系，就形成了网页的架构。 CSS虽然HTML定义了网页的结构，但是只有HTML页面的布局并不美观，可能只是简单的节点元素的排列，为了让网页看起来更好看一些，这里就需要借助CSS了。 CSS，全称叫作Cascading Style Sheets，即层叠样式表。“层叠”是指当在HTML中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式”指网页中文字大小、颜色、元素间距、排列等 格式。 CSS是目前唯一的网页页面排版样式标准，有了它的帮助，页面才会变得更为美观。例如： 这就是一个CSS样式。大括号前面是一个CSS选择器。此选择器的作用是首先选中id为head_wrapper且class 为s-ps-islite的节点，然后再选中其内部的class为s-p-top的节点。大括号内部写的就是一条条样式规则，例如position指定了这个元素的布局方式为绝对布局，bottom指定元素的下边距为40像素，width指定了宽度为100%占满父元素，height则指定了元素的高度。也就是说，我们将位置、宽度、高度等样式配置统一写成这样的形式，然后用大括号括起来，接着在开头再加上CSS选择器，这就代表这个样式对CSS选择器选中的元素生效，元素就会根据此样式来展示了。 在网页中，一般会统一定义整个网页的样式规则，并写入CSS文件中（其后缀为css）。在HTML中，只需要用link标签即可引入写好的CSS文件，这样整个页面就会变得美观、优雅。 JavaScriptJavaScript，简称JS，是一种脚本语言。HTML和CSS配合使用，提供给用户的只是一种静态信息，缺乏交互性。我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是JavaScript的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系，而是实现了一种实时、动态、交互的页面功能。 JavaScript通常也是以单独的文件形式加载的，后缀为js，在 HTML中通过script标签即可引入，例如: 综上所述，HTML定义了网页的内容和结构，CSS描述了网页的布局，JavaScript定义了网页的行为。"},{"title":"在Hexo博客中嵌入外链视频","date":"2021-05-11T01:12:11.000Z","url":"/2021/05/11/%E5%9C%A8Hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%B5%8C%E5%85%A5%E5%A4%96%E9%93%BE%E8%A7%86%E9%A2%91/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"嵌入的视频Hexo支持Youtube视频的嵌入，可以参考其实现方式。 首先，在node_modules/hexo/lib/plugins/tag/index.js中添加以下代码。 然后在node_modules/hexo/lib/plugins/tag/目录下新建bilibili.js文件，打开并添加如下代码： 重新启动下Hexo Server,在md页面中添加下列： 重新刷新页面，就可以看到视频正常加载了。 "},{"title":"腾讯云配置ssl证书","date":"2021-05-10T07:36:11.000Z","url":"/2021/05/10/%E8%85%BE%E8%AE%AF%E4%BA%91%E9%85%8D%E7%BD%AEssl%E8%AF%81%E4%B9%A6/","tags":[["nginx","/tags/nginx/"],["https","/tags/https/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"什么是SSL证书？SSL证书是用于在Web服务器与浏览器以及客户端之间建立加密链接的加密技术，通过配置和应用SSL证书来启用HTTPS协议，来保证互联网数据传输的安全，全球每天有数以亿计的网站都是通过HTTPS来确保数据安全，保护用户隐私。 申请腾讯云SSL证书百毒搜索腾讯SSL证书，找到免费使用SSL一年的产品，一系列骚操作后得到证书。 nginx配置修改下载SSL证书，上传到服务器/etc/pki/nginx/目录下。 修改/etc/配置文件，注意SSL证书的路径和实际上传的路径和名称一致。 "},{"title":"Hexo在腾讯云的部署","date":"2021-05-10T04:51:56.000Z","url":"/2021/05/10/Hexo%E5%9C%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E7%9A%84%E9%83%A8%E7%BD%B2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"简介Hexo在GitHub pages上的访问太慢了，迁移到腾讯云服务器上。 部署环境腾讯云服务器（Centos 64位）。 服务器配置安装git 创建git用户并修改权限 找到一下内容 在该语句下添加 退出（esc + :wq）并修改权限 本地使用gitbash创建密钥 在腾讯云中创建ssh，并将本地的id_rsa.pub中的文件内容全部复制到authorized_keys中。 修改权限 本地测试 云服务器中创建网站目录并设置权限 安装nginx 以上执行完之后，在浏览器中输入你的公网IP如果可以进入CentOs界面，说明Nginx安装成功。 配置nginx 重启服务 建立git仓库并修改权限 同步网站根目录 填入如下内容 修改权限 在本地Hexo目录下修改_config.yml文件中的deploy后的repo改为： 以上全部完成后，执行hexo的部署命令即可完成在腾讯云服务器上的博客部署。"},{"title":"HTTP基本原理","date":"2021-05-09T12:48:23.000Z","url":"/2021/05/09/HTTP%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["Http","/tags/Http/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"URI和URLURI （Uniform Resource Identifier） 即统一资源标志符。URL （Uniform Resource Locator 即统一资源定位符。 例如：既是一个URL，也是一个URI。用URL/URI来唯一指定了它的访问方式，这其中包括了访问协议Https，访问路径（即根目录）和资源名称favicon.ico。 URL是URI的一个子集，也就是每个URL都是URI，但不是每个URI都是URL。 URI还包括一个子类叫做URN（Universal Resource Name）即统一资源名称。但是在目前的互联网，URN的使用非常少，几乎所有的 URI都是URL，所以一般的网页链接我们可以称之为 URL，也可以称之为 URI。 超文本Hypertext，我们在浏览器里看到的网页就是超文本解析而成的，其网页源代码是一系列 HTML代码，里面包含了一系列标签，比如img显示图片，p指定显示段落等。浏览器解析这些标签后，便形成了我们平常看到的网页，而网页的源代码HTML就可以称作超文本。 HTTP和HTTPSHTTP的全称是Hyper Text Transfer Protocol，中文名叫作超文本传输协议，HTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。HTTP由万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet EngineeringTask Force）共同合作制定的规范，目前广泛使用的是HTTP 1.1版本。 HTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，简称为HTTPS。 HTTPS的安全基础是SSL，因此通过它传输的内容都是经过SSL加密的，它的主要作用可以分为两种： 建立一个信息安全通道，来保证数据传输的安全。 确认网站的真实性，凡是使用了HTTPS的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过CA机构颁发的安全签章来查询。 现在越来越多的网站和 App 都已经向 HTTPS 方向发展。例如： 苹果公司强制所有 iOS App 在 2017 年 1 月 1 日 前全部改为使用 HTTPS 加密，否则 App 就无法在应用商店上架。 谷歌从 2017 年 1 月推出的 Chrome 56 开始，对未进行 HTTPS 加密的网址链接亮出风险提示，即在地址栏的显著位置提醒用户 “此网页不安全”。 腾讯微信小程序的官方需求文档要求后台使用 HTTPS 请求进行网络通信，不满足条件的域名和协议无法请求。 因此，HTTPS 已经已经是大势所趋。 HTTP请求过程我们在浏览器中输入一个URL，回车之后便可以在浏览器中观察到页面内容。实际上，这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器。响应里包含了页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来，传输模型如图所示： 此处客户端即代表我们自己的 PC 或手机浏览器，服务器即要访问的网站所在的服务器。 为了更直观地说明这个过程，这里用浏览器的开发者模式下的Network监听组件来做演示，它可以显示访问当前请求网页时发生的所有网络请求和响应。打开浏览器（Chrome或Edge都可以），右击并选择 “检查”项，即可打开浏览器的开发者工具。这里访问百度，输入该 URL后回车，观察这个过程中发生了怎样的网络请求。可以看到，在Network页面下方 出现了一个个的条目，其中一个条目就代表一次发送请求和接收响应的过程，如图所示: 我们先观察第一个网络请求，即www.baidu.com，其中各列的含义如下。 第一列 Name：请求的名称，一般会将 URL的最后一部分内容当作名称。 第二列 Status：响应的状态码，这里显示为 200，代表响应是正常的。通过状态码，我们可以判断发送了请求之后是否得到了正常的响应。 第三列 Type：请求的文档类型。这里为document，代表我们这次请求的是一个 HTML文档，内容就是一些 HTML代码。 第四列 Initiator：请求源。用来标记请求是由哪个对象或进程发起的。 第五列 Size：从服务器下载的文件和请求的资源大小。如果是从缓存中取得的资源，则该列会显示 fromcache。 第六列 Time：发起请求到获取响应所用的总时间。 第七列 Waterfall：网络请求的可视化瀑布流。 我们点击这个条目即可看到其更详细的信息，如图所示。 首先是General部分，Request URL为请求的URL，Request Method为请求的方法，Status Code为响应状态码，Remote Address为远程服务器的地址和端口，Referrer Policy为 Referrer判别策略。 再继续往下，可以看到，有Response Headers和Request Headers，这分别代表响应头和请求头。请求头里带有许多请求信息，例如浏览器标识、Cookies、Host等信息，这是请求的一部分，服务器会根据请求头内的信息判断请求是否合法，进而作出对应的响应。图中看到的Response Headers就是响应的一部分，例如其中包含了服务器的类型、文档类型、日期等信息，浏览器接受到响应后，会解析响应内容，进而呈现网页内容。 请求请求，由客户端向服务端发出，可以分为4部分内容：请求方法（Request Method、请求的网址（Request URL）、请求头（Request Headers）、请求体（Request Body）。 请求方法常见的请求方法有两种：GET和POST。 在浏览器中直接输入URL并回车，这便发起了一个GET请求，请求的参数会直接包含到URL里。例如，在百度中搜索Python，这就是一个GET请求，链接为，其中URL中包 含了请求的参数信息，这里参数wd表示要搜寻的关键字。POST请求大多在表单提交时发起。比如，对于一个登录表单，输入用户名和密码后，点击“登录”按钮，这通常会发起一个POST请求，其数据通常 以表单的形式传输，而不会体现在URL中。 GET和POST请求方法有如下区别。 GET请求中的参数包含在URL里面，数据可以在URL中看到，而POST请求的URL不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。 GET请求提交的数据最多只有1024字节，而POST请求没有限制。 一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用GET方式请求的话，密码就会暴露在URL里面，造成密码泄露，所以这里最好以POST方式发送。上传文件时，由于文件内容比较大，也会选用POST方式。 我们平常遇到的绝大部分请求都是GET或POST请求，另外还有一些请求方法，如HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE等，我们简单将其总结为下表。 方法 描述 GET 请求页面，并返回页面内容 HEAD 类似于GET请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 大多用于表单提交或上传文件，数据包含在请求体中 PUT 从客户端向服务器传送的数据取代指定文档中的内容 DELETE 请求服务器删除指定的页面 CONNECT 把服务器当作跳板，让服务器代替客户端访问其他网页 OPTIONS 允许客户端查看服务器的性能 TRACE 回显服务器收到的请求，主要用于测试或诊断 table { margin: auto; font-size: 50%; } 请求的网址，即统一资源定位符 URL，它可以唯一确定我们想请求的资源。 请求头请求头，用来说明服务器要使用的附加信息，比较重要的信息有Cookie、Referer、User-Agent等。下面简要说明一些常用的头信息。 Accept：请求报头域，用于指定客户端可接受哪些类型的信息。 Accept-Language：指定客户端可接受的语言类型。 Accept-Encoding：指定客户端可接受的内容编码。 Host：用于指定请求资源的主机IP和端口号，其内容为请求URL的原始服务器或网关的位置。从 HTTP 1.1 版本开始，请求必须包含此内容。 Cookie：也常用复数形式Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会 话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是Cookies的功劳。Cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页 面时，都会在请求头中加上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。 Referer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、防盗链处理等。 User-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫。 Content-Type：也叫互联网媒体类型（Internet Media Type）或者MIME类型，在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html代表HTML格式，image/gif代表GIF图片，application/json代表JSON类型，更多对应关系可以查看此对照表：。 因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。 请求体请请求体一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。 登录之前，我们填写了用户名和密码信息，提交时这些内容就会以表单数据的形式提交给服务器，此时需要注意Request Headers中指定Content-Type为application/x-www-form-urlencoded。只有设置Content-Type为application/x-www-form-urlencoded，才会以表单数据的形式提交。另外，我们也可以将Content-Type设置为application/json来提交JSON数据，或者设置为multipart/form-data来上传文件。 表格中列出了Content-Type和POST提交数据方式的关系。 Content-Type 提交数据的方式 application/x-www-form-urlencodeed 表单数据 multipart/form-data 表单文件上传 application/json 序列化JSON数据 text/xml XML数据 在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type，不然可能会导致POST提交后无法正常响应。 响应响应，由服务端返回给客户端，可以分为三部分：响应状态码（Response Status Code）、响应头（Response Headers）和响应体（Response Body）。 响应状态码响应状态码表示服务器的响应状态，如200代表服务器正常响应，404代表页面未找到，500代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为200，则证明成功返 回数据，再进行进一步的处理，否则直接忽略。下表列出了常见的错误代码及错误原因。 状态码 说明 详情 100 继续 请求者应当继续提出请求，服务器已收到请求的一部分，正在等待其余部分 101 切换协议 请求者已要求服务器切换协议，服务器已确认并确认切换 200 成功 服务器已成功处理了请求 201 已创建 请求成功并且服务器创建了新的资源 202 已接受 服务器已接受请求，但尚未处理 203 非授权信息 服务器已经成功处理请求，但返回的信息可能来自另一个源 204 无内容 服务器成功处理了请求，但没有返回任何内容 205 重置内容 服务器成功处理了请求，但内容被重置 206 部分内容 服务器成功处理了部分请求 300 多种选择 针对请求，服务器可执行多种操作 301 永久移动 请求的网页已永久移动到新位置，即永久重定向 302 临时移动 请求的网页暂时跳转到其他页面，即暂时重定向 303 查看其他位置 如果原来的请求是POST，重定向目标文档应当通过GET方式访问资源 304 未修改 此次请求返回的网页未修改，继续使用上次的资源 305 使用代理 请求者应该使用代理访问该网页 307 临时重定向 请求的资源临时从其他位置响应 400 错误请求 服务器无法解析该请求 401 未授权 请求没有进行身份验证或验证未通过 403 禁止访问 服务器拒绝此请求 404 未找到 服务器找不到请求的网页 405 方法禁用 服务器禁用了请求中指定的方法 406 不接受 无法使用请求的内容响应请求的网页 407 需要代理授权 请求者需要使用代理授权 408 请求超时 服务器请求超时 409 冲突 服务器在完成请求时发生冲突 410 已删除 请求的资源已永久删除 411 需要有效长度 服务器不接受不含有效内容长度标头字段的请求 412 未满足前提条件 服务器未满足请求者在请求中设置的其中一个前提条件 413 请求实体过大 请求实体过大，超出服务器的处理能力 415 请求URL过长 请求的网址过长，服务器无法处理 416 请求范围不符 页面无法提供请求页面支持 417 未满足期望值 服务器为满足期望请求标头字段的要求 500 服务器内部错误 服务器遇到错误，无法完成请求 501 未实现 服务器不具备完成请求的功能 502 错误网关 服务器作为网关或代理，从上游服务器收到无效响应 503 服务不可用 服务器目前无法使用 504 网关超时 服务器作为网关或代理，但是没有及时从上游服务器收到响应 505 HTTP版本不支持 服务器不支持请求中所用的HTTP协议版本 响应头包含了服务器对请求的应答信息，如Content-Type、Server、Set-Cookie等。下面简要说明一些常用的响应头信息。 Date：标识响应产生的时间。 Last-Modified：指定资源的最后修改时间。 Content-Encoding：指定响应内容的编码。 Server：包含服务器的信息，比如名称、版本号等。 Content-Type：文档类型，指定返回的数据类型是什么，如text/html代表返回HTML文档，application/x-javascript则代表返回JavaScript文件，image/jpeg则代表返回图片。 Set-Cookie：设置Cookies。响应头中的Set-Cookie告诉浏览器需要将此内容放在Cookies中，下次请求携带Cookies请求。 Expires：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。 响应体最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体，如图所示。 在浏览器开发者工具中点击Response，就可以看到网页的源代码，也就是响应体的内容，它是解析的目标。在做爬虫时，我们主要通过响应体得到网页的源代码、JSON数据等，然后从中做相应内容的提取。"},{"title":"Git常用命令","date":"2021-05-08T10:03:19.000Z","url":"/2021/05/08/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":[["Git","/tags/Git/"]],"categories":[["Git","/categories/Git/"]],"content":"Git常用命令仓库在当前目录新建一个Git代码库 新建一个目录，将其初始化为Git代码库 下载一个项目和它的整个代码历史 配置显示当前的Git配置 编辑Git配置文件 设置提交代码时的用户信息 增加/删除文件添加指定文件到暂存区 添加指定目录到暂存区，包括子目录 添加当前目录的所有文件到暂存区 添加每个变化前，都会要求确认对于同一个文件的多处变化，可以实现分次提交 停止追踪指定文件，但该文件会保留在工作区 改名文件，并且将这个改名放入暂存区 代码提交提交暂存区到仓库区 提交暂存区的指定文件到仓库区 提交工作区自上次commit之后的变化，直接到仓库区 提交时显示所有diff信息 使用一次新的commit，替代上一次提交,如果代码没有任何新变化，则用来改写上一次commit的提交信息 重做上一次commit，并包括指定文件的新变化 分支列出所有本地分支 列出所有远程分支 列出所有本地分支和远程分支 新建一个分支，但依然停留在当前分支 新建一个分支，并切换到该分支 新建一个分支，指向指定commit 新建一个分支，与指定的远程分支建立追踪关系 切换到指定分支，并更新工作区 切换到上一个分支 建立追踪关系，在现有分支与指定的远程分支之间 合并指定分支到当前分支 选择一个commit，合并进当前分支 删除分支 删除远程分支 标签列出所有tag 新建一个tag在当前commit 新建一个tag在指定commit 删除本地tag 删除远程tag 查看tag信息 提交指定tag 提交所有tag 新建一个分支，指向某个tag 查看信息显示有变更的文件 显示当前分支的版本历史 显示commit历史，以及每次commit发生变更的文件 搜索提交历史，根据关键词 显示某个commit之后的所有变动，每个commit占据一行 显示某个commit之后的所有变动，其”提交说明”必须符合搜索条件 显示某个文件的版本历史，包括文件改名 显示指定文件相关的每一次diff 显示过去5次提交 显示所有提交过的用户，按提交次数排序 显示指定文件是什么人在什么时间修改过 显示暂存区和工作区的差异 显示暂存区和上一个commit的差异 显示工作区与当前分支最新commit之间的差异 显示两次提交之间的差异 显示今天你写了多少行代码 显示某次提交的元数据和内容变化 显示某次提交发生变化的文件 显示某次提交时，某个文件的内容 显示当前分支的最近几次提交 远程同步下载远程仓库的所有变动 显示所有远程仓库 显示某个远程仓库的信息 增加一个新的远程仓库，并命名 取回远程仓库的变化，并与本地分支合并 上传本地指定分支到远程仓库 强行推送当前分支到远程仓库，即使有冲突 推送所有分支到远程仓库 撤销恢复暂存区的指定文件到工作区 恢复某个commit的指定文件到暂存区和工作区 恢复暂存区的所有文件到工作区 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 重置暂存区与工作区，与上一次commit保持一致 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 重置当前HEAD为指定commit，但保持暂存区和工作区不变 新建一个commit，用来撤销指定commit，后者的所有变化都将被前者抵消，并且应用到当前分支 暂时将未提交的变化移除，稍后再移入 其他生成一个可供发布的压缩包 "},{"title":"Hexo搭建个人博客","date":"2021-05-08T07:39:22.000Z","url":"/2021/05/08/Hexo%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"HexoHexo是一个快速、简洁且高效的博客框架。 安装git安装 安装 Node.jsNode.js 为大多数平台提供了官方的 安装程序。对于中国大陆地区用户，可以前往 淘宝 Node.js 镜像 下载。 其它的安装方法： 安装 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 安装完成后，win+R输入cmd调出命令行，输入hexo提示如下，说明安装正确。 建站安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 新建完成后，指定文件夹的目录如下： _config.yml网站的 配置 信息，您可以在此配置大部分的参数。 package.json应用程序的信息。EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。 scaffolds模版文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。 Hexo的模板是指在新建的文章文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。 source资源文件夹是存放用户资源的地方。除 _posts文件夹之外，开头命名为 _ (下划线)的文件/文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public文件夹，而其他文件会被拷贝过去。 themes主题文件夹。Hexo 会根据主题来生成静态页面。 配置相关配置可直接访问官方文档查看，我们先从使用别人的主题开始，官方提供了335个主题下载使用，你也可以根据规范制定自己的主题。 主题创建Hexo主题非常容易，您只要在themes文件夹内，新增一个任意名称的文件夹，并修改_config.yml内的theme设定，即可切换主题。一个主题可能会有以下的结构： _config.yml主题的配置文件。和 Hexo 配置文件不同，主题配置文件修改时会自动更新，无需重启Hexo Server。 获取主题选择相应的主题，从github上获取到themes目录下。 修改主目录下_config.yml中的配置文件，将theme修改为获取主题的文件夹名。 运行在主目录下调用cmd命令hexo server运行服务，访问进入博客。 添加文章"}]