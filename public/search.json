[{"title":"爬虫的基本原理","date":"2021-05-11T16:12:50.000Z","url":"/2021/05/12/%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["http","/tags/http/"],["爬虫","/tags/%E7%88%AC%E8%99%AB/"],["python","/tags/python/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"我们可以把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。如果把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，网站的数据就可以被抓取下来了。 爬虫概述简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，概要介绍一下。 获取网页获爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。 源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。 前面讲了请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个 请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样实现呢？ Python提供了许多库来帮助我们实现这个操作，如urllib、requests等。可以用这些库来帮助我们实现HTTP请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的Body部分，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。 提取信息提获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。 另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS选择器或XPath来提取网页信息的库，如Beautiful Soup、pyquery、lxml等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。 提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。 保存数据提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为TXT文本或JSON文本，也可以保存到数据库，如MySQL和MongoDB等，还可保存至远程服务器，如借助SFTP进行操作等。 自动化程序自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各 种异常处理、错误重试等操作，确保爬取持续高效地运行。 能抓怎样的数据能在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML代码，而最常抓取的便是HTML源代码。 另外，可能有些网页返回的不是HTML代码，而是一个JSON字符串（其中API接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。 此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。 另外，还可以看到各种扩展名的文件，如CSS、JavaScript和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。 上述内容其实都对应各自的URL，是基于HTTP或HTTPS协议的，只要是这种数据，爬虫都可以抓取。 JavaScript渲染页面有时候，在用urllib或requests抓取网页时，得到的源代码实际和浏览器中看到的不一样。 这是一个非常常见的问题。现在网页越来越多地采用Ajax、前端模块化工具来构建，整个网页可能都是由JavaScript渲染出来的，也就是说原始的HTML代码就是一个空壳，例如： body节点里面只有一个id为container的节点，但是需要注意在body节点后引入了app.js，它便负责整个网站的渲染。 在浏览器中打开这个页面时，首先会加载这个HTML内容，接着浏览器会发现其中引入了一个app.js文件，然后便会接着去请求这个文件，获取到该文件后，便会执行其中的JavaScript代码，而JavaScript则会改变HTML中的节点，向其添加内容，最后得到完整的页面。 但是在用urllib或requests等库请求当前页面时，我们得到的只是这个HTML代码，它不会帮助我们去继续加载这个JavaScript文件，这样也就看不到浏览器中的内容。 这也解释了为什么有时我们得到的源代码和浏览器中看到的不一样。 因此，使用基本HTTP请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，可以分析其后台Ajax接口，也可使用Selenium、Splash这样的库来实现模拟JavaScript渲染。"},{"title":"web网页基础","date":"2021-05-11T15:32:38.000Z","url":"/2021/05/11/web%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80/","tags":[["html","/tags/html/"],["css","/tags/css/"],["javascript","/tags/javascript/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"网页的组成首先，我们来了解网页的基本组成，网页可以分为三大部分：HTML、CSS和JavaScript。 如果把网页比作一个人的话，HTML相当于骨架，JavaScript相当于肌肉，CSS相当于皮肤，三者结合起来才能形成一个完整的网页。下面我们来分别介绍一下这三部分的功能。 HTMLHTML是用来描述网页的一种语言，其全称叫作Hyper Text Markup Language，即超文本标记语言。 我们浏览的网页包括文字、按钮、图片和视频等各种复杂的元素，其基础架构就是HTML。不同类型的元素通过不同类型的标签来表示，如图片用img标签表示，视频用video标签表示，段落用p标签表示，它们之间的布局又常通过布局标签div嵌套组合而成，各种标签通过不同的排列和嵌套就可以形成网页的框架。 在Chrome浏览器中打开百度，右击并选择 “检查”项（或按F12键），打开开发者模式，这时在Elements选项卡中即可看到网页的源代码，如图所示。 这就是HTML，整个网页就是由各种标签嵌套组合而成的。这些标签定义的节点元素相互嵌套和组合形成了复杂的层次关系，就形成了网页的架构。 CSS虽然HTML定义了网页的结构，但是只有HTML页面的布局并不美观，可能只是简单的节点元素的排列，为了让网页看起来更好看一些，这里就需要借助CSS了。 CSS，全称叫作Cascading Style Sheets，即层叠样式表。“层叠”是指当在HTML中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式”指网页中文字大小、颜色、元素间距、排列等 格式。 CSS是目前唯一的网页页面排版样式标准，有了它的帮助，页面才会变得更为美观。例如： 这就是一个CSS样式。大括号前面是一个CSS选择器。此选择器的作用是首先选中id为head_wrapper且class 为s-ps-islite的节点，然后再选中其内部的class为s-p-top的节点。大括号内部写的就是一条条样式规则，例如position指定了这个元素的布局方式为绝对布局，bottom指定元素的下边距为40像素，width指定了宽度为100%占满父元素，height则指定了元素的高度。也就是说，我们将位置、宽度、高度等样式配置统一写成这样的形式，然后用大括号括起来，接着在开头再加上CSS选择器，这就代表这个样式对CSS选择器选中的元素生效，元素就会根据此样式来展示了。 在网页中，一般会统一定义整个网页的样式规则，并写入CSS文件中（其后缀为css）。在HTML中，只需要用link标签即可引入写好的CSS文件，这样整个页面就会变得美观、优雅。 JavaScriptJavaScript，简称JS，是一种脚本语言。HTML和CSS配合使用，提供给用户的只是一种静态信息，缺乏交互性。我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是JavaScript的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系，而是实现了一种实时、动态、交互的页面功能。 JavaScript通常也是以单独的文件形式加载的，后缀为js，在 HTML中通过script标签即可引入，例如: 综上所述，HTML定义了网页的内容和结构，CSS描述了网页的布局，JavaScript定义了网页的行为。"},{"title":"在Hexo博客中嵌入外链视频","date":"2021-05-11T01:12:11.000Z","url":"/2021/05/11/%E5%9C%A8Hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%B5%8C%E5%85%A5%E5%A4%96%E9%93%BE%E8%A7%86%E9%A2%91/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"嵌入的视频Hexo支持Youtube视频的嵌入，可以参考其实现方式。 首先，在node_modules/hexo/lib/plugins/tag/index.js中添加以下代码。 然后在node_modules/hexo/lib/plugins/tag/目录下新建bilibili.js文件，打开并添加如下代码： 重新启动下Hexo Server,在md页面中添加下列： 重新刷新页面，就可以看到视频正常加载了。 "},{"title":"腾讯云配置ssl证书","date":"2021-05-10T07:36:11.000Z","url":"/2021/05/10/%E8%85%BE%E8%AE%AF%E4%BA%91%E9%85%8D%E7%BD%AEssl%E8%AF%81%E4%B9%A6/","tags":[["nginx","/tags/nginx/"],["https","/tags/https/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"什么是SSL证书？SSL证书是用于在Web服务器与浏览器以及客户端之间建立加密链接的加密技术，通过配置和应用SSL证书来启用HTTPS协议，来保证互联网数据传输的安全，全球每天有数以亿计的网站都是通过HTTPS来确保数据安全，保护用户隐私。 申请腾讯云SSL证书百毒搜索腾讯SSL证书，找到免费使用SSL一年的产品，一系列骚操作后得到证书。 nginx配置修改下载SSL证书，上传到服务器/etc/pki/nginx/目录下。 修改/etc/配置文件，注意SSL证书的路径和实际上传的路径和名称一致。 "},{"title":"Hexo在腾讯云的部署","date":"2021-05-10T04:51:56.000Z","url":"/2021/05/10/Hexo%E5%9C%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E7%9A%84%E9%83%A8%E7%BD%B2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"简介Hexo在GitHub pages上的访问太慢了，迁移到腾讯云服务器上。 部署环境腾讯云服务器（Centos 64位）。 服务器配置安装git 创建git用户并修改权限 找到一下内容 在该语句下添加 退出（esc + :wq）并修改权限 本地使用gitbash创建密钥 在腾讯云中创建ssh，并将本地的id_rsa.pub中的文件内容全部复制到authorized_keys中。 修改权限 本地测试 云服务器中创建网站目录并设置权限 安装nginx 以上执行完之后，在浏览器中输入你的公网IP如果可以进入CentOs界面，说明Nginx安装成功。 配置nginx 重启服务 建立git仓库并修改权限 同步网站根目录 填入如下内容 修改权限 在本地Hexo目录下修改_config.yml文件中的deploy后的repo改为： 以上全部完成后，执行hexo的部署命令即可完成在腾讯云服务器上的博客部署。"},{"title":"HTTP基本原理","date":"2021-05-09T12:48:23.000Z","url":"/2021/05/09/HTTP%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","tags":[["Http","/tags/Http/"]],"categories":[["爬虫","/categories/%E7%88%AC%E8%99%AB/"]],"content":"URI和URLURI （Uniform Resource Identifier） 即统一资源标志符。URL （Uniform Resource Locator 即统一资源定位符。 例如：既是一个URL，也是一个URI。用URL/URI来唯一指定了它的访问方式，这其中包括了访问协议Https，访问路径（即根目录）和资源名称favicon.ico。 URL是URI的一个子集，也就是每个URL都是URI，但不是每个URI都是URL。 URI还包括一个子类叫做URN（Universal Resource Name）即统一资源名称。但是在目前的互联网，URN的使用非常少，几乎所有的 URI都是URL，所以一般的网页链接我们可以称之为 URL，也可以称之为 URI。 超文本Hypertext，我们在浏览器里看到的网页就是超文本解析而成的，其网页源代码是一系列 HTML代码，里面包含了一系列标签，比如img显示图片，p指定显示段落等。浏览器解析这些标签后，便形成了我们平常看到的网页，而网页的源代码HTML就可以称作超文本。 HTTP和HTTPSHTTP的全称是Hyper Text Transfer Protocol，中文名叫作超文本传输协议，HTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档。HTTP由万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet EngineeringTask Force）共同合作制定的规范，目前广泛使用的是HTTP 1.1版本。 HTTPS的全称是Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，简称为HTTPS。 HTTPS的安全基础是SSL，因此通过它传输的内容都是经过SSL加密的，它的主要作用可以分为两种： 建立一个信息安全通道，来保证数据传输的安全。 确认网站的真实性，凡是使用了HTTPS的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过CA机构颁发的安全签章来查询。 现在越来越多的网站和 App 都已经向 HTTPS 方向发展。例如： 苹果公司强制所有 iOS App 在 2017 年 1 月 1 日 前全部改为使用 HTTPS 加密，否则 App 就无法在应用商店上架。 谷歌从 2017 年 1 月推出的 Chrome 56 开始，对未进行 HTTPS 加密的网址链接亮出风险提示，即在地址栏的显著位置提醒用户 “此网页不安全”。 腾讯微信小程序的官方需求文档要求后台使用 HTTPS 请求进行网络通信，不满足条件的域名和协议无法请求。 因此，HTTPS 已经已经是大势所趋。 HTTP请求过程我们在浏览器中输入一个URL，回车之后便可以在浏览器中观察到页面内容。实际上，这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器。响应里包含了页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来，传输模型如图所示： 此处客户端即代表我们自己的 PC 或手机浏览器，服务器即要访问的网站所在的服务器。 为了更直观地说明这个过程，这里用浏览器的开发者模式下的Network监听组件来做演示，它可以显示访问当前请求网页时发生的所有网络请求和响应。打开浏览器（Chrome或Edge都可以），右击并选择 “检查”项，即可打开浏览器的开发者工具。这里访问百度，输入该 URL后回车，观察这个过程中发生了怎样的网络请求。可以看到，在Network页面下方 出现了一个个的条目，其中一个条目就代表一次发送请求和接收响应的过程，如图所示: 我们先观察第一个网络请求，即www.baidu.com，其中各列的含义如下。 第一列 Name：请求的名称，一般会将 URL的最后一部分内容当作名称。 第二列 Status：响应的状态码，这里显示为 200，代表响应是正常的。通过状态码，我们可以判断发送了请求之后是否得到了正常的响应。 第三列 Type：请求的文档类型。这里为document，代表我们这次请求的是一个 HTML文档，内容就是一些 HTML代码。 第四列 Initiator：请求源。用来标记请求是由哪个对象或进程发起的。 第五列 Size：从服务器下载的文件和请求的资源大小。如果是从缓存中取得的资源，则该列会显示 fromcache。 第六列 Time：发起请求到获取响应所用的总时间。 第七列 Waterfall：网络请求的可视化瀑布流。 我们点击这个条目即可看到其更详细的信息，如图所示。 首先是General部分，Request URL为请求的URL，Request Method为请求的方法，Status Code为响应状态码，Remote Address为远程服务器的地址和端口，Referrer Policy为 Referrer判别策略。 再继续往下，可以看到，有Response Headers和Request Headers，这分别代表响应头和请求头。请求头里带有许多请求信息，例如浏览器标识、Cookies、Host等信息，这是请求的一部分，服务器会根据请求头内的信息判断请求是否合法，进而作出对应的响应。图中看到的Response Headers就是响应的一部分，例如其中包含了服务器的类型、文档类型、日期等信息，浏览器接受到响应后，会解析响应内容，进而呈现网页内容。 请求请求，由客户端向服务端发出，可以分为4部分内容：请求方法（Request Method、请求的网址（Request URL）、请求头（Request Headers）、请求体（Request Body）。 请求方法常见的请求方法有两种：GET和POST。 在浏览器中直接输入URL并回车，这便发起了一个GET请求，请求的参数会直接包含到URL里。例如，在百度中搜索Python，这就是一个GET请求，链接为，其中URL中包 含了请求的参数信息，这里参数wd表示要搜寻的关键字。POST请求大多在表单提交时发起。比如，对于一个登录表单，输入用户名和密码后，点击“登录”按钮，这通常会发起一个POST请求，其数据通常 以表单的形式传输，而不会体现在URL中。 GET和POST请求方法有如下区别。 GET请求中的参数包含在URL里面，数据可以在URL中看到，而POST请求的URL不会包含这些数据，数据都是通过表单形式传输的，会包含在请求体中。 GET请求提交的数据最多只有1024字节，而POST请求没有限制。 一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用GET方式请求的话，密码就会暴露在URL里面，造成密码泄露，所以这里最好以POST方式发送。上传文件时，由于文件内容比较大，也会选用POST方式。 我们平常遇到的绝大部分请求都是GET或POST请求，另外还有一些请求方法，如HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE等，我们简单将其总结为下表。 方法 描述 GET 请求页面，并返回页面内容 HEAD 类似于GET请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 大多用于表单提交或上传文件，数据包含在请求体中 PUT 从客户端向服务器传送的数据取代指定文档中的内容 DELETE 请求服务器删除指定的页面 CONNECT 把服务器当作跳板，让服务器代替客户端访问其他网页 OPTIONS 允许客户端查看服务器的性能 TRACE 回显服务器收到的请求，主要用于测试或诊断 table { margin: auto; font-size: 50%; } 请求的网址，即统一资源定位符 URL，它可以唯一确定我们想请求的资源。 请求头请求头，用来说明服务器要使用的附加信息，比较重要的信息有Cookie、Referer、User-Agent等。下面简要说明一些常用的头信息。 Accept：请求报头域，用于指定客户端可接受哪些类型的信息。 Accept-Language：指定客户端可接受的语言类型。 Accept-Encoding：指定客户端可接受的内容编码。 Host：用于指定请求资源的主机IP和端口号，其内容为请求URL的原始服务器或网关的位置。从 HTTP 1.1 版本开始，请求必须包含此内容。 Cookie：也常用复数形式Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会 话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是Cookies的功劳。Cookies里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页 面时，都会在请求头中加上Cookies并将其发送给服务器，服务器通过Cookies识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。 Referer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、防盗链处理等。 User-Agent：简称UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫。 Content-Type：也叫互联网媒体类型（Internet Media Type）或者MIME类型，在HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html代表HTML格式，image/gif代表GIF图片，application/json代表JSON类型，更多对应关系可以查看此对照表：。 因此，请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。 请求体请请求体一般承载的内容是POST请求中的表单数据，而对于GET请求，请求体则为空。 登录之前，我们填写了用户名和密码信息，提交时这些内容就会以表单数据的形式提交给服务器，此时需要注意Request Headers中指定Content-Type为application/x-www-form-urlencoded。只有设置Content-Type为application/x-www-form-urlencoded，才会以表单数据的形式提交。另外，我们也可以将Content-Type设置为application/json来提交JSON数据，或者设置为multipart/form-data来上传文件。 表格中列出了Content-Type和POST提交数据方式的关系。 Content-Type 提交数据的方式 application/x-www-form-urlencodeed 表单数据 multipart/form-data 表单文件上传 application/json 序列化JSON数据 text/xml XML数据 在爬虫中，如果要构造POST请求，需要使用正确的Content-Type，并了解各种请求库的各个参数设置时使用的是哪种Content-Type，不然可能会导致POST提交后无法正常响应。 响应响应，由服务端返回给客户端，可以分为三部分：响应状态码（Response Status Code）、响应头（Response Headers）和响应体（Response Body）。 响应状态码响应状态码表示服务器的响应状态，如200代表服务器正常响应，404代表页面未找到，500代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为200，则证明成功返 回数据，再进行进一步的处理，否则直接忽略。下表列出了常见的错误代码及错误原因。 状态码 说明 详情 100 继续 请求者应当继续提出请求，服务器已收到请求的一部分，正在等待其余部分 101 切换协议 请求者已要求服务器切换协议，服务器已确认并确认切换 200 成功 服务器已成功处理了请求 201 已创建 请求成功并且服务器创建了新的资源 202 已接受 服务器已接受请求，但尚未处理 203 非授权信息 服务器已经成功处理请求，但返回的信息可能来自另一个源 204 无内容 服务器成功处理了请求，但没有返回任何内容 205 重置内容 服务器成功处理了请求，但内容被重置 206 部分内容 服务器成功处理了部分请求 300 多种选择 针对请求，服务器可执行多种操作 301 永久移动 请求的网页已永久移动到新位置，即永久重定向 302 临时移动 请求的网页暂时跳转到其他页面，即暂时重定向 303 查看其他位置 如果原来的请求是POST，重定向目标文档应当通过GET方式访问资源 304 未修改 此次请求返回的网页未修改，继续使用上次的资源 305 使用代理 请求者应该使用代理访问该网页 307 临时重定向 请求的资源临时从其他位置响应 400 错误请求 服务器无法解析该请求 401 未授权 请求没有进行身份验证或验证未通过 403 禁止访问 服务器拒绝此请求 404 未找到 服务器找不到请求的网页 405 方法禁用 服务器禁用了请求中指定的方法 406 不接受 无法使用请求的内容响应请求的网页 407 需要代理授权 请求者需要使用代理授权 408 请求超时 服务器请求超时 409 冲突 服务器在完成请求时发生冲突 410 已删除 请求的资源已永久删除 411 需要有效长度 服务器不接受不含有效内容长度标头字段的请求 412 未满足前提条件 服务器未满足请求者在请求中设置的其中一个前提条件 413 请求实体过大 请求实体过大，超出服务器的处理能力 415 请求URL过长 请求的网址过长，服务器无法处理 416 请求范围不符 页面无法提供请求页面支持 417 未满足期望值 服务器为满足期望请求标头字段的要求 500 服务器内部错误 服务器遇到错误，无法完成请求 501 未实现 服务器不具备完成请求的功能 502 错误网关 服务器作为网关或代理，从上游服务器收到无效响应 503 服务不可用 服务器目前无法使用 504 网关超时 服务器作为网关或代理，但是没有及时从上游服务器收到响应 505 HTTP版本不支持 服务器不支持请求中所用的HTTP协议版本 响应头包含了服务器对请求的应答信息，如Content-Type、Server、Set-Cookie等。下面简要说明一些常用的响应头信息。 Date：标识响应产生的时间。 Last-Modified：指定资源的最后修改时间。 Content-Encoding：指定响应内容的编码。 Server：包含服务器的信息，比如名称、版本号等。 Content-Type：文档类型，指定返回的数据类型是什么，如text/html代表返回HTML文档，application/x-javascript则代表返回JavaScript文件，image/jpeg则代表返回图片。 Set-Cookie：设置Cookies。响应头中的Set-Cookie告诉浏览器需要将此内容放在Cookies中，下次请求携带Cookies请求。 Expires：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。 响应体最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的HTML代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体，如图所示。 在浏览器开发者工具中点击Response，就可以看到网页的源代码，也就是响应体的内容，它是解析的目标。在做爬虫时，我们主要通过响应体得到网页的源代码、JSON数据等，然后从中做相应内容的提取。"},{"title":"Git常用命令","date":"2021-05-08T10:03:19.000Z","url":"/2021/05/08/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":[["Git","/tags/Git/"]],"categories":[["Git","/categories/Git/"]],"content":"Git常用命令仓库在当前目录新建一个Git代码库 新建一个目录，将其初始化为Git代码库 下载一个项目和它的整个代码历史 配置显示当前的Git配置 编辑Git配置文件 设置提交代码时的用户信息 增加/删除文件添加指定文件到暂存区 添加指定目录到暂存区，包括子目录 添加当前目录的所有文件到暂存区 添加每个变化前，都会要求确认对于同一个文件的多处变化，可以实现分次提交 停止追踪指定文件，但该文件会保留在工作区 改名文件，并且将这个改名放入暂存区 代码提交提交暂存区到仓库区 提交暂存区的指定文件到仓库区 提交工作区自上次commit之后的变化，直接到仓库区 提交时显示所有diff信息 使用一次新的commit，替代上一次提交,如果代码没有任何新变化，则用来改写上一次commit的提交信息 重做上一次commit，并包括指定文件的新变化 分支列出所有本地分支 列出所有远程分支 列出所有本地分支和远程分支 新建一个分支，但依然停留在当前分支 新建一个分支，并切换到该分支 新建一个分支，指向指定commit 新建一个分支，与指定的远程分支建立追踪关系 切换到指定分支，并更新工作区 切换到上一个分支 建立追踪关系，在现有分支与指定的远程分支之间 合并指定分支到当前分支 选择一个commit，合并进当前分支 删除分支 删除远程分支 标签列出所有tag 新建一个tag在当前commit 新建一个tag在指定commit 删除本地tag 删除远程tag 查看tag信息 提交指定tag 提交所有tag 新建一个分支，指向某个tag 查看信息显示有变更的文件 显示当前分支的版本历史 显示commit历史，以及每次commit发生变更的文件 搜索提交历史，根据关键词 显示某个commit之后的所有变动，每个commit占据一行 显示某个commit之后的所有变动，其”提交说明”必须符合搜索条件 显示某个文件的版本历史，包括文件改名 显示指定文件相关的每一次diff 显示过去5次提交 显示所有提交过的用户，按提交次数排序 显示指定文件是什么人在什么时间修改过 显示暂存区和工作区的差异 显示暂存区和上一个commit的差异 显示工作区与当前分支最新commit之间的差异 显示两次提交之间的差异 显示今天你写了多少行代码 显示某次提交的元数据和内容变化 显示某次提交发生变化的文件 显示某次提交时，某个文件的内容 显示当前分支的最近几次提交 远程同步下载远程仓库的所有变动 显示所有远程仓库 显示某个远程仓库的信息 增加一个新的远程仓库，并命名 取回远程仓库的变化，并与本地分支合并 上传本地指定分支到远程仓库 强行推送当前分支到远程仓库，即使有冲突 推送所有分支到远程仓库 撤销恢复暂存区的指定文件到工作区 恢复某个commit的指定文件到暂存区和工作区 恢复暂存区的所有文件到工作区 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 重置暂存区与工作区，与上一次commit保持一致 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 重置当前HEAD为指定commit，但保持暂存区和工作区不变 新建一个commit，用来撤销指定commit，后者的所有变化都将被前者抵消，并且应用到当前分支 暂时将未提交的变化移除，稍后再移入 其他生成一个可供发布的压缩包 "},{"title":"Hexo搭建个人博客","date":"2021-05-08T07:39:22.000Z","url":"/2021/05/08/Hexo%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["博客教程","/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"]],"content":"HexoHexo是一个快速、简洁且高效的博客框架。 安装git安装 安装 Node.jsNode.js 为大多数平台提供了官方的 安装程序。对于中国大陆地区用户，可以前往 淘宝 Node.js 镜像 下载。 其它的安装方法： 安装 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 安装完成后，win+R输入cmd调出命令行，输入hexo提示如下，说明安装正确。 建站安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 新建完成后，指定文件夹的目录如下： _config.yml网站的 配置 信息，您可以在此配置大部分的参数。 package.json应用程序的信息。EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。 scaffolds模版文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。 Hexo的模板是指在新建的文章文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。 source资源文件夹是存放用户资源的地方。除 _posts文件夹之外，开头命名为 _ (下划线)的文件/文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public文件夹，而其他文件会被拷贝过去。 themes主题文件夹。Hexo 会根据主题来生成静态页面。 配置相关配置可直接访问官方文档查看，我们先从使用别人的主题开始，官方提供了335个主题下载使用，你也可以根据规范制定自己的主题。 主题创建Hexo主题非常容易，您只要在themes文件夹内，新增一个任意名称的文件夹，并修改_config.yml内的theme设定，即可切换主题。一个主题可能会有以下的结构： _config.yml主题的配置文件。和 Hexo 配置文件不同，主题配置文件修改时会自动更新，无需重启Hexo Server。 获取主题选择相应的主题，从github上获取到themes目录下。 修改主目录下_config.yml中的配置文件，将theme修改为获取主题的文件夹名。 运行在主目录下调用cmd命令hexo server运行服务，访问进入博客。 添加文章"}]