<!DOCTYPE html>
<html lang="Zh">
    <head>
  <!-- 元数据 -->
  <meta charset="utf-8">
  <link rel="icon" href="/images/logo.webp">
  <title>scrapy的基本使用 | Holy的个人站点</title>
  <meta name="author" content="Holy Chan" />
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <meta name="format-detection" content="telphone=no, email=no" />
  
    <meta name="keywords" content="scrapy" />
  
  <meta name="description" content="抓取目标要完成的任务如下。  创建一个Scrapy项目。 创建一个Spider来抓取站点和处理数据。 通过命令行将抓取的内容导出。 将抓取的内容保存到MongoDB数据库。  抓取的目标站点为。 准备工作安装好scrapy框架、MongoDB和PyMongo库。 创建项目创建一个Scrapy项目，项目文件可以直接用Scrapy命令生成，命令如下所示： 1scrapy startproject tu">
<meta property="og:type" content="article">
<meta property="og:title" content="scrapy的基本使用">
<meta property="og:url" content="http://example.com/2021/05/29/scrapy%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/index.html">
<meta property="og:site_name" content="Holy的个人站点">
<meta property="og:description" content="抓取目标要完成的任务如下。  创建一个Scrapy项目。 创建一个Spider来抓取站点和处理数据。 通过命令行将抓取的内容导出。 将抓取的内容保存到MongoDB数据库。  抓取的目标站点为。 准备工作安装好scrapy框架、MongoDB和PyMongo库。 创建项目创建一个Scrapy项目，项目文件可以直接用Scrapy命令生成，命令如下所示： 1scrapy startproject tu">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/images/logo.webp">
<meta property="article:published_time" content="2021-05-29T14:49:55.000Z">
<meta property="article:modified_time" content="2021-05-30T04:56:02.000Z">
<meta property="article:author" content="Holy Chan">
<meta property="article:tag" content="scrapy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/logo.webp">
<meta name="twitter:site" content="@null">
  
  <!-- 站点验证相关 -->
  
    
    
    
  
  <!-- 样式表文件 -->
  <link rel="stylesheet" id="kratos-css" href="/css/kratosr.min.css" type="text/css" media="all">
  
    <link rel="stylesheet" id="highlight-css" href="/css/highlight/night-eighties.min.css" type="text/css" media="all">
  
  
  <link rel="stylesheet" id="fontawe-css" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css" media="all">
  <link rel="stylesheet" id="nprogress-css" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" type="text/css" media="all">
  
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
  
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  
    <link rel="stylesheet" id="darkmode-css" href="/css/kr-dark.min.css" type="text/css" media="all">
  
  <!-- 不得不预先加载的一些JS文件 -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
  
    <script src="https://cdn.jsdelivr.net/npm/qrcode_js@1.0.0/qrcode.min.js"></script>
  
<meta name="generator" content="Hexo 5.4.0"></head>


    <body class="custom-background">
        <div id="kratos-wrapper">
    <div id="kratos-page">
        <div id="kratos-header">
            <header id="kratos-desktop-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="nav-header">
                        <nav id="kratos-menu-wrap">
                            <ul id="kratos-primary-menu" class="sf-menu">
                                
                                    
                                        <li><a href="/"><i class="fa fa-home"></i>首页</a></li>
                                    
                                
                                    
                                        <li><a href="/archives/"><i class="fa fa-file"></i>归档</a></li>
                                    
                                
                                    
                                        <li><a href="/about/"><i class="fa fa-paw"></i>关于我</a></li>
                                    
                                
                                    
                                        <li>
                                            <a><i class="fa fa-link"></i>链接</a>
                                            <ul class="sub-menu">
                                                
                                                    
                                                
                                                    
                                                        <li><a target="_blank" rel="noopener" href="http://holychan.ltd:8000">个人网盘</a></li>
                                                    
                                                
                                            </ul>
                                        </li>
                                    
                                
                            </ul>
                        </nav>
                    </div>
                </div>
            </header>
            <header id="kratos-mobile-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="color-logo"><a href="/">Holy的个人站点</a></div>
                    <div class="nav-toggle">
                        <a class="kratos-nav-toggle js-kratos-nav-toggle">
                            <i></i>
                        </a>
                    </div>
                </div>
            </header>
        </div>
        <div class="kratos-start kratos-hero-2">
            <!-- <div class="kratos-overlay"></div> -->
            <div class="kratos-cover kratos-cover-2 text-center">
                <div class="desc desc2 animate-box">
                    <a href="/">
                        <h2>Holy的个人站点</h2> <br />
                        <span></span>
                    </a>
                </div>
            </div>
        </div>

        <div id="kratos-blog-post">
            <div class="container">
                <div id="main" class="row">
                    

        <section class="col-md-8">
    <article>
        <div class="kratos-hentry kratos-post-inner clearfix">
            <header class="kratos-entry-header">
                
                    <h1 class="kratos-entry-title text-center">scrapy的基本使用</h1>
                
                
                <ul class="kratos-post-meta text-center">
                    <li><i class="fa fa-calendar"></i> 2021-05-29</li>
                    <li><i class="fa fa-user"></i> 作者 Holy Chan</li>
                    <li>
                        <i class="fa fa-edit"></i> 
                        
                        
                            ~8.90K
                        
                        字
                    </li>
                    
                </ul>
            </header>
            <div class="kratos-post-content">
                <div id="expire-alert" class="alert alert-warning hidden" role="alert">
                    本文最后编辑于 <time datetime="1622350562000"></time> 前，其中的内容可能需要更新。
                </div>
                
                    <div class="kratos-post-inner-toc">
                        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%93%E5%8F%96%E7%9B%AE%E6%A0%87"><span class="toc-number">1.</span> <span class="toc-text">抓取目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">2.</span> <span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="toc-number">3.</span> <span class="toc-text">创建项目</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BASpider"><span class="toc-number">4.</span> <span class="toc-text">创建Spider</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAItem"><span class="toc-number">5.</span> <span class="toc-text">创建Item</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90Response"><span class="toc-number">6.</span> <span class="toc-text">解析Response</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Item"><span class="toc-number">7.</span> <span class="toc-text">使用Item</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8E%E7%BB%ADRequest"><span class="toc-number">8.</span> <span class="toc-text">后续Request</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C"><span class="toc-number">9.</span> <span class="toc-text">运行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%88%B0%E6%96%87%E4%BB%B6"><span class="toc-number">10.</span> <span class="toc-text">保存到文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8ItemPileline"><span class="toc-number">11.</span> <span class="toc-text">使用ItemPileline</span></a></li></ol>
                    </div>
                
                <hr />
                <h2 id="抓取目标"><a href="#抓取目标" class="headerlink" title="抓取目标"></a>抓取目标</h2><p>要完成的任务如下。</p>
<ul>
<li>创建一个<code>Scrapy</code>项目。</li>
<li>创建一个<code>Spider</code>来抓取站点和处理数据。</li>
<li>通过命令行将抓取的内容导出。</li>
<li>将抓取的内容保存到<code>MongoDB</code>数据库。</li>
</ul>
<p>抓取的目标站点为<a target="_blank" rel="noopener" href="http://quotes.toscrape.com/"></a>。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>安装好<code>scrapy</code>框架、<code>MongoDB</code>和<code>PyMongo</code>库。</p>
<h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h2><p>创建一个<code>Scrapy</code>项目，项目文件可以直接用<code>Scrapy</code>命令生成，命令如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject tutorial</span><br></pre></td></tr></table></figure>

<p>这个命令将会创建一个名为<code>tutorial</code>的文件夹，文件夹结构如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scrapy.cfg    <span class="comment"># Scrapy 部署时的配置文件 </span></span><br><span class="line">tutorial    <span class="comment"># 项目的模块，引入的时候需要从这里引入 </span></span><br><span class="line">  __init__.py </span><br><span class="line">  items.py    <span class="comment"># Items 的定义，定义爬取的数据结构 </span></span><br><span class="line">  middlewares.py     <span class="comment"># Middlewares 的定义，定义爬取时的中间件 </span></span><br><span class="line">  pipelines.py    <span class="comment"># Pipelines 的定义，定义数据管道 </span></span><br><span class="line">  settings.py     <span class="comment"># 配置文件 </span></span><br><span class="line">  spiders     <span class="comment"># 放置 Spiders 的文件夹 </span></span><br><span class="line">    __init__.py</span><br></pre></td></tr></table></figure>

<h2 id="创建Spider"><a href="#创建Spider" class="headerlink" title="创建Spider"></a>创建Spider</h2><p><code>Spider</code>是自己定义的类，<code>Scrapy</code>用它从网页里抓取内容，并解析抓取的结果。不过这个类必须继承<code>Scrapy</code>提供的<code>Spider</code>类<code>scrapy.Spider</code>，还要定义<code>Spider</code>的名称和起始请求，以及怎样处理爬取后的结果的方法。</p>
<p>也可以使用命令行创建一个<code>Spider</code>。比如要生成<code>Quotes</code>这个<code>Spider</code>，可以执行如下命令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>scrapy genspider quotes quotes.toscrape.com     </span><br><span class="line">Created spider <span class="string">&#x27;quotes&#x27;</span> using template <span class="string">&#x27;basic&#x27;</span> <span class="keyword">in</span> module:</span><br><span class="line">  tutorial.spiders.quotes</span><br></pre></td></tr></table></figure>

<p>进入刚才创建的<code>tutorial</code>文件夹，然后执行<code>genspider</code>命令。第一个参数是<code>Spider</code>的名称，第二个参数是<code>网站域名</code>。执行完毕之后，<code>spiders</code>文件夹中多了一个<code>quotes.py</code>，它就是刚刚创建的<code>Spider</code>，内容如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;quotes&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;quotes.toscrape.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>这里有三个属性——<code>name</code>、<code>allowed_domains</code>和<code>start_urls</code>，还有一个方法<code>parse</code>。</p>
<ul>
<li><code>name</code>：它是每个项目唯一的名字，用来区分不同的<code>Spider</code>。</li>
<li><code>allowed_domains</code>：它是允许爬取的域名，如果初始或后续的请求链接不是这个域名下的，则请求链接会被过滤掉。</li>
<li><code>start_urls</code>：它包含了<code>Spider</code>在启动时爬取的<code>url</code>列表，初始请求是由它来定义的。</li>
<li><code>parse</code>：它是<code>Spider</code>的一个方法。默认情况下，被调用时<code>start_urls</code>里面的链接构成的请求完成下载执行后，返回的响应就会作为唯一的参数传递给这个函数。该方法负责解析返回的响应、提取数据或者进一 步生成要处理的请求。</li>
</ul>
<h2 id="创建Item"><a href="#创建Item" class="headerlink" title="创建Item"></a>创建Item</h2><p><code>Item</code>是保存爬虫数据的容器，它的使用方法和字典类似，不过相比字典，<code>Item</code>多了额外的保护机制，可以避免拼写错误或者定义字段错误。</p>
<p>创建<code>Item</code>需要继承<code>scrapy.Item</code>类，并且定义类型为<code>scrapy.Field</code>的字段。观察目标网站，可以获取到的内容有<code>text</code>、<code>author</code>、<code>tags</code>。</p>
<p>定义<code>Item</code>，此时将<code>items.py</code>修改如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuoteItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    text = scrapy.Field()</span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>这里定义了三个字段，将类的名称修改为<code>QuoteItem</code>，接下来爬取时会使用到这个<code>Item</code>。</p>
<h2 id="解析Response"><a href="#解析Response" class="headerlink" title="解析Response"></a>解析Response</h2><p><code>parse</code>方法的参数<code>response</code>是<code>start_urls</code>里面的链接爬取后的结果。所以在<code>parse</code>方法中，可以直接对<code>response</code>变量包含的内容进行解析，比如浏览请求结果的网页源代码，或者进一步分析源代码内容，或者找出结果中的链接而得到下一个请求。</p>
<p>首先看看网页结构，如图所示。每一页都有多个<code>class</code>为<code>quote</code>的区块，每个区块内都包含<code>text</code>、<code>author</code>、<code>tags</code>。那么先找出所有的<code>quote</code>，然后提取每一个<code>quote</code>中的内容。</p>
<p><img src="Screenshot_1.webp"></p>
<p>提取的方式可以是<code>CSS</code>选择器或<code>XPath</code>选择器。在这里使用<code>CSS</code>选择器进行选择，<code>parse</code>方法的改写如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;quotes&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://http://quotes.toscrape.com//&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        quotes = response.css(<span class="string">&#x27;.quote&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">            text = quote.css(<span class="string">&#x27;.text::text&#x27;</span>).extract_first()</span><br><span class="line">            author = quote.css(<span class="string">&#x27;.author::text&#x27;</span>).extract_first()</span><br><span class="line">            tags = quote.css(<span class="string">&#x27;.tags .tag::text&#x27;</span>).extract()</span><br></pre></td></tr></table></figure>

<p>对于<code>text</code>，获取结果的第一个元素即可，所以使用<code>extract_first</code>方法，对于<code>tags</code>，要获取所有结果组成的列表，所以使用<code>extract</code>方法。</p>
<h2 id="使用Item"><a href="#使用Item" class="headerlink" title="使用Item"></a>使用Item</h2><p>上文定义了<code>Item</code>，接下来就要使用它了。<code>Item</code>可以理解为一个字典，不过在声明的时候需要实例化。然后依次用刚才解析的结果赋值<code>Item</code>的每一个字段，最后将<code>Item</code>返回即可。</p>
<p><code>QuotesSpider</code>的改写如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">    quotes = response.css(<span class="string">&#x27;.quote&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">        item = QuoteItem()</span><br><span class="line">        item[<span class="string">&#x27;text&#x27;</span>] = quote.css(<span class="string">&#x27;.text::text&#x27;</span>).extract_first()</span><br><span class="line">        item[<span class="string">&#x27;author&#x27;</span>] = quote.css(<span class="string">&#x27;.author::text&#x27;</span>).extract_first()</span><br><span class="line">        item[<span class="string">&#x27;tags&#x27;</span>] = quote.css(<span class="string">&#x27;.tags .tag::text&#x27;</span>).extract()</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<p>如此一来，首页的所有内容被解析出来，并被赋值成了一个个<code>QuoteItem</code>。</p>
<h2 id="后续Request"><a href="#后续Request" class="headerlink" title="后续Request"></a>后续Request</h2><p>接下来就需要从当前页面中找到信息来生成下一个请求，然后在下一个请求的页面里面找到信息再构造下一个请求。这样循环往复迭代，从而实现整站的爬取。</p>
<p>将刚才的页面拉到最底部，如图所示。</p>
<p><img src="Screenshot_2.webp"></p>
<p>有一个<code>Next</code>按钮，查看一下源代码，可以发现它的链接是<code>/page/2/</code>，实际上全链接就是：<code>http://quotes.toscrape.com/page/2</code>，通过这个链接我们就可以构造下一个请求。</p>
<p>构造请求时需要用到<code>scrapy.Request</code>。这里传递两个参数——<code>url</code>和<code>callback</code>，这两个参数的说明如下。</p>
<ul>
<li><code>url</code>：它是请求链接。</li>
<li><code>callback</code>：它是回调函数。当指定了该回调函数的请求完成之后，获取到响应，引擎会将该响应作为参数传递给这个回调函数。回调函数进行解析或生成下一个请求，回调函数如上文的<code>parse()</code>所示。</li>
</ul>
<p>由于<code>parse</code>就是解析<code>text</code>、<code>author</code>、<code>tags</code>的方法，而下一页的结构和刚才已经解析的页面结构是一样的，所以可以再次使用<code>parse</code>方法来做页面解析。</p>
<p>接下来要做的就是利用选择器得到下一页链接并生成请求，在<code>parse</code>方法后追加如下的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span> = response.css(<span class="string">&#x27;.pager .next a::attr(href)&#x27;</span>).extract_first()</span><br><span class="line">url = response.urljoin(<span class="built_in">next</span>)</span><br><span class="line"><span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse, dont_filter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>第一句代码首先通过<code>CSS</code>选择器获取下一个页面的链接，即要获取<code>a</code>超链接中的<code>href</code>属性。这里用到了<code>::attr(href)</code>操作。然后再调用<code>extract_first</code>方法获取内容。</p>
<p>第二句代码调用了<code>urljoin</code>方法，<code>urljoin()</code>方法可以将相对<code>URL</code>构造成一个绝对的<code>URL</code>。例如，获取到的下一页地址是<code>/page/2</code>，<code>urljoin</code>方法处理后得到的结果就是：<code>http://quotes.toscrape.com/page/2/</code>。</p>
<p>第三句代码通过<code>url</code>和<code>callback</code>变量构造了一个新的请求，回调函数<code>callback</code>依然使用<code>parse</code>方法。这个请求完成后，响应会重新经过<code>parse</code>方法处理，得到第二页的解析结果，然后生成第二页的下一页，也就是第三页的请求。这样爬虫就进入了一个循环，直到最后一页。<code>dont_filter</code>设置为<code>Ture</code>不进行域名过滤，这样就能继续爬取。</p>
<p>通过几行代码，就轻松实现了一个抓取循环，将每个页面的结果抓取下来了。现在，改写之后的整个<code>Spider</code>类如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> QuoteItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;quotes&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;quotes.toscrape.com/&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        quotes = response.css(<span class="string">&#x27;.quote&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">            item = QuoteItem()</span><br><span class="line">            item[<span class="string">&#x27;text&#x27;</span>] = quote.css(<span class="string">&#x27;.text::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;author&#x27;</span>] = quote.css(<span class="string">&#x27;.author::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;tags&#x27;</span>] = quote.css(<span class="string">&#x27;.tags .tag::text&#x27;</span>).extract()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="built_in">next</span> = response.css(<span class="string">&#x27;.pager .next a::attr(href)&#x27;</span>).extract_first()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">next</span> !== <span class="string">&quot;&quot;</span>:</span><br><span class="line">          url = response.urljoin(<span class="built_in">next</span>)</span><br><span class="line">          <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse, dont_filter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>进入目录，运行如下命令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure>

<p>爬虫一边解析，一边翻页，直至将所有内容抓取完毕，然后终止。</p>
<p>最后，<code>Scrapy</code>输出了整个抓取过程的统计信息，如请求的字节数、请求次数、响应次数、完成原因等。</p>
<h2 id="保存到文件"><a href="#保存到文件" class="headerlink" title="保存到文件"></a>保存到文件</h2><p><code>Scrapy</code>提供的<code>Feed Exports</code>可以轻松将抓取结果输出。例如，想将上面的结果保存成<code>JSON</code>文件，可以执行如下命令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.json</span><br></pre></td></tr></table></figure>

<p>命令运行后，项目内多了一个<code>quotes.json</code>文件，文件包含了刚才抓取的所有内容，内容是<code>JSON</code>格式。</p>
<p><img src="Screenshot_3.webp"></p>
<p>另外还可以每一个<code>Item</code>输出一行<code>JSON</code>，输出后缀为<code>jl</code>，为<code>jsonline</code>的缩写，命令如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.jl</span><br></pre></td></tr></table></figure>

<p>或</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.jsonlines</span><br></pre></td></tr></table></figure>

<p>输出格式还支持很多种，例如<code>csv</code>、<code>xml</code>、<code>pickle</code>、<code>marshal</code>等，还支持<code>ftp</code>、<code>s3</code> 等远程输出，另外还可以通过自定义<code>ItemExporter</code>来实现其他的输出。</p>
<p>例如，下面命令对应的输出分别为<code>csv</code>、<code>xml</code>、<code>pickle</code>、<code>marshal</code>格式以及<code>ftp</code>远程输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.csv</span><br><span class="line">scrapy crawl quotes -o quotes.xml</span><br><span class="line">scrapy crawl quotes -o quotes.pickle</span><br><span class="line">scrapy crawl quotes -o quotes.marshal</span><br><span class="line">scrapy crawl quotes -o ftp://user:<span class="keyword">pass</span>@ftp.example.com/path/to/quotes.csv </span><br></pre></td></tr></table></figure>

<p>其中，<code>ftp</code>输出需要正确配置用户名、密码、地址、输出路径，否则会报错。通过<code>Scrapy</code>提供的<code>Feed Exports</code>，可以轻松地输出抓取结果到文件。对于一些小型项目来说，这应该足够了。不过如果想要更复杂的输出，如输出到数据库等，可以使用<code>ItemPileline</code>来完成。</p>
<h2 id="使用ItemPileline"><a href="#使用ItemPileline" class="headerlink" title="使用ItemPileline"></a>使用ItemPileline</h2><p>如果想进行更复杂的操作，如将结果保存到<code>MongoDB</code>数据库，或者筛选某些有用的<code>Item</code>，则可以定义<code>ItemPipeline</code>来实现。</p>
<p><code>ItemPipeline</code>为项目管道。当<code>Item</code>生成后，它会自动被送到<code>ItemPipeline</code>进行处理，常用<code>ItemPipeline</code>来做如下操作。</p>
<ul>
<li>清洗<code>HTML</code>数据；</li>
<li>验证爬取数据，检查爬取字段；</li>
<li>查重并丢弃重复内容；</li>
<li>将爬取结果储存到数据库。</li>
</ul>
<p>要实现<code>ItemPipeline</code>很简单，只需要定义一个类并实现<code>process_item</code>方法即可。启用<code>ItemPipeline</code>后，<code>ItemPipeline</code>会自动调用这个方法。<code>process_item</code>方法必须返回包含数据的字典或<code>Item</code>对象，或者抛出<code>DropItem</code>异常。</p>
<p><code>process_item</code>方法有两个参数。一个参数是<code>item</code>，每次<code>Spider</code>生成的<code>Item</code>都会作为参数传递过来。另一个参数是<code>spider</code>，就是<code>Spider</code>的实例。</p>
<p>接下来，实现一个<code>ItemPipeline</code>，筛掉<code>text</code>长度大于<code>50</code>的<code>Item</code>，并将结果保存到<code>MongoDB</code>。</p>
<p>修改项目里的<code>pipelines</code>.py文件，之前用命令行自动生成的文件内容可以删掉，增加一个<code>TextPipeline</code>类，内容如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.limit = <span class="number">50</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">&#x27;text&#x27;</span>]:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="string">&#x27;text&#x27;</span>]) &gt; self.limit:</span><br><span class="line">                item[<span class="string">&#x27;text&#x27;</span>] =item[<span class="string">&#x27;text&#x27;</span>][<span class="number">0</span>:self.limit].rstrip() + <span class="string">&#x27;...&#x27;</span></span><br><span class="line">                <span class="keyword">return</span> item</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> DropItem</span><br></pre></td></tr></table></figure>

<p>这段代码在构造方法里定义了限制长度为50，实现了<code>process_item</code>方法，其参数是<code>item</code>和<code>spider</code>。首先该方法判断<code>item</code>的<code>text</code>属性是否存在，如果不存在，则抛出<code>DropItem</code>异常；如果存在，再判断长度是否大于50，如果大于，那就截断然后拼接省略号，再将<code>item</code>返回即可。</p>
<p>将处理后的<code>item</code>存入<code>MongoDB</code>，定义另外一个<code>Pipeline</code>。同样在<code>pipelines.py</code>中，实现另一个类<code>MongoPipeline</code>，内容如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="keyword">return</span> cls(mongo_uri=crawler.settings.get(<span class="string">&#x27;MONGO_URI&#x27;</span>), mongo_db=crawler.settings.get(<span class="string">&#x27;MONGO_DB&#x27;</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span> </span><br><span class="line">        name = item.__class__.__name__ </span><br><span class="line">        self.db[name].insert(<span class="built_in">dict</span>(item)) </span><br><span class="line">        <span class="keyword">return</span> item </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span> </span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure>

<p><code>MongoPipeline</code>类实现了<code>API</code>定义的另外几个方法。</p>
<ul>
<li><code>from_crawler</code>：这是一个类方法，用<code>@classmethod</code>标识，是一种依赖注入的方式，方法的参数就是<code>crawler</code>，通过<code>crawler</code>这个参数我们可以拿到全局配置的每个配置信息，在全局配置<code>settings.py</code>中可以定义<code>MONGO_URI</code>和<code>MONGO_DB</code>来指定<code>MongoDB</code>连接需要的地址和数据库名称，拿到配置信息之后返回类对象即可。所以这个方法的定义主要是用来获取<code>settings.py</code>中的配置的。</li>
<li><code>open_spider</code>：当<code>Spider</code>被开启时，这个方法被调用。在这里主要进行了一些初始化操作。</li>
<li><code>close_spider</code>：当<code>Spider</code>被关闭时，这个方法会调用，在这里将数据库连接关闭。</li>
</ul>
<p>最主要的<code>process_item</code>方法则执行了数据插入操作。</p>
<p>定义好<code>TextPipeline</code>和<code>MongoPipeline</code>这两个类后，需要在<code>settings.py</code>中使用它们。<code>MongoDB</code>的连接信息还需要定义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;tutorial.pipelines.TextPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;tutorial.pipelines.MongoPipeline&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br><span class="line">MONGO_URI=<span class="string">&#x27;localhost&#x27;</span></span><br><span class="line">MONGO_DB=<span class="string">&#x27;tutorial&#x27;</span></span><br></pre></td></tr></table></figure>

<p>赋值<code>ITEM_PIPELINES</code>字典，键名是<code>Pipeline</code>的类名称，键值是调用优先级，是一个数字，数字越小则对应的<code>Pipeline</code>越先被调用。<br>再重新执行爬取，命令如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure>

<p>爬取结束后，<code>MongoDB</code>中创建了一个<code>tutorial</code>的数据库、<code>QuoteItem</code>的表。</p>

            </div>
            
                <div class="kratos-copyright text-center clearfix">
                    <h5>本作品采用 <a rel="license nofollow" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 国际许可协议</a> 进行许可</h5>
                </div>
            
            <footer class="kratos-entry-footer clearfix">
                
                    <div class="post-like-donate text-center clearfix" id="post-like-donate">
                    
                    
                        <a class="share" href="javascript:;"><i class="fa fa-share-alt"></i> 分享</a>
                        <div class="share-wrap" style="display: none;">
    <div class="share-group">
        <a href="javascript:;" class="share-plain qq" onclick="share('qq');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-qq"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain qzone" onclick="share('qzone');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-star"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weixin pop style-plain" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weixin"></i>
            </div>
            <div class="share-int">
                <div class="qrcode" id="wechat-qr"></div>
                <p>打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮</p>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weibo" onclick="share('weibo');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weibo"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain facebook style-plain" onclick="share('facebook');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-facebook"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain twitter style-plain" onclick="share('twitter');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-twitter"></i>
            </div>
        </a>
    </div>
    <script type="text/javascript">
        $(()=>{
            new QRCode("wechat-qr", {
                text: "http://example.com/2021/05/29/scrapy%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/",
                width: 150,
                height: 150,
                correctLevel : QRCode.CorrectLevel.H
            });
        });
        function share(dest) {
            const qqBase        = "https://connect.qq.com/widget/shareqq/index.html?";
            const weiboBase     = "https://service.weibo.com/share/share.php?";
            const qzoneBase     = "https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?";
            const facebookBase  = "https://www.facebook.com/sharer/sharer.php?";
            const twitterBase   = "https://twitter.com/intent/tweet?";
            const hostUrl       = "http://example.com/2021/05/29/scrapy%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/";
            const title         = "「scrapy的基本使用」";
            const excerpt       = `抓取目标要完成的任务如下。

创建一个Scrapy项目。
创建一个Spider来抓取站点和处理数据。
通过命令行将抓取的内容导出。
将抓取的内容保存到MongoDB数据库。

抓取的目标站点为。
准备工作安装好scrapy框架、Mon...`;
            let _URL;
            switch (dest) {
                case "qq"       : _URL = qqBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";     break;
                case "weibo"    : _URL = weiboBase+"url="+hostUrl+"&title="+title+excerpt;                                 break;
                case "qzone"    : _URL = qzoneBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";  break;
                case "facebook" : _URL = facebookBase+"u="+hostUrl;                                                        break;
                case "twitter"  : _URL = twitterBase+"text="+title+excerpt+"&url="+hostUrl;                                break;
            }
            window.open(_URL);
        };
    </script>
</div>
                    
                    </div>
                
                <div class="footer-tag clearfix">
                    <div class="pull-left">
                    <i class="fa fa-tags"></i>
                        <a class="tag-none-link" href="/tags/scrapy/" rel="tag">scrapy</a>
                    </div>
                    <div class="pull-date">
                    <span>最后编辑：2021-05-30</span>
                    </div>
                </div>
            </footer>
        </div>
        
            <nav class="navigation post-navigation clearfix" role="navigation">
                
                <div class="nav-previous clearfix">
                    <a title=" scrapy框架介绍" href="/2021/05/29/scrapy框架介绍/">&lt; 上一篇</a>
                </div>
                
                
            </nav>
        
        
    </article>
</section>

                
            

<section id="kratos-widget-area" class="col-md-4 hidden-xs hidden-sm">
    <!-- 文章和页面根据splitter来分割，没有的话就从头开始设置为sticky -->
    
    
                <aside id="krw-about" class="widget widget-kratos-about clearfix">
    <div class="photo-background"></div>
    <div class="photo-wrapper clearfix">
        <div class="photo-wrapper-tip text-center">
            <img class="about-photo" src="/images/my.webp" />
        </div>
    </div>
    <div class="textwidget">
        <p class="text-center"></p>
    </div>
</aside>
            
                    <div class="sticky-area">
                
                    <aside id="krw-toc" class="widget widget-kratos-toc clearfix">
    <div class="photo-background"></div>
    <h4 class="widget-title no-after">
        <i class="fa fa-compass"></i>
        文章目录
        <span class="toc-progress-bar"></span>
    </h4>
    <div class="textwidget">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%93%E5%8F%96%E7%9B%AE%E6%A0%87"><span class="toc-text">抓取目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="toc-text">创建项目</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BASpider"><span class="toc-text">创建Spider</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAItem"><span class="toc-text">创建Item</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90Response"><span class="toc-text">解析Response</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Item"><span class="toc-text">使用Item</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8E%E7%BB%ADRequest"><span class="toc-text">后续Request</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C"><span class="toc-text">运行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%88%B0%E6%96%87%E4%BB%B6"><span class="toc-text">保存到文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8ItemPileline"><span class="toc-text">使用ItemPileline</span></a></li></ol>
    </div>
</aside>
                
                
  <aside id="krw-categories" class="widget widget-kratos-categories clearfix">
    <h4 class="widget-title"><i class="fa fa-folder"></i>分类目录</h4>
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Go/">Go</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Nginx/">Nginx</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/">博客教程</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BA%84%E5%AD%90/">庄子</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">14</span></li></ul>
  </aside>


            
                
  <aside id="krw-tags" class="widget widget-kratos-tags clearfix">
    <h4 class="widget-title"><i class="fa fa-tags"></i>标签聚合</h4>
      <div class="tag-clouds">
        <a href="/tags/Git/" style="font-size: 0.6em;">Git</a> <a href="/tags/Hexo/" style="font-size: 0.73em;">Hexo</a> <a href="/tags/Nginx/" style="font-size: 0.6em;">Nginx</a> <a href="/tags/ajax/" style="font-size: 0.67em;">ajax</a> <a href="/tags/css/" style="font-size: 0.6em;">css</a> <a href="/tags/docker/" style="font-size: 0.73em;">docker</a> <a href="/tags/git/" style="font-size: 0.6em;">git</a> <a href="/tags/go/" style="font-size: 0.6em;">go</a> <a href="/tags/html/" style="font-size: 0.6em;">html</a> <a href="/tags/http/" style="font-size: 0.8em;">http</a> <a href="/tags/https/" style="font-size: 0.6em;">https</a> <a href="/tags/javascript/" style="font-size: 0.67em;">javascript</a> <a href="/tags/linux/" style="font-size: 0.67em;">linux</a> <a href="/tags/mongodb/" style="font-size: 0.6em;">mongodb</a> <a href="/tags/multiprocessing/" style="font-size: 0.6em;">multiprocessing</a> <a href="/tags/nextCloud/" style="font-size: 0.6em;">nextCloud</a> <a href="/tags/nginx/" style="font-size: 0.6em;">nginx</a> <a href="/tags/pymongodb/" style="font-size: 0.6em;">pymongodb</a>
      </div>
  </aside>

            
                
  <aside id="krw-posts" class="widget widget-kratos-posts">
  <h4 class="widget-title"><i class="fa fa-file"></i>最新文章</h4>
  <div class="tab-content">
      <ul class="list-group">
        
        
          
          
            <a class="list-group-item" href="/2021/05/29/scrapy%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"><i class="fa  fa-book"></i> scrapy的基本使用</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/05/29/scrapy%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/"><i class="fa  fa-book"></i> scrapy框架介绍</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/05/29/%E5%86%85%E7%AF%87%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E9%BD%90%E7%89%A9%E8%AE%BA/"><i class="fa  fa-book"></i> 内篇（二）——齐物论</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/05/29/%E5%86%85%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E9%80%8D%E9%81%A5%E6%B8%B8/"><i class="fa  fa-book"></i> 内篇（一）——逍遥游</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/05/28/python%E7%9A%84%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"><i class="fa  fa-book"></i> python的内置类型</a>
            
          
        
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
      </ul>
  </div>
  </aside>

            
    </div>
</section>
        
        </div>
    </div>
</div>
<footer>
    <div id="footer"  class="ap-lrc"  >
        <div class="kr-tool text-center">
            <div class="tool">
                
                    <div class="box search-box">
                        <a href="/search/">
                            <span class="fa fa-search"></span>
                        </a>
                    </div>
                
                
                    <div class="box theme-box" id="darkmode-switch">
                        <span class="fa fa-adjust"></span>
                    </div>
                
                
            </div>
            <div class="box gotop-box">
                <span class="fa fa-chevron-up"></span>
            </div>
        </div>
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-3 footer-list text-center">
                    <ul class="kratos-social-icons">
                        
                        <li><a href="mailto:espholychan@outlook.com"><i class="fa fa-envelope"></i></a></li>
                        
                        
                        
                        
                        
                        
                        
                    </ul>
                    <ul class="kratos-copyright">
                        <div>
                            <li>&copy; 2021 Holy的个人站点 版权所有.</li>
                            <li>本站已运行<span id="span_dt">Loading...</span></li>
                        </div>
                        <div>
                            <li>Theme <a href="https://github.com/Candinya/Kratos-Rebirth" target="_blank">Kratos:Rebirth</a></li>
                            <li>Site built with&nbsp;<i class="fa fa-heart throb" style="color:#d43f57"></i>&nbsp;by Holy Chan.</li>
                        </div>
                        <div>
                            <li>Powered by <a href="https://hexo.io" target="_blank" rel="nofollow">Hexo</a></li>
                            
                        </div>
                        <div>
                            <li><a href="https://beian.miit.gov.cn" rel="external nofollow" target="_blank">鄂ICP备2021008617号-1</a></li>
                            
                        </div>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</footer>
</div>
</div>

        <script defer src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.4/dist/js/bootstrap.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.js"></script>
<script>const notMobile = (!(navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)));</script>

    <div>
        <canvas id="snow"></canvas>
        <script async type="text/javascript" src="/js/snow.min.js"></script>
    </div>

<script async src="/js/candy.min.js"></script>

    <script defer src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>
    <meting-js
        server="netease"
        type="playlist"
        id="3204190542"
        order="random"
        fixed="true"
    >
    </meting-js>



    <script defer src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script>
<script defer src="/js/kratosr.min.js"></script>
<script defer src="/js/pjax.min.js"></script>


    <script defer src="/js/kr-dark.min.js"></script>



<!-- Extra support for third-party plguins  -->


    </body>
</html>