<!DOCTYPE html>
<html lang="Zh">
    <head>
  <!-- 元数据 -->
  <meta charset="utf-8">
  <link rel="icon" href="/images/logo.webp">
  <title>request、pyquest和pymongodb案例实战 | Holy的个人站点</title>
  <meta name="author" content="Holy Chan" />
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <meta name="format-detection" content="telphone=no, email=no" />
  
    <meta name="keywords" content="pymongodb, pyquest, request" />
  
  <meta name="description" content="准备工作在本节课开始之前，我们需要做好如下的准备工作：  安装好Python3（最低为 3.6 版本），并能成功运行Python3程序。 了解Python多进程的基本原理。 了解PythonHTTP请求库requests的基本用法。 了解正则表达式的用法和Python中正则表达式库re的基本用法。 了解PythonHTML解析库pyquery的基本用法。 了解MongoDB并安装和启动MongoD">
<meta property="og:type" content="article">
<meta property="og:title" content="request、pyquest和pymongodb案例实战">
<meta property="og:url" content="http://example.com/2021/05/23/request%E3%80%81pyquest%E5%92%8Cpymongodb%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/index.html">
<meta property="og:site_name" content="Holy的个人站点">
<meta property="og:description" content="准备工作在本节课开始之前，我们需要做好如下的准备工作：  安装好Python3（最低为 3.6 版本），并能成功运行Python3程序。 了解Python多进程的基本原理。 了解PythonHTTP请求库requests的基本用法。 了解正则表达式的用法和Python中正则表达式库re的基本用法。 了解PythonHTML解析库pyquery的基本用法。 了解MongoDB并安装和启动MongoD">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/images/logo.webp">
<meta property="article:published_time" content="2021-05-23T13:12:36.000Z">
<meta property="article:modified_time" content="2021-05-24T05:37:30.000Z">
<meta property="article:author" content="Holy Chan">
<meta property="article:tag" content="request">
<meta property="article:tag" content="pyquest">
<meta property="article:tag" content="pymongodb">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/logo.webp">
<meta name="twitter:site" content="@null">
  
  <!-- 站点验证相关 -->
  
    
    
    
  
  <!-- 样式表文件 -->
  <link rel="stylesheet" id="kratos-css" href="/css/kratosr.min.css" type="text/css" media="all">
  
    <link rel="stylesheet" id="highlight-css" href="/css/highlight/night-eighties.min.css" type="text/css" media="all">
  
  
  <link rel="stylesheet" id="fontawe-css" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css" media="all">
  <link rel="stylesheet" id="nprogress-css" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" type="text/css" media="all">
  
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
  
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  
    <link rel="stylesheet" id="darkmode-css" href="/css/kr-dark.min.css" type="text/css" media="all">
  
  <!-- 不得不预先加载的一些JS文件 -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
  
    <script src="https://cdn.jsdelivr.net/npm/qrcode_js@1.0.0/qrcode.min.js"></script>
  
<meta name="generator" content="Hexo 5.4.0"></head>


    <body class="custom-background">
        <div id="kratos-wrapper">
    <div id="kratos-page">
        <div id="kratos-header">
            <header id="kratos-desktop-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="nav-header">
                        <nav id="kratos-menu-wrap">
                            <ul id="kratos-primary-menu" class="sf-menu">
                                
                                    
                                        <li><a href="/"><i class="fa fa-home"></i>首页</a></li>
                                    
                                
                                    
                                        <li><a href="/archives/"><i class="fa fa-file"></i>归档</a></li>
                                    
                                
                                    
                                        <li><a href="/about/"><i class="fa fa-paw"></i>关于我</a></li>
                                    
                                
                                    
                                        <li>
                                            <a><i class="fa fa-link"></i>链接</a>
                                            <ul class="sub-menu">
                                                
                                                    
                                                
                                                    
                                                        <li><a target="_blank" rel="noopener" href="http://holychan.ltd:8000">个人网盘</a></li>
                                                    
                                                
                                            </ul>
                                        </li>
                                    
                                
                            </ul>
                        </nav>
                    </div>
                </div>
            </header>
            <header id="kratos-mobile-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="color-logo"><a href="/">Holy的个人站点</a></div>
                    <div class="nav-toggle">
                        <a class="kratos-nav-toggle js-kratos-nav-toggle">
                            <i></i>
                        </a>
                    </div>
                </div>
            </header>
        </div>
        <div class="kratos-start kratos-hero-2">
            <!-- <div class="kratos-overlay"></div> -->
            <div class="kratos-cover kratos-cover-2 text-center">
                <div class="desc desc2 animate-box">
                    <a href="/">
                        <h2>Holy的个人站点</h2> <br />
                        <span></span>
                    </a>
                </div>
            </div>
        </div>

        <div id="kratos-blog-post">
            <div class="container">
                <div id="main" class="row">
                    

        <section class="col-md-8">
    <article>
        <div class="kratos-hentry kratos-post-inner clearfix">
            <header class="kratos-entry-header">
                
                    <h1 class="kratos-entry-title text-center">request、pyquest和pymongodb案例实战</h1>
                
                
                <ul class="kratos-post-meta text-center">
                    <li><i class="fa fa-calendar"></i> 2021-05-23</li>
                    <li><i class="fa fa-user"></i> 作者 Holy Chan</li>
                    <li>
                        <i class="fa fa-edit"></i> 
                        
                        
                            ~13.21K
                        
                        字
                    </li>
                    
                </ul>
            </header>
            <div class="kratos-post-content">
                <div id="expire-alert" class="alert alert-warning hidden" role="alert">
                    本文最后编辑于 <time datetime="1621834650000"></time> 前，其中的内容可能需要更新。
                </div>
                
                    <div class="kratos-post-inner-toc">
                        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%9B%AE%E6%A0%87"><span class="toc-number">2.</span> <span class="toc-text">爬虫目标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E5%88%97%E8%A1%A8%E9%A1%B5"><span class="toc-number">2.1.</span> <span class="toc-text">爬取列表页</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E8%AF%A6%E6%83%85%E9%A1%B5"><span class="toc-number">2.2.</span> <span class="toc-text">爬取详情页</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%88%B0MongoDB"><span class="toc-number">3.</span> <span class="toc-text">保存到MongoDB</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%8A%A0%E9%80%9F"><span class="toc-number">4.</span> <span class="toc-text">多进程加速</span></a></li></ol>
                    </div>
                
                <hr />
                <h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>在本节课开始之前，我们需要做好如下的准备工作：</p>
<ul>
<li>安装好<code>Python3</code>（最低为 3.6 版本），并能成功运行<code>Python3</code>程序。</li>
<li>了解<code>Python</code>多进程的基本原理。</li>
<li>了解<code>PythonHTTP</code>请求库<code>requests</code>的基本用法。</li>
<li>了解正则表达式的用法和<code>Python</code>中正则表达式库<code>re</code>的基本用法。</li>
<li>了解<code>PythonHTML</code>解析库<code>pyquery</code>的基本用法。</li>
<li>了解<code>MongoDB</code>并安装和启动<code>MongoDB</code>服务。</li>
<li>了解<code>Python</code>的<code>MongoDB</code>操作库<code>PyMongo</code>的基本用法。</li>
</ul>
<h2 id="爬虫目标"><a href="#爬虫目标" class="headerlink" title="爬虫目标"></a>爬虫目标</h2><p>一个基本的静态网站作为案例进行爬取，需要爬取的链接为：<a target="_blank" rel="noopener" href="https://static1.scrape.cuiqingcai.com/">https://static1.scrape.cuiqingcai.com/</a>，这个网站里面包含了一些电影信息。</p>
<p>要完成的目标是：</p>
<ul>
<li>用<code>requests</code>爬取这个站点每一页的电影列表，顺着列表再爬取每个电影的详情页。</li>
<li>用<code>pyquery</code>和正则表达式提取每部电影的名称、封面、类别、上映时间、评分、剧情简介等内容。</li>
<li>把以上爬取的内容存入<code>MongoDB</code>数据库。</li>
<li>使用多进程实现爬取的加速。</li>
</ul>
<h3 id="爬取列表页"><a href="#爬取列表页" class="headerlink" title="爬取列表页"></a>爬取列表页</h3><p>爬取的第一步肯定要从列表页入手，首先观察一下列表页的结构和翻页规则。在浏览器中访问<a target="_blank" rel="noopener" href="https://static1.scrape.cuiqingcai.com/"></a>，然后打开浏览器开发者工具，观察每一个电影信息区块对应的<code>HTML</code>，以及进入到详情页的<code>URL</code>是怎样的，如图所示：</p>
<p><img src="Screenshot_1.webp"></p>
<p>每部电影对应的区块都是一个<code>div</code>节点，它的<code>class</code>属性都有<code>el-card</code>这个值。每个列表页有10个这样的<code>div</code>节点，也就对应着10部电影的信息。</p>
<p>再分析下从列表页是怎么进入到详情页的，选中电影的名称，看下结果：</p>
<p><img src="Screenshot_2.webp"></p>
<p>这个名称实际上是一个<code>h2</code>节点，其内部的文字就是电影的标题。<code>h2</code>节点的外面包含了一个<code>a</code>节点，这个<code>a</code>节点带有<code>href</code>属性，这就是一个超链接，其中<code>href</code>的值为<code>/detail/1</code>，这是一个相对网站的根<code>URL</code><a target="_blank" rel="noopener" href="https://static1.scrape.cuiqingcai.com/">https://static1.scrape.cuiqingcai.com/</a>路径，加上网站的根<code>URL</code>就构成了<a target="_blank" rel="noopener" href="https://static1.scrape.cuiqingcai.com/detail/1">https://static1.scrape.cuiqingcai.com/detail/1</a>，也就是这部电影详情页的<code>URL</code>。这样只需要提取这个<code>href</code>属性就能构造出详情页的<code>URL</code>并接着爬取了。</p>
<p>接下来分析下翻页的逻辑，拉到页面的最下方，可以看到分页页码，如图所示：</p>
<p><img src="Screenshot_3.webp"></p>
<p>页面显示一共有<code>100</code>条数据，10页的内容，因此页码最多是10。接着我们点击第2页，如图所示：</p>
<p><img src="Screenshot_4.webp"></p>
<p>可以看到网页的<code>URL</code>变成了<code>https://static1.scrape.cuiqingcai.com/page/2</code>，相比根<code>URL</code>多了<code>/page/2</code>这部分内容。网页的结构还是和原来一模一样，所以我们可以和第1页一样处理。</p>
<p>接着查看第3页、第4页等内容，可以发现有这么一个规律，每一页的<code>URL</code>最后分别变成了<code>/page/3</code>、<code>/page/4</code>。所以，<code>/page</code>后面跟的就是列表页的页码，当然第1页也是一样，在根<code>URL</code>后面加上<code>/page/1</code>也是能访问的，只不过网站做了一下处理，默认的页码是1，所以显示第1页的内容。</p>
<p>分析到这里，逻辑基本就清晰了。</p>
<p>如果要完成列表页的爬取，可以这么实现：</p>
<ul>
<li>遍历页码构造<code>10</code>页的索引页<code>URL</code>。</li>
<li>从每个索引页分析提取出每个电影的详情页<code>URL</code>。</li>
</ul>
<p>先定义一些基础的变量，并引入一些必要的库，写法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO, <span class="built_in">format</span>=<span class="string">&#x27;%(asctime)s - %(levelname)s: %(message)s&#x27;</span>)</span><br><span class="line"></span><br><span class="line">BASE_URL = <span class="string">&#x27;https://static1.scrape.cuiqingcai.com&#x27;</span></span><br><span class="line">TOTAL_PAGE = <span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>引入<code>requests</code>用来爬取页面，<code>logging</code>用来输出信息，<code>re</code>用来实现正则表达式解析，<code>pyquery</code>用来直接解析网页，<code>pymongo</code>用来实现<code>MongoDB</code>存储，<code>urljoin</code>用来做<code>URL</code>的拼接。</p>
<p>接着定义日志输出级别和输出格式，完成之后再定义<code>BASE_URL</code>为当前站点的根<code>URL</code>，<code>TOTAL_PAGE</code>为需要爬取的总页码数量。</p>
<p>定义好了之后，来实现一个页面爬取的方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_page</span>(<span class="params">url</span>):</span></span><br><span class="line">    logging.info(<span class="string">&#x27;scraping %s...&#x27;</span>, url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url, verify=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        logging.error(<span class="string">&#x27;get invalid status code %s while scraping %s&#x27;</span>, response.status_code, url)</span><br><span class="line">    <span class="keyword">except</span> requests.RequestException:</span><br><span class="line">        logging.error(<span class="string">&#x27;error occurred while scraping %s&#x27;</span>, url, exc_info=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>考虑到不仅要爬取列表页，还要爬取详情页，在这里定义一个较通用的爬取页面的方法，叫作<code>scrape_page</code>，它接收一个<code>url</code>参数，返回页面的<code>html</code>代码。</p>
<p>首先判断状态码是不是<code>200</code>，如果是，则直接返回页面的<code>HTML</code>代码，如果不是，则会输出错误日志信息。另外，这里实现了<code>requests</code>的异常处理，如果出现了爬取异常，则会输出对应的错误日志信息。这时将<code>logging</code>的<code>error</code>方法的<code>exc_info</code>参数设置为<code>True</code>则可以打印出<code>Traceback</code>错误堆栈信息。 </p>
<p>有了<code>scrape_page</code>方法之后，给这个方法传入一个<code>url</code>，正常情况下它就可以返回页面的<code>HTML</code>代码。</p>
<p>在这个基础上，来定义列表页的爬取方法吧，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_index</span>(<span class="params">page</span>):</span></span><br><span class="line">    index_url = <span class="string">f&#x27;<span class="subst">&#123;BASE_URL&#125;</span>/page/<span class="subst">&#123;page&#125;</span>&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> scrape_page(index_url)</span><br></pre></td></tr></table></figure>

<p>方法名称叫作<code>scrape_index</code>，这个方法会接收一个<code>page</code>参数，即列表页的页码，在方法里面实现列表页的<code>URL</code>拼接，然后调用<code>scrape_page</code>方法爬取即可得到列表页的<code>HTML</code>代码了。</p>
<p>获取了<code>HTML</code>代码后，下一步就是解析列表页，并得到每部电影的详情页的<code>URL</code>了，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_index</span>(<span class="params">html</span>):</span></span><br><span class="line">    doc = pq(html)</span><br><span class="line">    links = doc(<span class="string">&#x27;.el-card .name&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> links.items():</span><br><span class="line">        href = link.attr(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">        detail_url = urljoin(BASE_URL, href)</span><br><span class="line">        logging.info(<span class="string">&#x27;get detail info %s&#x27;</span>, detail_url)</span><br><span class="line">        <span class="keyword">yield</span> detail_url</span><br></pre></td></tr></table></figure>

<p>这里我们定义了<code>parse_index</code>方法，它接收一个<code>html</code>参数，即列表页的<code>HTML</code>代码。接着用<code>pyquery</code>新建一个<code>PyQuery</code>对象，完成之后再用<code>.el-card .name</code>选择器选出来每个电影名称对应的超链接节点。遍历这些节点，通过调用<code>attr</code>方法并传入<code>href</code>获得详情页的<code>URL</code>路径，得到的<code>href</code>就是上文所说的类似<code>/detail/1</code>这样的结果。这并不是一个完整的<code>URL</code>，所以需要借助<code>urljoin</code>方法把<code>BASE_URL</code>和<code>href</code>拼接起来，获得详情页的完整<code>URL</code>，得到的结果就是类似<code>https://static1.scrape.cuiqingcai.com/detail/1</code>这样完整的<code>URL</code>了，最后<code>yield</code>返回即可。</p>
<p>通过调用<code>parse_index</code>方法传入列表页的<code>HTML</code>代码就可以获得该列表页所有电影的详情页<code>URL</code>了，接下来把上面的方法串联调用一下，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, TOTAL_PAGE+<span class="number">1</span>):</span><br><span class="line">        index_html = scrape_index(page)</span><br><span class="line">        detail_urls = parse_index((index_html))</span><br><span class="line">        logging.info(<span class="string">&#x27;detail urls %s&#x27;</span>, <span class="built_in">list</span>(detail_urls))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>定义了<code>main</code>方法来完成上面所有方法的调用，首先使用<code>range</code>方法遍历一下页码，得到的<code>page</code>是<code>1~10</code>，接着把<code>page</code>变量传给<code>scrape_index</code>方法，得到列表页的<code>HTML</code>，赋值为<code>index_html</code>变量。接下来再将<code>index_html</code>变量传给<code>parse_index</code>方法，得到列表页所有电影的详情页<code>URL</code>，赋值为<code>detail_urls</code>，结果是一个生成器，调用<code>list</code>方法就可以将其输出出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,059 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">1</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,060 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">2</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,061 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">3</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,062 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">4</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,063 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">5</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,064 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">6</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,065 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">7</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,067 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">8</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,070 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">9</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,072 - INFO: get detail info https://static1.scrape.cuiqingcai.com/detail/<span class="number">10</span></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,074 - INFO: detail urls [<span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/1&#x27;</span>, <span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/2&#x27;</span>, <span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/3&#x27;</span>, <span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/4&#x27;</span>, <span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/5&#x27;</span>, <span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/6&#x27;</span>, <span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/7&#x27;</span>, <span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/8&#x27;</span>, <span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/9&#x27;</span>, <span class="string">&#x27;https://static1.scrape.cuiqingcai.com/detail/10&#x27;</span>]</span><br><span class="line"><span class="number">2021</span>-05-<span class="number">23</span> <span class="number">23</span>:<span class="number">23</span>:04,076 - INFO: scraping https://static1.scrape.cuiqingcai.com/page/<span class="number">2.</span>..</span><br><span class="line">....</span><br></pre></td></tr></table></figure>

<p>由于输出内容比较多，这里只贴了一部分。可以看到，在这个过程中程序首先爬取了第<code>1</code>页列表页，然后得到了对应详情页的每个<code>URL</code>，接着再接着爬第2页、第3页，一直到第10页，依次输出了每一页的详情页<code>URL</code>。这样，就成功获取到所有电影详情页<code>URL</code>。</p>
<h3 id="爬取详情页"><a href="#爬取详情页" class="headerlink" title="爬取详情页"></a>爬取详情页</h3><p>首先观察一下详情页的<code>HTML</code>代码，如图所示：</p>
<p><img src="Screenshot_5.webp"></p>
<p>经过分析，要提取的内容和对应的节点信息如下：</p>
<ul>
<li>封面：是一个<code>img</code>节点，其<code>class</code>属性为<code>cover</code>。</li>
<li>名称：是一个<code>h2</code>节点，其内容便是名称。</li>
<li>类别：是<code>span</code>节点，其内容便是类别</li>
<li>内容，其外侧是<code>button</code>节点，再外侧则是<code>class</code>为<code>categories</code>的<code>div</code>节点。</li>
<li>上映时间：是<code>span</code>节点，其内容包含了上映时间，其外侧是包含了<code>class</code>为<code>info</code>的<code>div</code>节点。但注意这个<code>div</code>前面还有一个<code>class</code>为<code>info</code>的<code>div</code>节点，可以使用其内容来区分，也可以使用<code>nth-child</code>或<code>nth- of-type</code>这样的选择器来区分。另外提取结果中还多了「上映」二字，可以用正则表达式把日期提取出来。</li>
<li>评分：是一个<code>p</code>节点，其内容便是评分，<code>p</code>节点的<code>class</code>属性为<code>score</code>。</li>
<li>剧情简介：是一个<code>p</code>节点，其内容便是</li>
<li>剧情简介，其外侧是<code>class</code>为<code>drama</code>的<code>div</code>节点。</li>
</ul>
<p>刚才已经成功获取了详情页的<code>URL</code>，接下来要定义一个详情页的爬取方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrape_detail</span>(<span class="params">url</span>):</span> </span><br><span class="line">    <span class="keyword">return</span> scrape_page(url)</span><br></pre></td></tr></table></figure>

<p>定义了一个<code>scrape_detail</code>方法，它接收一个<code>url</code>参数，并通过调用<code>scrape_page</code>方法获得网页源代码。由于刚才已经实现了<code>scrape_page</code>方法，所以在这里不用再写一遍页面爬取的逻辑，直接调用即可，这就做到了代码复用。</p>
<p>单独定义一个<code>scrape_detail</code>方法在逻辑上会显得更清晰，而且以后如果想要对<code>scrape_detail</code>方法进行改动，比如添加日志输出或是增加预处理，都可以在 <code>scrape_detail</code>里面实现，而不用改动<code>scrape_page</code>方法，灵活性会更好。</p>
<p>详情页的爬取方法已经实现了，接着就是详情页的解析了，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span>(<span class="params">html</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;详情页的解析&quot;&quot;&quot;</span></span><br><span class="line">    doc = pq(html)</span><br><span class="line">    cover = doc(<span class="string">&#x27;img.cover&#x27;</span>).attr(<span class="string">&#x27;src&#x27;</span>)</span><br><span class="line">    name = doc(<span class="string">&#x27;a &gt; h2&#x27;</span>).text()</span><br><span class="line">    categories = [item.text() <span class="keyword">for</span> item <span class="keyword">in</span> doc(<span class="string">&#x27;.categories button span&#x27;</span>).items()]</span><br><span class="line">    published_at = doc(<span class="string">&#x27;.info:contains(上映)&#x27;</span>).text()</span><br><span class="line">    published_at = re.search(<span class="string">&#x27;(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)&#x27;</span>, published_at).group(<span class="number">1</span>) \</span><br><span class="line">        <span class="keyword">if</span> published_at <span class="keyword">and</span> re.search(<span class="string">&#x27;\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;&#x27;</span>, published_at) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    drama = doc(<span class="string">&#x27;.drama p&#x27;</span>).text()</span><br><span class="line">    score = doc(<span class="string">&#x27;p.score&#x27;</span>).text()</span><br><span class="line">    score = <span class="built_in">float</span>(score) <span class="keyword">if</span> score <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span>&#123;</span><br><span class="line">        <span class="string">&#x27;cover&#x27;</span>: cover,</span><br><span class="line">        <span class="string">&#x27;name&#x27;</span>: name,</span><br><span class="line">        <span class="string">&#x27;categories&#x27;</span>: categories,</span><br><span class="line">        <span class="string">&#x27;published_at&#x27;</span>: published_at,</span><br><span class="line">        <span class="string">&#x27;drama&#x27;</span>: drama,</span><br><span class="line">        <span class="string">&#x27;score&#x27;</span>: score</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>定义了<code>parse_detail</code>方法用于解析详情页，它接收一个<code>html</code>参数，解析其中的内容，并以字典的形式返回结果。每个字段的解析情况如下所述：</p>
<ul>
<li><code>cove</code>r：封面，直接选取<code>class</code>为<code>cover</code>的<code>img</code>节点，并调用<code>attr</code>方法获取<code>src</code>属性的内容即可。</li>
<li><code>name</code>：名称，直接选取<code>a</code>节点的直接子节点<code>h2</code>节点，并调用<code>text</code>方法提取其文本内容即可得到名称。</li>
<li><code>categories</code>：类别，由于类别是多个，所以这里首先用<code>.categories button span</code>选取了<code>class</code>为<code>categories</code>的节点内部的<code>span</code>节点，其结果是多个，所以这里进行了遍历，取出了每个<code>span</code>节点的文本内容，得到的便是列表形式的类别。</li>
<li><code>published_at</code>：上映时间，由于<code>pyquery</code>支持使用<code>:contains</code>直接指定包含的文本内容并进行提取，且每个上映时间信息都包含了「上映」二字，所以这里就直接使用<code>:contains(上映)</code>提取了<code>class</code>为<code>info</code>的<code>div</code>节点。提取之后，得到的结果类似「1993-07-26 上映」这样，并不想要「上映」这两个字，所以又调用了正则表达式把日期单独提取出来了。当然这里也可以直接使用<code>strip</code>或<code>replace</code>方法把多余的文字去掉。</li>
<li><code>drama</code>：直接提取<code>class</code>为<code>drama</code>的节点内部的<code>p</code>节点的文本即可。</li>
<li><code>score</code>：直接提取<code>class</code>为<code>score</code>的<code>p</code>节点的文本即可，由于提取结果是字符串，所以我们需要把它转成浮点数，即``float`类型。</li>
</ul>
<p>上述字段提取完毕之后，构造一个字典返回。这样，成功完成了详情页的提取和分析了。</p>
<p>将main方法稍微改写一下，增加这两个方法的调用，改写如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, TOTAL_PAGE+<span class="number">1</span>):</span><br><span class="line">        index_html = scrape_index(page)</span><br><span class="line">        detail_urls = parse_index((index_html))</span><br><span class="line">        <span class="keyword">for</span> detail_url <span class="keyword">in</span> detail_urls:</span><br><span class="line">            detail_html = scrape_detail(detail_url)</span><br><span class="line">            data = parse_detail(detail_html)</span><br><span class="line">            logging.info(<span class="string">&#x27;get detail data %s&#x27;</span>, data)</span><br></pre></td></tr></table></figure>

<p>首先遍历了<code>detail_urls</code>，获取了每个详情页的<code>URL</code>，然后依次调用了<code>scrape_detail</code>和<code>parse_detail</code>方法，最后得到了每个详情页的提取结果，赋值为<code>data</code>并输出。</p>
<p>运行结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2021</span>-05-<span class="number">24</span> <span class="number">12</span>:<span class="number">46</span>:<span class="number">31</span>,<span class="number">432</span> - INFO: get detail data &#123;<span class="string">&#x27;cover&#x27;</span>: <span class="string">&#x27;https://p0.meituan.net/movie/b0d986a8bf89278afbb19f6abaef70f31206570.jpg@464w_644h_1e_1c&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&quot;辛德勒的名单 - Schindler&#x27;s List&quot;</span>, <span class="string">&#x27;categories&#x27;</span>: [<span class="string">&#x27;剧情&#x27;</span>, <span class="string">&#x27;历史&#x27;</span>, <span class="string">&#x27;战争&#x27;</span>], <span class="string">&#x27;published_at&#x27;</span>: <span class="string">&#x27;1993-11-30&#x27;</span>, <span class="string">&#x27;drama&#x27;</span>: <span class="string">&#x27;1939年，波兰在纳粹德国的统治下，党卫军对犹太人进行了隔离统治。德国商人奥斯卡·辛德勒（连姆·尼森 饰）来到德军统治下的克拉科夫，开设了一间搪瓷厂，生产军需用品。凭着出众的社交能力和大量的金钱，辛德勒和德军建立了良好 的关系，他的工厂雇用犹太人工作，大发战争财。1943年，克拉科夫的犹太人遭到了惨绝人寰的大屠杀，辛德勒目睹这一切，受到了极大的震撼，他贿赂军官，让自己的工厂成为集中营的附属劳役营，在那些疯狂屠杀的日子里，他的工厂也成为了犹太人的避难所。1944年，德国战败前夕，屠杀犹太人的行动越发疯狂，辛德勒向德军军官开出了1200人的名单，倾家荡产买下了这些犹太人的生命。在那些暗无天日的岁月里，拯救一个人，就是拯救全世界。&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">9.5</span>&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，已经成功提取出每部电影的基本信息，包括封面、名称、类别，等等。</p>
<h2 id="保存到MongoDB"><a href="#保存到MongoDB" class="headerlink" title="保存到MongoDB"></a>保存到MongoDB</h2><p>请确保现在有一个可以正常连接和使用的<code>MongoDB</code>数据库。 将数据导入<code>MongoDB</code>需要用到<code>PyMongo</code>这个库，这个在最开始已经引入过了。那么接下来我们定义一下 <code>MongoDB</code>的连接配置，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MONGO_CONNECTION_STRING = <span class="string">&#x27;mongodb://localhost:27017&#x27;</span></span><br><span class="line">MONGO_DB_NAME = <span class="string">&#x27;movies&#x27;</span></span><br><span class="line">MONGO_COLLECTIONS_NAME = <span class="string">&#x27;movies&#x27;</span></span><br><span class="line"></span><br><span class="line">client = pymongo.MongoClient(MONGO_CONNECTION_STRING)</span><br><span class="line">db = client[<span class="string">&#x27;movies&#x27;</span>]</span><br><span class="line">collection = db[<span class="string">&#x27;movies&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>在这里声明了几个变量，介绍如下：</p>
<ul>
<li><code>MONGO_CONNECTION_STRING</code>：<code>MongoDB</code>的连接字符串，里面定义了<code>MongoDB</code>的基本连接信息，如<code>host</code>、<code>port</code>，还可以定义用户名密码等内容。</li>
<li><code>MONGO_DB_NAME</code>：<code>MongoDB</code>数据库的名称。</li>
<li><code>MONGO_COLLECTION_NAME</code>：<code>MongoDB</code>的集合名称。</li>
</ul>
<p>这里用<code>MongoClient</code>声明了一个连接对象，然后依次声明了存储的数据库和集合。接下来，再实现一个将数据保存到<code>MongoDB</code>的方法，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_data</span>(<span class="params">data</span>):</span></span><br><span class="line">    collection.update_one(&#123;</span><br><span class="line">        <span class="string">&#x27;name&#x27;</span>: data.get(<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        <span class="string">&#x27;$set&#x27;</span>: data</span><br><span class="line">    &#125;, upsert=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>声明了一个<code>save_data</code>方法，它接收一个<code>data</code>参数，也就是我们刚才提取的电影详情信息。在方法里面，调用了<code>update_one</code>方法，第1个参数是<strong>查询条件</strong>，即根据<code>name</code>进行查询；第2个参数是<code>data</code>对象本身，也就是所有的数据，这里用<code>$set</code>操作符表示更新操作；第3个参数很关键，这里实际上是<code>upsert</code>参数，如果把这个设置为 <code>True</code>，则可以做到存在即更新，不存在即插入的功能，更新会根据第一个参数设置的<code>name</code>字段，所以这样可以防止数据库中出现同名的电影数据。</p>
<blockquote>
<p>注：实际上电影可能有同名，但该场景下的爬取数据没有同名情况，当然这里更重要的是实现<code>MongoDB</code>的去重操作。</p>
</blockquote>
<p>接下来将<code>main</code>方法稍微改写一下就好了，改写如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, TOTAL_PAGE+<span class="number">1</span>):</span><br><span class="line">        index_html = scrape_index(page)</span><br><span class="line">        detail_urls = parse_index((index_html))</span><br><span class="line">        <span class="keyword">for</span> detail_url <span class="keyword">in</span> detail_urls:</span><br><span class="line">            detail_html = scrape_detail(detail_url)</span><br><span class="line">            data = parse_detail(detail_html)</span><br><span class="line">            logging.info(<span class="string">&#x27;get detail data %s\n&#x27;</span>, data)</span><br><span class="line">            logging.info(<span class="string">&#x27;saving data to mongodb&#x27;</span>)</span><br><span class="line">            save_data(data)</span><br><span class="line">            logging.info(<span class="string">&#x27;data saved successfully\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>重新运行，输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line"><span class="number">2021</span>-05-<span class="number">24</span> <span class="number">13</span>:<span class="number">16</span>:<span class="number">46</span>,<span class="number">882</span> - INFO: get detail data &#123;<span class="string">&#x27;cover&#x27;</span>: <span class="string">&#x27;https://p0.meituan.net/movie/58782fa5439c25d764713f711ebecd1e201941.jpg@464w_644h_1e_1c&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;魂断蓝桥 - Waterloo Bridge&#x27;</span>, <span class="string">&#x27;categories&#x27;</span>: [<span class="string">&#x27;剧情&#x27;</span>, <span class="string">&#x27;爱情&#x27;</span>, <span class="string">&#x27;战争&#x27;</span>], <span class="string">&#x27;published_at&#x27;</span>: <span class="string">&#x27;1940-05-17&#x27;</span>, <span class="string">&#x27;drama&#x27;</span>: <span class="string">&#x27;第一次世界大战期间，回国度假的陆军中 </span></span><br><span class="line"><span class="string">尉罗伊（罗伯特·泰勒 饰）在滑铁卢桥上邂逅了舞蹈演员玛拉（费雯·丽 饰），两人彼此倾心，爱情迅速升温。就在两人决定结婚之时，罗伊应招回营地，两人被迫分离。由</span></span><br><span class="line"><span class="string">于错过剧团演出，玛拉被开除，只能和好友相依为命。不久玛拉得知罗伊阵亡的消息，几欲崩溃，备受打击。失去爱情的玛拉感到一切都失去了意义，为了生存，她和好友不</span></span><br><span class="line"><span class="string">得不沦为妓女。然而命运弄人，就在此时玛拉竟然再次遇到了罗伊。虽然为罗伊的生还兴奋不已，玛拉却因自己的失身陷入痛苦之中。感到一切难以挽回的玛拉潸然离开，独</span></span><br><span class="line"><span class="string">自来到两人最初相遇的地点——滑铁卢桥上…&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">9.5</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="number">2021</span>-05-<span class="number">24</span> <span class="number">13</span>:<span class="number">16</span>:<span class="number">46</span>,<span class="number">885</span> - INFO: saving data to mongodb</span><br><span class="line"><span class="number">2021</span>-05-<span class="number">24</span> <span class="number">13</span>:<span class="number">16</span>:<span class="number">46</span>,<span class="number">889</span> - INFO: data saved successfully</span><br></pre></td></tr></table></figure>

<p>运行完毕之后我们可以使用<code>MongoDB</code>客户端工具可视化查看已经爬取到的数据，结果如下：</p>
<p><img src="Screenshot_6.webp"></p>
<h2 id="多进程加速"><a href="#多进程加速" class="headerlink" title="多进程加速"></a>多进程加速</h2><p>由于整个的爬取是单进程的，而且只能逐条爬取，速度稍微有点慢，有没有方法来对整个爬取过程进行加速呢？</p>
<p>在前面学习了多进程的基本原理和使用方法，下面就来实践一下多进程的爬取。</p>
<p>由于一共有<code>10</code>页详情页，并且这<code>10</code>页内容是互不干扰的，所以可以一页开一个进程来爬取。由于这<code>10</code>个列表页页码正好可以提前构造成一个列表，所以可以选用多进程里面的进程池<code>Pool</code>来实现这个过程。</p>
<p>这里需要改写下<code>main</code>方法的调用，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">page</span>):</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, TOTAL_PAGE+<span class="number">1</span>):</span><br><span class="line">        index_html = scrape_index(page)</span><br><span class="line">        detail_urls = parse_index((index_html))</span><br><span class="line">        <span class="keyword">for</span> detail_url <span class="keyword">in</span> detail_urls:</span><br><span class="line">            detail_html = scrape_detail(detail_url)</span><br><span class="line">            data = parse_detail(detail_html)</span><br><span class="line">            logging.info(<span class="string">&#x27;get detail data %s\n&#x27;</span>, data)</span><br><span class="line">            logging.info(<span class="string">&#x27;saving data to mongodb&#x27;</span>)</span><br><span class="line">            save_data(data)</span><br><span class="line">            logging.info(<span class="string">&#x27;data saved successfully\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    pool = multiprocessing.Pool()</span><br><span class="line">    pages = <span class="built_in">range</span>(<span class="number">1</span>, TOTAL_PAGE + <span class="number">1</span>)</span><br><span class="line">    pool.<span class="built_in">map</span>(main, pages)</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure>

<p>这里首先给<code>main</code>方法添加一个参数<code>page</code>，用以表示列表页的页码。接着声明了一个进程池，并声明<code>pages</code>为所有需要遍历的页码，即<code>1~10</code>。最后调用<code>map</code>方法，第1个参数就是需要被调用的方法，第2个参数就是<code>pages</code>，即需要遍历的页码。</p>
<p>这样<code>pages</code>就会被依次遍历。把<code>1~10</code>这10个页码分别传递给<code>main</code>方法，并把每次的调用变成一个进程，加入到进程池中执行，进程池会根据当前运行环境来决定运行多少进程。</p>
<p>运行输出结果和之前类似，但是可以明显看到加了多进程执行之后，爬取速度快了非常多。可以清空一下之前的<code>MongoDB</code>数据，可以发现数据依然可以被正常保存到<code>MongoDB</code>数据库中。</p>

            </div>
            
                <div class="kratos-copyright text-center clearfix">
                    <h5>本作品采用 <a rel="license nofollow" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 国际许可协议</a> 进行许可</h5>
                </div>
            
            <footer class="kratos-entry-footer clearfix">
                
                    <div class="post-like-donate text-center clearfix" id="post-like-donate">
                    
                    
                        <a class="share" href="javascript:;"><i class="fa fa-share-alt"></i> 分享</a>
                        <div class="share-wrap" style="display: none;">
    <div class="share-group">
        <a href="javascript:;" class="share-plain qq" onclick="share('qq');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-qq"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain qzone" onclick="share('qzone');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-star"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weixin pop style-plain" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weixin"></i>
            </div>
            <div class="share-int">
                <div class="qrcode" id="wechat-qr"></div>
                <p>打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮</p>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weibo" onclick="share('weibo');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weibo"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain facebook style-plain" onclick="share('facebook');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-facebook"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain twitter style-plain" onclick="share('twitter');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-twitter"></i>
            </div>
        </a>
    </div>
    <script type="text/javascript">
        $(()=>{
            new QRCode("wechat-qr", {
                text: "http://example.com/2021/05/23/request%E3%80%81pyquest%E5%92%8Cpymongodb%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/",
                width: 150,
                height: 150,
                correctLevel : QRCode.CorrectLevel.H
            });
        });
        function share(dest) {
            const qqBase        = "https://connect.qq.com/widget/shareqq/index.html?";
            const weiboBase     = "https://service.weibo.com/share/share.php?";
            const qzoneBase     = "https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?";
            const facebookBase  = "https://www.facebook.com/sharer/sharer.php?";
            const twitterBase   = "https://twitter.com/intent/tweet?";
            const hostUrl       = "http://example.com/2021/05/23/request%E3%80%81pyquest%E5%92%8Cpymongodb%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98/";
            const title         = "「request、pyquest和pymongodb案例实战」";
            const excerpt       = `准备工作在本节课开始之前，我们需要做好如下的准备工作：

安装好Python3（最低为 3.6 版本），并能成功运行Python3程序。
了解Python多进程的基本原理。
了解PythonHTTP请求库requests的基本用法。
...`;
            let _URL;
            switch (dest) {
                case "qq"       : _URL = qqBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";     break;
                case "weibo"    : _URL = weiboBase+"url="+hostUrl+"&title="+title+excerpt;                                 break;
                case "qzone"    : _URL = qzoneBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";  break;
                case "facebook" : _URL = facebookBase+"u="+hostUrl;                                                        break;
                case "twitter"  : _URL = twitterBase+"text="+title+excerpt+"&url="+hostUrl;                                break;
            }
            window.open(_URL);
        };
    </script>
</div>
                    
                    </div>
                
                <div class="footer-tag clearfix">
                    <div class="pull-left">
                    <i class="fa fa-tags"></i>
                        <a class="tag-none-link" href="/tags/pymongodb/" rel="tag">pymongodb</a>, <a class="tag-none-link" href="/tags/pyquest/" rel="tag">pyquest</a>, <a class="tag-none-link" href="/tags/request/" rel="tag">request</a>
                    </div>
                    <div class="pull-date">
                    <span>最后编辑：2021-05-24</span>
                    </div>
                </div>
            </footer>
        </div>
        
            <nav class="navigation post-navigation clearfix" role="navigation">
                
                <div class="nav-previous clearfix">
                    <a title=" MongoDB数据库的使用" href="/2021/05/23/MongoDB数据库的使用/">&lt; 上一篇</a>
                </div>
                
                
                <div class="nav-next clearfix">
                    <a title=" Ajax的原理和解析" href="/2021/05/24/Ajax的原理和解析/">下一篇 &gt;</a>
                </div>
                
            </nav>
        
        
    </article>
</section>

                
            

<section id="kratos-widget-area" class="col-md-4 hidden-xs hidden-sm">
    <!-- 文章和页面根据splitter来分割，没有的话就从头开始设置为sticky -->
    
    
                <aside id="krw-about" class="widget widget-kratos-about clearfix">
    <div class="photo-background"></div>
    <div class="photo-wrapper clearfix">
        <div class="photo-wrapper-tip text-center">
            <img class="about-photo" src="/images/my.webp" />
        </div>
    </div>
    <div class="textwidget">
        <p class="text-center"></p>
    </div>
</aside>
            
                    <div class="sticky-area">
                
                    <aside id="krw-toc" class="widget widget-kratos-toc clearfix">
    <div class="photo-background"></div>
    <h4 class="widget-title no-after">
        <i class="fa fa-compass"></i>
        文章目录
        <span class="toc-progress-bar"></span>
    </h4>
    <div class="textwidget">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%9B%AE%E6%A0%87"><span class="toc-text">爬虫目标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E5%88%97%E8%A1%A8%E9%A1%B5"><span class="toc-text">爬取列表页</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E8%AF%A6%E6%83%85%E9%A1%B5"><span class="toc-text">爬取详情页</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%88%B0MongoDB"><span class="toc-text">保存到MongoDB</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%8A%A0%E9%80%9F"><span class="toc-text">多进程加速</span></a></li></ol>
    </div>
</aside>
                
                
  <aside id="krw-categories" class="widget widget-kratos-categories clearfix">
    <h4 class="widget-title"><i class="fa fa-folder"></i>分类目录</h4>
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Go/">Go</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Nginx/">Nginx</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/">博客教程</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BA%84%E5%AD%90/">庄子</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">14</span></li></ul>
  </aside>


            
                
  <aside id="krw-tags" class="widget widget-kratos-tags clearfix">
    <h4 class="widget-title"><i class="fa fa-tags"></i>标签聚合</h4>
      <div class="tag-clouds">
        <a href="/tags/Git/" style="font-size: 0.6em;">Git</a> <a href="/tags/Hexo/" style="font-size: 0.73em;">Hexo</a> <a href="/tags/Nginx/" style="font-size: 0.6em;">Nginx</a> <a href="/tags/ajax/" style="font-size: 0.67em;">ajax</a> <a href="/tags/css/" style="font-size: 0.6em;">css</a> <a href="/tags/docker/" style="font-size: 0.73em;">docker</a> <a href="/tags/git/" style="font-size: 0.6em;">git</a> <a href="/tags/go/" style="font-size: 0.6em;">go</a> <a href="/tags/html/" style="font-size: 0.6em;">html</a> <a href="/tags/http/" style="font-size: 0.8em;">http</a> <a href="/tags/https/" style="font-size: 0.6em;">https</a> <a href="/tags/javascript/" style="font-size: 0.67em;">javascript</a> <a href="/tags/linux/" style="font-size: 0.67em;">linux</a> <a href="/tags/mongodb/" style="font-size: 0.6em;">mongodb</a> <a href="/tags/multiprocessing/" style="font-size: 0.6em;">multiprocessing</a> <a href="/tags/nextCloud/" style="font-size: 0.6em;">nextCloud</a> <a href="/tags/nginx/" style="font-size: 0.6em;">nginx</a> <a href="/tags/pymongodb/" style="font-size: 0.6em;">pymongodb</a>
      </div>
  </aside>

            
                
  <aside id="krw-posts" class="widget widget-kratos-posts">
  <h4 class="widget-title"><i class="fa fa-file"></i>最新文章</h4>
  <div class="tab-content">
      <ul class="list-group">
        
        
          
          
            <a class="list-group-item" href="/2021/05/29/scrapy%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"><i class="fa  fa-book"></i> scrapy的基本使用</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/05/29/scrapy%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/"><i class="fa  fa-book"></i> scrapy框架介绍</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/05/29/%E5%86%85%E7%AF%87%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E9%BD%90%E7%89%A9%E8%AE%BA/"><i class="fa  fa-book"></i> 内篇（二）——齐物论</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/05/29/%E5%86%85%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E9%80%8D%E9%81%A5%E6%B8%B8/"><i class="fa  fa-book"></i> 内篇（一）——逍遥游</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/05/28/python%E7%9A%84%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B/"><i class="fa  fa-book"></i> python的内置类型</a>
            
          
        
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
      </ul>
  </div>
  </aside>

            
    </div>
</section>
        
        </div>
    </div>
</div>
<footer>
    <div id="footer"  class="ap-lrc"  >
        <div class="kr-tool text-center">
            <div class="tool">
                
                    <div class="box search-box">
                        <a href="/search/">
                            <span class="fa fa-search"></span>
                        </a>
                    </div>
                
                
                    <div class="box theme-box" id="darkmode-switch">
                        <span class="fa fa-adjust"></span>
                    </div>
                
                
            </div>
            <div class="box gotop-box">
                <span class="fa fa-chevron-up"></span>
            </div>
        </div>
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-3 footer-list text-center">
                    <ul class="kratos-social-icons">
                        
                        <li><a href="mailto:espholychan@outlook.com"><i class="fa fa-envelope"></i></a></li>
                        
                        
                        
                        
                        
                        
                        
                    </ul>
                    <ul class="kratos-copyright">
                        <div>
                            <li>&copy; 2021 Holy的个人站点 版权所有.</li>
                            <li>本站已运行<span id="span_dt">Loading...</span></li>
                        </div>
                        <div>
                            <li>Theme <a href="https://github.com/Candinya/Kratos-Rebirth" target="_blank">Kratos:Rebirth</a></li>
                            <li>Site built with&nbsp;<i class="fa fa-heart throb" style="color:#d43f57"></i>&nbsp;by Holy Chan.</li>
                        </div>
                        <div>
                            <li>Powered by <a href="https://hexo.io" target="_blank" rel="nofollow">Hexo</a></li>
                            
                        </div>
                        <div>
                            <li><a href="https://beian.miit.gov.cn" rel="external nofollow" target="_blank">鄂ICP备2021008617号-1</a></li>
                            
                        </div>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</footer>
</div>
</div>

        <script defer src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.4/dist/js/bootstrap.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.js"></script>
<script>const notMobile = (!(navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)));</script>

    <div>
        <canvas id="snow"></canvas>
        <script async type="text/javascript" src="/js/snow.min.js"></script>
    </div>

<script async src="/js/candy.min.js"></script>

    <script defer src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>
    <meting-js
        server="netease"
        type="playlist"
        id="3204190542"
        order="random"
        fixed="true"
    >
    </meting-js>



    <script defer src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script>
<script defer src="/js/kratosr.min.js"></script>
<script defer src="/js/pjax.min.js"></script>


    <script defer src="/js/kr-dark.min.js"></script>



<!-- Extra support for third-party plguins  -->


    </body>
</html>