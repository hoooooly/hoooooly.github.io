---
title: 数据存入数据库
tags:
  - SQL
comments: true
typora-root-url: 数据存入数据库
date: 2021-08-17 14:15:24
categories: 爬虫
index_img:
banner_img:
---

# SQLite

SQLite是一个文件型轻量级数据库，它的处理速度很快，在数据量不是很大的情况下，使用SQLite足够了。

首先，创建一个供Scrapy使用的SQLite数据库，取名为`scrapy.db`：

```python
[root@VM-8-16-centos SQL]# sqlite3 scrapy.db
```

接下来，在客户端中创建数据表（Table）：

```python
CREATE TABLE books(
	upc CHAR(16) NOT NULL PRIMARY KEY,
    name VARCHAR(256) NOT NULL,
    price VARCHAR(16) NOT NULL,
    review_rating INT,
    review_num INT,
    stock INT
);
```

在Python中访问SQLite数据库可使用Python标准库中的`sqlite3`模块。下面是使用`sqlite3`模块将数据写入SQLite数据库的简单示例：

```python
import sqlite3

# 连接数据库
conn = sqlite3.connect('scrapy.db')

# 创建corosr对象，用来执行SQL语句
cur = conn.cursor()

# 创建数据表
cur.execute('CREATE TABLE person(name VARCHAR(32), age INT, sex char(1))')

# 插入一条数据
cur.execute('INSERT INTO person VALUES  (?, ?, ?)', ('陈浩', 25, 'M'))

# 保存变更，提交保存
conn.commit()

# 关闭连接
conn.close()
```

了解了在Python中如何操作SQLite数据库后，接下来编写一个能将爬取到的数据写入SQLite数据库的`Item Pipeline`。在`pipelines.py`中实现`SQLitePipeline`的代码如下：

```python
import sqlite3


class SQLitePipeline(object):
    def open_spider(self, spider):
        db_name = spider.settings.get('SQLITE_DB_NAME', 'scrapy_defaut.db')
        self.db_conn = sqlite3.connect(db_name)
        self.db_cur = self.db_conn.cursor()

    def close(self, spider):
        self.db_conn.commit()
        self.db_conn.close()

    def process_item(self, item, spider):
        self.insert_db(item)
        return item

    def insert_db(self, item):
        values = (
            item['upc'],
            item['name'],
            item['price'],
            item['review_rating'],
            item['review_num'],
            item['stock'],
        )

        sql = 'INSERT INTO books VALUES (?, ?, ?, ?, ?, ?)'

        self.db_cur.execute(sql, values)

        # 每插入一条就commit一次会影响效率
        self.db_conn.commit()
```

上述代码：

- `open_spider`方法在开始爬取数据之前被调用，在该方法中通过`spider.settings`对象读取用户在配置文件中指定的数据库，然后建立与数据库的连接，将得到的Connection对象和Cursor对象分别赋值给`self.db_conn`和`self.db_cur`，以便之后使用。
- `process_item`方法处理爬取到的每一项数据，在该方法中调用insert_db方法，执行插入数据操作的SQL语句。但需要注意的是，在insert_db中并没有调用连接对象的commit方法，也就意味着此时数据并没有实际写入数据库。如果每插入一条数据都调用一次commit方法，会严重降低程序执行效率，并且我们对数据插入数据库的实时性并没有什么要求，因此可以在爬取完全部数据后再调用commit方法。
- `close_spider`方法在爬取完全部数据后被调用，在该方法中，调用连接对象的commit方法将之前所有的插入数据操作一次性提交给数据库，然后关闭连接对象。

在配置文件`settings.py`中指定我们所要使用的SQLite数据库，并启用`SQLitePipeline`：

```python
SQLITE_DB_NAME = 'scrapy.db'
ITEM_PIPELINES = {
    'example.pipelines.SQLitePipeline': 400,
}
```

运行爬虫。







[//]:#(设置表格整体居中显示)
<style>
    table
    {
        margin: auto;
        font-size: 80%;
    }
</style>


