---
title: 优美图库抓取
tags:
  - python
  - bs4
  - requests
comments: true
toc: true
only:
  - home
  - category
  - tag
date: 2021-06-06 13:56:08
categories: 爬虫
pic:
---





## 抓取思路

1.拿到主页面的源代码，然后提取子页面的链接接地址，`href`
2.通过`href`拿到子页面的内容，从子页面中找到图片的下载地址 `img -> src`
3.下载图片并保存

实现代码如下：

```python
import os
import time
import requests
from bs4 import BeautifulSoup

url = "https://www.umei.net/weimeitupian/hunshatupian/"

def main_spider(url):
    resp = requests.get(url, verify=False)
    resp.encoding = "utf-8"
    # print(resp.text)
    return resp.text

def parse(content):
    main_page = BeautifulSoup(content, "html.parser")
    a_list = main_page.find("div", class_="TypeList").find_all("a")
    # print(a_list)
    for a in a_list:
        href = "https://www.umei.net" + a.get("href")
        # 接下来子页面源代码
        child_page_text = main_spider(href)
        # 从子页面中获取图片的下载路径
        child_page = BeautifulSoup(child_page_text, "html.parser")
        image_name = child_page.find("h1", class_="inline").text
        image_href = child_page.find("div", class_="ImageBody").find("img").get("src")
        print(image_name, image_href)
        # 下载图片
        image_resp = requests.get(image_href).content
        if os.path.exists("./images") == False:
            os.mkdir("./images")
        with open ("./images/"+ image_name + ".jpg", mode="wb") as f:
            f.write(image_resp) # 图片保存到文件
        f.close()
        print("{}.jpg保存成功".format(image_name))
        time.sleep(1)


if __name__ == '__main__':
    parse(main_spider(url))
```

没有做进一步的爬取，只爬取了一个页面。

所有图片保存到文件夹`images`。

![](Screenshot_1.webp)

[//]:#(设置表格整体居中显示)
<style>
    table
    {
        margin: auto;
        font-size: 80%;
    }
</style>